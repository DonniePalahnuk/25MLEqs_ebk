% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12 pt,
  a4paper,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin = 0.9in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newcommand{\docTitle}{25 ML Fundamental Equations}

\raggedbottom                         % ensures no page fill

% setup special chapter only counter
\newcounter{chonlycounter}
\renewcommand{\thechonlycounter}{\thechapter}

% setup some special branding fonts 
% (this is only used at start of document - for product names )
  \usepackage{fontspec}
  \usepackage{textcomp}
  \newfontface\poppins{Poppins}[Path=fonts/, 
                            UprightFont=*-Regular, 
                            BoldFont=*-Bold]
  \newcommand{\textpoppins}[1]{{\poppins #1}}

  \newfontface\fluxreg{FluxRegular}[Path=fonts/, 
                            UprightFont=*.otf]
  \newcommand{\textfluxreg}[1]{{\fluxreg #1}}

  \newfontface\lato{Lato}[Path=fonts/, 
                            UprightFont=*-Regular]
  \newcommand{\textlato}[1]{{\lato #1}}

  \newfontface\ssp{SourceSansPro}[Path=fonts/, 
                            UprightFont=*-Regular]
  \newcommand{\textssp}[1]{{\ssp #1}}

  \newfontface\couriernew{Courier New}
  \newcommand{\textcouriernew}[1]{{\couriernew #1}}

  \newfontface\arial{Arial}
  \newcommand{\textarial}[1]{{\arial #1}}

  \newfontface\palatino{Palatino}
  \newcommand{\textpalatino}[1]{{\palatino #1}}

% FOR GRAPHICS tikz and pgfplots
%\usepackage{pgfplots}                % math plotting
%\pgfplotsset{compat=1.18}

%\usepackage{tikz}                    % tikz megatool
%\usepackage{tikz-3dplot}
%\usetikzlibrary{arrows}

%\usepackage{fvextra}                 % special linebreaking 
%\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}

%\usepackage[skins]{tcolorbox}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{changepage}               % width formatting on page

\definecolor{dblu}{HTML}{002db3}
%\definecolor{cyan}{HTML}{99ccff}
%\definecolor{ltorng}{HTML}{99ccff}
%\definecolor{ltgrn}{HTML}{002db3}

\usepackage{titling}

% testing 20240414 - margin methods - to adjust

\usepackage{layout}       % for margins setting ->employ:"\layout"
\usepackage{multicol}  
\usepackage{geometry} 

% Set asymmetric page margins (book formatting)
\geometry{
inner=0.75in,   % Inner margin
outer=1.0in,    % Outer margin
bottom=0.6in,   % Bottom margin
top=0.6in,      % Bottom margin
includeheadfoot  }

%\usepackage{lipsum}         % Generates filler text

% math fonts and coloring
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}        % specific symbols
\usepackage{mathrsfs}
\usepackage{xcolor}         % For coloring text
\usepackage{graphicx}

\definecolor{impactblue}{RGB}{30, 144, 255}
\definecolor{royalimpactblue}{RGB}{65, 105, 225} % Royal Blue, more vivid
\definecolor{navyimpactblue}{RGB}{0, 0, 128} %  avy Blue tone 
 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{float}

% so that weblinks are colored bluw
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}


\usepackage{enumitem}
\usepackage{varwidth}
\usepackage[nochapter, tocentry, owncaptions, tablegrid]{vhistory}
\usepackage{tasks}
\usepackage{scrlayer-scrpage}
\numberwithin{equation}{section}
\usepackage{lastpage}
\captionsetup[figure]{font=small,labelfont=small}

% Regular Theorems, definitions, lemmas, proofs

\theoremstyle{plain}      % from `amsthm'
\newtheorem{theorem}{Theorem}

\theoremstyle{definition} % from `amsthm'
\newtheorem{defn}{Definition}

\newtheorem{xmpl}{Example}% from `amsthm'

\theoremstyle{remark}     % from `amsthm'
\newtheorem{remark}{Remark}

\newtheorem{algorithm}{Algorithm} 
\newtheoremstyle{note}    % style name
{2ex}                     % above space
{2ex}                     % below space
{}                        % body font
{}                        % indent amount
{\scshape}                % head font
{.}                       % post head punctuation
{\newline}                % post head punctuation
{}                        % head spec

\theoremstyle{note}         % from `amsthm'
\newtheorem{scnote}{Note}  

% Colorbox - Theorems, definitions, lemmas, proofs

\usepackage[most]{tcolorbox}          % does not included 'minted' (issue with rmd) 
\newtheorem{lem}{Lemma}		% from `amsthm'
\newtheorem{pruf}{Proof}		% from `amsthm'

\newtcbtheorem[number within=chapter]{a_thm}{Theorem}
{enhanced,frame empty,interior empty,colframe=green!50!white,
  coltitle=green!40!black,fonttitle=\bfseries,colbacktitle=green!15!white,
  borderline={0.5mm}{0mm}{green!15!white},  
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, , 
  breakable, colback=white
}{theo}

% This is for Key Equations - and only indexes the Charter Number
% Should use only once in a Chapter 
\newtcbtheorem[use counter=chonlycounter]{a_def_eq}{Key ML Equation}
{enhanced,frame empty,interior empty,colframe=blue!50!white,
  coltitle=blue!40!black,fonttitle=\bfseries,colbacktitle=blue!15!white,
  borderline={0.5mm}{0mm}{blue!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, 
  breakable, colback=white
}{def_eq}

\newtcbtheorem[number within=chapter]{a_def}{Definition}
{enhanced,frame empty,interior empty,colframe=blue!50!white,
  coltitle=blue!40!black,fonttitle=\bfseries,colbacktitle=blue!15!white,
  borderline={0.5mm}{0mm}{blue!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, 
  breakable, colback=white
}{defi}

\newtcbtheorem[number within=chapter]{a_exmp}{Example}
{enhanced,frame empty,interior empty,colframe=cyan!50!white,
  coltitle=cyan!20!black,fonttitle=\bfseries,colbacktitle=cyan!15!white,
  borderline={0.5mm}{0mm}{cyan!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, , 
  breakable, colback=white
}{exmp}

\newtcbtheorem[number within=chapter]{a_lemma}{Lemma}
{enhanced,frame empty,interior empty,colframe=cyan!50!white,
  coltitle=cyan!20!black,fonttitle=\bfseries,colbacktitle=cyan!15!white,
  borderline={0.5mm}{0mm}{cyan!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, , 
  breakable, colback=white
}{lema}

\tcolorboxenvironment{lem}{
enhanced jigsaw,colframe=cyan,interior hidden, breakable,before skip=10pt,after skip=10pt 
}
\tcolorboxenvironment{pruf}{			
blanker,breakable,left=5mm,
before skip=10pt,after skip=10pt,
borderline west={1mm}{0pt}{red}
}

\pretitle{%
\begin{flushleft} \LARGE
\includegraphics[width=2cm,height=2cm]{pictures/IH2.jpg}
\end{flushleft}
\begin{flushright} \LARGE
}

\posttitle{\end{flushright}}
\preauthor{\begin{flushright}}
\postauthor{\end{flushright}}
\predate{\begin{flushright}}
\postdate{\end{flushright}}

\ohead[]{IH - Machine Learning Reviews}
\ifoot{\hsize=350pt \docTitle\ -- Version \vhCurrentVersion}
\ofoot{\thepage~/~\pageref{LastPage}}
\cfoot[]{}


\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={25 Key Equations in Machine Learning},
  pdfauthor={to-ohru iwanami \{the hatch keeper\}},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{25 Key Equations in Machine Learning}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{\textit{Insight Hatch Machine Leanring Reviews - Vol 1}}
\author{to-ohru iwanami \{the hatch keeper\}}
\date{\vhCurrentDate\\
version \vhCurrentVersion\\
InsightHatch: ``25MLEqs''\\
\textbf{IH-25MLEqs-v}\textbf{\vhCurrentVersion}}

\begin{document}
\frontmatter
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\setstretch{1.25}
\mainmatter
\hfill\break

\begin{versionhistory}
  \vhEntry{0.00.0}{2024.10.25}{DP}{Created!}
  \vhEntry{0.01.0}{2024.10.25}{DP}{Thereom boxes changes, layout changes}
  \vhEntry{0.03.0}{2024.10.26}{DP}{Buildout Chapter Frames}
  \vhEntry{0.04.0}{2024.11.07}{DP}{Sigmoid Chapter}
  \vhEntry{0.04.1}{2024.11.08}{DP}{Code clean up - house keeping, typos}
  \vhEntry{0.05.0}{2024.11.10}{DP}{Correlation Chapter}
\end{versionhistory}
\newpage

\setcounter{table}{0}

\frontmatter

\hypertarget{prefice-and-introduction}{%
\chapter{Prefice and Introduction}\label{prefice-and-introduction}}

This book is a journey through the 25 most important mathematical
equations that underpin modern data science and machine learning. The
idea is not only to learn about each of these equations but also to
deeply understand their applications, background, and practical
examples. We will explore each concept visually and instructively,
shedding light on how these fundamental equations contribute to building
intelligent systems.

Mathematics is the language of the universe, and it is the foundation of
the incredible advances we see today in artificial intelligence and
machine learning. The purpose of this book is to develop a learning
guide that is both informative and visually compelling, bringing out the
beauty and utility of these powerful mathematical tools. From
optimization algorithms like Gradient Descent to classification
techniques like Naive Bayes, and from measures of information like
Entropy to clustering algorithms like K-Means, this book aims to provide
an accessible yet thorough explanation of how these concepts work and
why they are essential.

Each chapter focuses on one equation, starting with an introduction,
followed by a detailed description, and ending with examples of how it
is used in practice. Whether you're an aspiring data scientist, a
seasoned engineer, or just curious about the mathematics that drives
intelligent technology, this book will provide you with a solid
understanding of these essential tools and how they interact to solve
complex problems. Visual aids, illustrative examples, and in-depth
explanations will help demystify each topic, making learning both
engaging and enjoyable.

Each section will be followed by questions and homework problems. These
problems are selected based on their popularity and effectiveness in
enhancing understanding. The goal is to provide exercises that solidify
the reader's comprehension of each topic. An answer key is provided at
the end of the book to facilitate learning and self-assessment.

Many of the examples and problems use Python and R to provide practical
insights. Code blocks in Python and R (and in some instances
Mathematica) are used throughout to illustrate the concepts discussed.
All the example codes, as well as the entire book, are posted on the
author's GitHub repository for easy access and reproducibility.

Whenever possible, all examples that originate from a particular source
are acknowledged by citation and recognition of the original author. If
any citation or attribution is missed, the author kindly requests
feedback so that proper corrections can be made.

\hypertarget{some-notes-on-the-typesetting-framework-and-tools}{%
\subsection*{Some notes on the typesetting framework and
tools}\label{some-notes-on-the-typesetting-framework-and-tools}}
\addcontentsline{toc}{subsection}{Some notes on the typesetting
framework and tools}

With the current advent of ML based LLMs and the availability of
typesetting \LaTeX , we combine tools like
\textbf{\large{\textpoppins{Mathpix}}} (the \LaTeX  equation generating
snipping tool) and \textcouriernew{R Markdown} driven by
\textbf{\large{\textarial{Pandoc}}} and
\textbf{\large{\textpalatino{knitr}}} to essentially ``GET IT DONE''.
Obviously your mileage may vary; we find this tool set perfect for
typesetting combined with computational analysis. Why? Well you have
nearly full \LaTeX  capability with access to R \&
\textbf{\large{\textfluxreg{Python}}} (via
\textbf{\large{\textlato{reticulate}}}) programming code blocks and also
the ability to bridge to
\text{\large{\textssp{MATHEMATICA}}}\textsuperscript{\textregistered} if
needed. On a powerful laptop, this tool suite (under RStudio) leaves the
door open to a wider world of analysis and computation. While RStudio is
not perfect, it does allow for an IDE approach which leverages
typesetting and computation framework (through R,
\textbf{\large{\textfluxreg{Python}}} and
\text{\large{\textssp{MATHEMATICA}}}\textsuperscript{\textregistered})
that makes it a worthy Swiss Army Knife. The alternative is also VS
Code, which is great, however the mixed mode \textcouriernew{R Markdown}
\& \LaTeX  computational environment is pretty flexible.

\hypertarget{the-complimentary-nod-to-ml-llm-tools}{%
\subsection*{The complimentary nod to ML LLM
tools}\label{the-complimentary-nod-to-ml-llm-tools}}
\addcontentsline{toc}{subsection}{The complimentary nod to ML LLM tools}

Most of this compilation and summary work would not be possible without
the advent of LLMs and new found ability to build concise summaries of
difficult mathematical concepts. The time savings is basically a gift
which save more grey hairs and time flipping pages of numerous cross
references. Actually we want to spend time ``learning'' and not
``flipping and skimming'' to find the properly defined statement which
is of utility in our endeavor. So for that - we lean on LLMs to be more
productive, and help clearly convey underlying concepts. This is not to
be lazy, each section must be scrutinized for accuracy and consistency.
The speed gains allow for a very large group of examples and highly
detailed presentations.

\hypertarget{notes-to-usage}{%
\subsection*{Notes to usage}\label{notes-to-usage}}
\addcontentsline{toc}{subsection}{Notes to usage}

We give full credit for any original works that are used herein to reach
the stated objective. At this point this is not a financial pursuit we
do not provide any license related respect to any previous publisher. Go
figure. Why would we take such a stand - this is educational open source
material. We strongly recommend you take the time to do some background
research on any referenced authors.\\

\emph{We truly hope you enjoy the tour, as much as we have enjoyed our
journey to write it\ldots{}}

to-ohru iwanami, 2024, somewhere in asia

\hypertarget{the-25-uhh-24}{%
\chapter{The 25 (uhh 24)}\label{the-25-uhh-24}}

How did we arrive at the 25 (uhh 24) most important equations? Well,
it's partly a matter of educated opinion, partly a rough statistical
average of what people talk about most when they're exhilarated by the
magic of machine learning, and partly---let's be real, you should just
listen to me (fully joking here). These equations are more than just
mathematical tools; they represent milestones that have shaped the
evolution of artificial intelligence, each one contributing a brick or
building block in the formidable wall of progress we've built over the
decades.

Take, for instance, Gradient Descent --- a cornerstone of optimization,
whose principles date back to the early 19th century and the work of
Adrien-Marie Legendre and Carl Friedrich Gauss in minimizing residuals.
The modern incarnation of Gradient Descent, essential for training deep
neural networks, only emerged in the mid-20th century, revitalized by
researchers trying to teach machines to ``learn.'' Fast forward to the
1960s, when Frank Rosenblatt was developing the perceptron, it became
evident that iterative optimization methods like Gradient Descent could
unlock the potential of early neural networks.

Then we have Bayes' theorem, after Reverend Thomas Bayes, who developed
his idea of conditional probability back in the 18th century. Although
it lay relatively dormant for years, it became a key breakthrough in the
1950s when Alan Turing and others applied Bayesian methods to
codebreaking during World War II. Today, its cousin---Naive Bayes --- is
still a powerful tool for classification, especially for natural
language processing, allowing us to create chatbots and email spam
filters. This dormant theorem found new life, propelling us toward a
world of probabilistic understanding in machines.

Singular Value Decomposition (SVD), another member of our elite list,
found its fame in the 1990s when it was used for information retrieval
in the famous Latent Semantic Analysis (LSA) algorithm---paving the way
for how we handle and understand textual data. The underlying
mathematics, discovered by Eugenio Beltrami and others in the 19th
century, helps us not only reduce dimensionality but also reveal the
hidden relationships between concepts in data.

Fast forward to today to the ReLU (Rectified Linear Unit) function.
While it seems so simple, this piece of mathematical elegance emerged as
a game-changer for deep learning in the 2010s, thanks to the work of
Geoffrey Hinton and his colleagues. Before ReLU, neural networks
struggled with the vanishing gradient problem, limiting their ability to
learn effectively. By simply transforming negative inputs to zero and
retaining positive ones, ReLU helped neural networks go deeper and
deeper (pun-ch!), giving birth to the deep learning revolution we know
today.

Each of these equations is a testament to the evolution of knowledge, a
story of breakthroughs spanning centuries. Together, they form a bridge
from the deterministic calculus of Newton and Leibniz to the
probabilistic dreams of Bayes and the practical algorithms of today's AI
pioneers. Think of them as the greatest hits of mathematics for data
scientists---carefully curated, occasionally debated, and ultimately
distilled into the essence of what drives machine learning today. This
collection captures a human legacy of curiosity and ingenuity, guiding
you on your intellectual journey through the ever-growing forest of
machine learning---one equation, one insight at a time.

As we reflect on these 1 through 24 equations, and on to 25, one cannot
ignore the human spirit that threads through each mathematical symbol
and formula. Behind every breakthrough is a mind driven by curiosity, a
determination to see beyond the obvious, to turn abstraction into
discovery. Each equation, in its own way, captures a moment where human
thought transcended barriers---whether it was a problem of optimization,
uncertainty, or understanding the nature of learning itself. Yet, even
as these 24 pillars of insight stand tall, there remains one more --- a
keystone that binds them all, born not out of calculation alone but from
the essence of what it means to explore, to question, and to create.
This final entry goes beyond numbers and symbols, embracing the very
source of every theorem, every insight, and every spark of ingenuity. So
as you journey onward, remember---there's always one more equation to
unveil, one that's been with us all along.

\mainmatter

\renewcommand{\chaptername}{Equation}

\hypertarget{gradient-descent}{%
\chapter{Gradient Descent}\label{gradient-descent}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $\theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-1-gradient_descent.jpeg}
    \caption*{\Large Finding a local minimum with Gradient Descent}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Gradient Descent}{def1} 

\begin{align}
\large{\theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)}
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$\theta_j$] The current value of the \textbf{parameter vector} at iteration $j$, which represents the current estimate of the model parameters.
    \vspace{0.5\baselineskip}
    \item[$\theta_{j+1}$] The updated value of the \textbf{parameter vector} for the next iteration, which results from applying the gradient step to the current parameters.
    \vspace{0.5\baselineskip}
    \item[$\alpha$] The \textbf{learning rate}, which is a positive scalar determining the size of the step to take in the direction of the negative gradient.
    \vspace{0.5\baselineskip}
    \item[$J$] The \textbf{cost function}, which is the function being minimized by adjusting the parameter vector $\theta$. It measures the difference between predicted values and actual values.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: Gradient Descent is an optimization algorithm
used to minimize the cost function by iteratively moving in the
direction of the steepest descent as defined by the negative of the
gradient.

\textbf{Description}: In machine learning, gradient descent is used to
update the parameters of the model, \(\theta\), to reduce the difference
between the predicted and actual outcomes.

\textbf{Importance in ML}: Gradient Descent is foundational for training
machine learning models, particularly in neural networks and linear
regression. It helps in finding optimal parameters by iteratively
reducing the error, making it crucial for model accuracy.\\

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation}{%
\section{What is behind the
equation}\label{what-is-behind-the-equation}}

\hypertarget{structure-of-the-parameter-vector-theta}{%
\section{\texorpdfstring{Structure of the Parameter Vector
\(\theta\)}{Structure of the Parameter Vector \textbackslash theta}}\label{structure-of-the-parameter-vector-theta}}

The parameter vector, typically denoted as \(\theta\), contains all the
adjustable parameters or weights of the model that you want to optimize
in order to minimize the cost function \(J(\theta)\). Depending on the
type of model, the structure of \(\theta\) can vary:

\begin{itemize}
\item
  \textbf{Linear Regression}:

  \begin{align}
  \theta = 
  \begin{bmatrix}
  \theta_0 \\
  \theta_1 \\
  \vdots \\
  \theta_n
  \end{bmatrix}
  \end{align}
\end{itemize}

In linear regression, \(\theta\) is often a column vector of weights
where each \(\theta_i\) corresponds to the weight associated with
feature \(x_i\). \(\theta_0\) is often referred to as the bias term or
intercept.

\begin{itemize}
\item
  \textbf{Logistic Regression / Neural Networks}: In these models,
  \(\theta\) can have more dimensions. For a neural network, the
  parameter vector could be a collection of weight matrices for
  different layers, such as:

  \begin{align}
  \theta = \{ W_1, W_2, \ldots, W_L, b_1, b_2, \ldots, b_L \}
  \end{align}

  where \(W_i\) and \(b_i\) are weights and biases associated with layer
  \(i\) of the network.
\item
  \textbf{Deep Learning Models}: The parameter vector is much more
  complex, often containing multiple matrices and vectors representing
  weights and biases for each layer in a deep neural network.
\end{itemize}

In general, \(\theta\) is a vector that can be expressed as:

\begin{align}
\theta = (\theta_1, \theta_2, \dots, \theta_n)^T
\end{align}

where \(n\) is the number of features or neurons, depending on the model
type.

\hypertarget{typical-value-of-the-learning-rate-alpha}{%
\section{\texorpdfstring{Typical Value of the Learning Rate
\(\alpha\)}{Typical Value of the Learning Rate \textbackslash alpha}}\label{typical-value-of-the-learning-rate-alpha}}

The learning rate \(\alpha\) controls the step size when updating the
parameters during gradient descent. Choosing a proper value for
\(\alpha\) is crucial for the convergence of the algorithm. Here are
some general guidelines:

\begin{itemize}
\item
  \textbf{Typical Range}: A typical value for the learning rate lies
  between \(0.001\) and \(0.1\). Values outside this range can either
  lead to a slow convergence or an unstable training process.
\item
  \textbf{Considerations}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Too Small}: If \(\alpha\) is too small, gradient descent
    will take very small steps towards the optimum, resulting in a very
    slow convergence process.
  \item
    \textbf{Too Large}: If \(\alpha\) is too large, gradient descent
    might overshoot the minimum or even diverge, causing the cost
    function to oscillate or increase.
  \item
    \textbf{Adaptive Learning Rates}: Some advanced algorithms like
    \textbf{Adam} use adaptive learning rates which adjust automatically
    as training progresses.
  \end{itemize}
\end{itemize}

In practice, tuning the learning rate often involves trial and error or
the use of techniques like \textbf{learning rate schedules} or
\textbf{grid search} to find the best value for a specific problem.

\hypertarget{math-behind-the-gradient-nabla-jtheta_j}{%
\section{\texorpdfstring{Math Behind the Gradient
\(\nabla J(\theta_j)\)}{Math Behind the Gradient \textbackslash nabla J(\textbackslash theta\_j)}}\label{math-behind-the-gradient-nabla-jtheta_j}}

The term \(\nabla J(\theta_j)\) is the \textbf{gradient} of the cost
function \(J(\theta)\) evaluated at \(\theta_j\). Let's break it down:

\begin{itemize}
\item
  \textbf{Gradient Definition}: The gradient of a function is a vector
  of partial derivatives with respect to each parameter in \(\theta\).
  In the case of \(J(\theta)\), the gradient \(\nabla J(\theta_j)\)
  tells us how much the cost function changes when we make an
  infinitesimally small change to each component of \(\theta\).
  Mathematically, for a parameter vector
  \(\theta_j = (\theta_1, \theta_2, \dots, \theta_n)^T\):

  \[
  \nabla J(\theta_j) = \begin{bmatrix}
  \frac{\partial J}{\partial \theta_1} \Big|_{\theta_j} \\
  \frac{\partial J}{\partial \theta_2} \Big|_{\theta_j} \\
  \vdots \\
  \frac{\partial J}{\partial \theta_n} \Big|_{\theta_j}
  \end{bmatrix}
  \]

  Each partial derivative \(\frac{\partial J}{\partial \theta_i}\)
  represents the rate of change of the cost function with respect to
  parameter \(\theta_i\).
\item
  \textbf{Intuition}: The gradient \(\nabla J(\theta_j)\) points in the
  direction of the \textbf{steepest ascent} of the cost function \(J\).
  In gradient descent, we want to \textbf{minimize} \(J(\theta)\), so we
  move in the opposite direction, which is why the update rule is:

  \[
  \theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(\nabla J(\theta_j)\) represents the slope or direction in which
    \(J(\theta)\) increases most quickly.
  \item
    By subtracting \(\alpha \nabla J(\theta_j)\), we effectively move in
    the direction of steepest \textbf{descent}, hence reducing
    \(J(\theta)\).
  \end{itemize}
\item
  \textbf{Computing the Gradient}: In practice, the gradient
  \(\nabla J(\theta_j)\) is computed using \textbf{differentiation}. For
  different cost functions, the gradient takes different forms:

  \begin{itemize}
  \tightlist
  \item
    For \textbf{linear regression} with a \textbf{mean squared error
    (MSE)} cost function, the gradient is relatively simple and involves
    the residuals (errors) between predictions and actual values.
  \item
    For \textbf{neural networks}, computing the gradient involves
    \textbf{backpropagation}, which is a process of applying the chain
    rule of calculus to calculate the gradients efficiently for each
    layer.
  \end{itemize}
\end{itemize}

In short, \(\nabla J(\theta_j)\) gives us the necessary information to
adjust \(\theta\) in a way that reduces the error. The learning rate
\(\alpha\) then determines how big the step should be in this direction.

\newpage

\hypertarget{gradient-descent-in-r}{%
\section{Gradient Descent in R}\label{gradient-descent-in-r}}

Gradient Descent is a foundational optimization algorithm used to
iteratively minimize cost functions. Here, we apply the gradient descent
method to find the local minimum of a specific polynomial function. The
function used is a quartic polynomial
\(P(x) = x^4 - 6x^3 + 11x^2 - 6x\). Our goal is to observe how the
gradient descent algorithm updates our parameter over successive
iterations to converge towards a minimum point of the polynomial.

The following R code demonstrates how to implement gradient descent for
this polynomial, including plotting the descent path and function value
across iterations. This practical example aims to give you a clear
understanding of how gradient descent operates on real functions, using
R for illustration. Note that gd\_result is the dataframe result fed to
ggplot.\\

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Define the new polynomial function and its derivative}
\NormalTok{polynomial\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{return}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{4} \SpecialCharTok{{-}} \DecValTok{8} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{3} \SpecialCharTok{+} \DecValTok{18} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{11} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Derivative of the new polynomial function}
\NormalTok{polynomial\_derivative }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{return}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{24} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{36} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{11}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Gradient Descent Algorithm}
\NormalTok{gradient\_descent }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{learning\_rate =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{iterations =} \DecValTok{1000}\NormalTok{, }\AttributeTok{start =} \DecValTok{3}\NormalTok{) \{}
\NormalTok{  x }\OtherTok{\textless{}{-}}\NormalTok{ start  }\CommentTok{\# Starting point}
  
  \CommentTok{\# Store x values and function values for plotting}
\NormalTok{  x\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(iterations)}
\NormalTok{  function\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(iterations)}
  
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{iterations) \{}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{polynomial\_derivative}\NormalTok{(x)}
\NormalTok{    x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{{-}}\NormalTok{ learning\_rate }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{    x\_values[i] }\OtherTok{\textless{}{-}}\NormalTok{ x}
\NormalTok{    function\_values[i] }\OtherTok{\textless{}{-}} \FunctionTok{polynomial\_function}\NormalTok{(x)}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Iteration =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{iterations, x\_values, function\_values))}
\NormalTok{\}}


\CommentTok{\# Run the Gradient Descent with specific parameters}
\NormalTok{learning\_rate }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\NormalTok{iterations }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{start\_point }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{gd\_result }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_descent}\NormalTok{(learning\_rate, iterations, start\_point)}
\end{Highlighting}
\end{Shaded}

\normalsize

The following R code demonstrates how to implement gradient descent for
this polynomial, including plotting the descent path and function value
across iterations. This practical example aims to give you a clear
understanding of how gradient descent operates on real functions, using
R for illustration.

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(gd\_result, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Iteration, }\AttributeTok{y =}\NormalTok{ function\_values)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Gradient Descent on Polynomial Function"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Iteration"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Objective Function Value: f(x)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{11}\NormalTok{)  }\CommentTok{\# Adjust the size value as needed}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{25MLEqs-V0.0_files/figure-latex/gradient-descent-iteration-plot1-1} 

}

\caption{Gradient Descent on Polynomial Function - Iteration vs Function Value}\label{fig:gradient-descent-iteration-plot1}
\end{figure}
\normalsize

\newpage
\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Additional Plot: Trace the Path of Gradient Descent on the Polynomial}
\NormalTok{x\_range }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(gd\_result}\SpecialCharTok{$}\NormalTok{x\_values) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\FunctionTok{max}\NormalTok{(gd\_result}\SpecialCharTok{$}\NormalTok{x\_values) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{500}\NormalTok{)}
\NormalTok{polynomial\_values }\OtherTok{\textless{}{-}} \FunctionTok{polynomial\_function}\NormalTok{(x\_range)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_range, }\AttributeTok{y =}\NormalTok{ polynomial\_values), }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ gd\_result, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_values, }\AttributeTok{y =}\NormalTok{ function\_values), }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Gradient Descent Path on Polynomial Function"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"f(x)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{11}\NormalTok{)  }\CommentTok{\# Adjust the size value as needed}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{25MLEqs-V0.0_files/figure-latex/gradient-descent-iteration-plot2-1} 

}

\caption{Gradient Descent on Polynomial Function - Trace the Path on the Polynomial}\label{fig:gradient-descent-iteration-plot2}
\end{figure}
\normalsize

\newpage

\hypertarget{linking-gradient-descent-to-our-polynomial-example}{%
\section{Linking Gradient Descent to Our Polynomial
Example}\label{linking-gradient-descent-to-our-polynomial-example}}

The equation we used to explain gradient descent is:

\[
\theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)
\]

\hypertarget{step-by-step-breakdown}{%
\subsection{Step-by-Step Breakdown}\label{step-by-step-breakdown}}

The fundamental idea behind gradient descent is to iteratively adjust
the model's parameters, \(\theta\), in a direction that reduces the cost
function, \(J(\theta)\). The parameter update is governed by the
\textbf{gradient} of the cost function (or loss function),
\(\nabla J(\theta)\).

In the above equation:

\begin{itemize}
\tightlist
\item
  \(\theta_{j}\) represents the parameter(s) at step \(j\), and
  \(\theta_{j+1}\) is the updated parameter at step \(j+1\).
\item
  \(\alpha\) is the \textbf{learning rate}, which controls how large
  each step is in the descent.
\item
  \(\nabla J(\theta_j)\) is the \textbf{gradient} of the cost function
  with respect to the parameters, evaluated at \(\theta_j\). It gives
  the direction of the steepest ascent, and since we want to minimize
  the function, we move in the opposite direction, hence the negative
  sign.
\end{itemize}

\hypertarget{gradient-descent-applied-to-the-polynomial}{%
\subsection{Gradient Descent Applied to the
Polynomial}\label{gradient-descent-applied-to-the-polynomial}}

In our R code example, we have a \textbf{polynomial function} defined
as:

\[
P(x) = x^4 - 6x^3 + 11x^2 - 6x
\]

Our goal is to minimize this polynomial function, which represents our
\textbf{cost function}, \(J(x)\). Here, \(x\) plays the role of our
parameter, \(\theta\), that we want to adjust iteratively using gradient
descent.

The \textbf{derivative} of the polynomial is:

\[
P'(x) = 4x^3 - 18x^2 + 22x - 6
\]

In the gradient descent algorithm, this derivative represents the
\textbf{gradient} of the cost function with respect to our parameter,
\(x\). This means that \(\nabla J(x) = P'(x)\). The iterative update
step becomes:

\[
x_{j+1} = x_j - \alpha P'(x_j)
\]

Where: - \(x_j\) is the current value of the parameter (similar to
\(\theta_j\)). - \(\alpha\) is the learning rate that we set in the R
code. - \(P'(x_j)\) is the gradient of the polynomial at the current
point.

\hypertarget{bringing-it-full-circle}{%
\subsection{Bringing It Full Circle}\label{bringing-it-full-circle}}

In our R code, we used \textbf{gradient descent} to minimize the
polynomial function by starting at an initial point (\(x = 3\)) and
repeatedly updating \(x\) using the gradient descent rule:

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{{-}}\NormalTok{ learning\_rate }\SpecialCharTok{*}\NormalTok{ grad}
\end{Highlighting}
\end{Shaded}

\normalsize

This corresponds exactly to the equation:

\[
x_{j+1} = x_j - \alpha P'(x_j)
\]

As each iteration proceeds, \(x\) gets closer and closer to a point
where the gradient is zero (i.e., a local minimum or a stationary
point). In this particular case, the polynomial \(P(x)\) has one real
minimum, and the gradient descent algorithm helps us converge towards
it.

\hypertarget{visual-connection}{%
\subsection{Visual Connection}\label{visual-connection}}

The \textbf{first plot} in our example (function value vs.~iteration)
shows how the value of the polynomial decreases over time as gradient
descent proceeds. The \textbf{second plot} (gradient descent path on the
polynomial curve) visually shows how \(x\) moves along the polynomial
curve, getting closer and closer to the minimum.

This linkage between the theoretical equation of gradient descent and
the practical implementation of minimizing the polynomial demonstrates
how gradient descent serves as a universal tool to solve optimization
problems, whether in machine learning or even in simple polynomial
functions like the one used in our example.

Gradient descent works by always taking steps in the direction that most
rapidly reduces the cost function---allowing us to find optimal
parameters, minimize errors, or reach local minima. It is an essential
part of training many machine learning models, and the concept remains
consistent regardless of the specific problem context.

\normalsize
\setstretch{1.25}

\newpage

\hypertarget{additonal-ideas-to-explore}{%
\section{Additonal ideas to explore}\label{additonal-ideas-to-explore}}

Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, or
concepts like Momentum and Learning Rate Scheduling.

We could also explore more code examples, such as:

Adding stopping criteria to our gradient descent R code, like a
tolerance for change in cost function.

Visualizing the convergence of the gradient descent with multiple
starting points.

Implementing different cost functions and comparing their optimization
trajectories. Let me know which direction you'd like to explore, and
I'll get started!

\newpage

\hypertarget{normal-distribution}{%
\chapter{Normal Distribution}\label{normal-distribution}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $f(x | \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-2-normal_distribution.jpeg}
    \caption*{\Large Seeing the average and variance with the Normal Distribution}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Normal Distribution}{def2} 

\begin{align}
 f(x | \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$x$] The \textbf{random variable} for which the probability density function is being calculated. It represents the value within the distribution.
    \vspace{0.5\baselineskip}
    \item[$\mu$] The \textbf{mean} of the distribution, which represents the center or "average" value around which the data clusters.
    \vspace{0.5\baselineskip}
    \item[$\sigma$] The \textbf{standard deviation} of the distribution, indicating how spread out the values are around the mean. A larger $\sigma$ means more spread, while a smaller $\sigma$ means values are more tightly clustered.
    \vspace{0.5\baselineskip}
    \item[$\sigma^2$] The \textbf{variance} of the distribution, which is the square of the standard deviation. It provides a measure of the dispersion of the distribution.
    \vspace{0.5\baselineskip}
    \item[$f(x | \mu, \sigma^2)$] The \textbf{probability density function (PDF)} for the normal distribution, which provides the likelihood of $x$ occurring given the parameters $\mu$ and $\sigma^2$.
    \vspace{0.5\baselineskip}
    \item[$\exp$] The \textbf{exponential function}, which ensures that the PDF value falls off symmetrically from the mean $\mu$. It plays a key role in modeling the bell-shaped curve characteristic of the normal distribution.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: The normal distribution, also called the Gaussian
distribution, is a probability distribution that is symmetric about the
mean.

\textbf{Description}: It represents how data tends to cluster around a
central point. The parameters \(\mu\) and \(\sigma^2\) represent the
mean and variance, respectively.

\textbf{Importance in ML}: The normal distribution is used in many ML
algorithms, especially in probabilistic models and hypothesis testing.
Assumptions of normality often simplify the mathematics of learning
models and are vital in Bayesian networks.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-1}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-1}}

The normal distribution equation represents a probability density
function (PDF) that models how data points are distributed around a
central value (the mean, \(\mu\)). This equation is essential in
statistics because it describes a common pattern found in natural
phenomena, from heights of people to errors in measurements. The
bell-shaped curve is a striking visual, representing how values tend to
cluster around the mean, with fewer observations occurring as we move
further from this center. The beauty of the normal distribution lies in
its symmetry and the way it characterizes many real-world datasets,
making it a cornerstone in both statistics and machine learning.

\newpage

\hypertarget{the-players}{%
\section{The Players}\label{the-players}}

\begin{figure}[H]
    \centering
    % Left Image (Normal Distribution Summary)
    \begin{minipage}[t]{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/normal_distribution.jpeg}
        \caption{Summary of Normal Distribution with PDF and CDF}
    \end{minipage}
    \hfill
    % Right Images (Carl Friedrich Gauss and Pierre-Simon Laplace)
    \begin{minipage}[t]{0.40\textwidth}
        \vspace{-38em} % Negative space to align the right images better with the left
        \centering
        \includegraphics[width=\textwidth, height=0.3\textheight]{pictures/gauss.jpeg}
        \caption{Carl Friedrich Gauss}
        \vspace{1em} % Adds some spacing between the two right images
        \includegraphics[width=\textwidth, height=0.3\textheight]{pictures/laplace.jpeg}
        \caption{Pierre-Simon Laplace}
    \end{minipage}

    \label{fig:normal-distribution-layout}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Normal Distribution}:
  {[}\href{https://en.wikipedia.org/wiki/Normal_distribution}{Normal Distribution on Wikipedia}{]}
\item
  \textbf{Carl Friedrich Gauss}:
  {[}\href{https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss}{Carl Friedrich Gauss on Wikipedia}{]}
\item
  \textbf{Pierre-Simon Laplace}:
  {[}\href{https://en.wikipedia.org/wiki/Pierre-Simon_Laplace}{Pierre-Simon Laplace on Wikipedia}{]}
  \newpage
\end{enumerate}

\hypertarget{historical-context}{%
\section{Historical Context}\label{historical-context}}

The normal distribution, often referred to as the Gaussian distribution,
has a rich history rooted in the development of probability theory and
statistics. The distribution is named after the German mathematician
Carl Friedrich Gauss, who used it extensively in his work on astronomy
and measurement errors in the early 19th century. Gauss formalized the
idea that errors in measurements tend to follow a symmetric pattern
around the true value, leading to the bell-shaped curve we now associate
with the normal distribution.

However, the concept of the normal distribution predates Gauss and can
be traced back to the work of Abraham de Moivre, an 18th-century French
mathematician. De Moivre first derived the normal distribution as an
approximation to the binomial distribution when the number of trials
becomes very large. His work laid the foundation for what would later be
formalized and popularized by Gauss.

The normal distribution became particularly significant due to the
Central Limit Theorem, which states that the sum of many independent
random variables, regardless of their original distribution, tends to
follow a normal distribution. This theorem, proven by mathematicians
such as Pierre-Simon Laplace, helped establish the normal distribution
as a fundamental tool in statistics and the natural sciences. Today, it
is widely used not only because of its mathematical properties but also
because it naturally arises in numerous real-world situations, making it
one of the most important and recognizable distributions in probability
and statistics.

\hypertarget{relative-standard-deviation-rsd}{%
\section{Relative Standard Deviation
(RSD)}\label{relative-standard-deviation-rsd}}

In the context of the normal distribution, the spread of data points is
characterized by the standard deviation (\(\sigma\)). A useful concept
derived from this is the \textbf{Relative Standard Deviation (RSD)},
which is expressed as a percentage of the mean:

\[
\text{RSD} = \left( \frac{\sigma}{\mu} \right) \times 100
\]

\textbf{Typical RSD Values}: The RSD provides insight into how variable
the data is relative to its mean. A low RSD (e.g., below 10\%) indicates
that the data points are closely clustered around the mean, whereas a
higher RSD suggests more dispersion. In many real-world datasets, RSDs
ranging between 5\% and 20\% are common, depending on the type of
measurement and its inherent variability. RSD helps give a standardized
view of spread that is independent of the scale of the data, which is
particularly useful when comparing the variability between datasets.

\hypertarget{understanding-the-notation-fx-mu-sigma2}{%
\section{\texorpdfstring{Understanding the Notation
\(f(x | \mu, \sigma^2)\)}{Understanding the Notation f(x \textbar{} \textbackslash mu, \textbackslash sigma\^{}2)}}\label{understanding-the-notation-fx-mu-sigma2}}

The notation \(f(x | \mu, \sigma^2)\) represents the \textbf{probability
density function} of a normal distribution given the parameters \(\mu\)
(mean) and \(\sigma^2\) (variance). This notation indicates a
\textbf{conditional relationship}, where the probability density
function \(f(x)\) is dependent on the parameters \(\mu\) and
\(\sigma^2\).

In other words, \(\mu\) and \(\sigma^2\) define the specific
characteristics of the distribution (where it is centered and how spread
out it is), while \(x\) represents the variable for which the
probability is being calculated. This notation highlights that the
distribution is characterized by these parameters, and the output
\(f(x)\) tells us how likely it is to observe a particular value of
\(x\) under that distribution.

\hypertarget{the-nature-of-the-exponential-function-in-normal-distribution}{%
\section{The Nature of the Exponential Function in Normal
Distribution}\label{the-nature-of-the-exponential-function-in-normal-distribution}}

A key component of the normal distribution equation is the
\textbf{exponential function}:

\[
\exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)
\]

This part of the function gives the normal distribution its famous
\textbf{bell shape}. The negative squared term in the exponent ensures
that values closer to the mean (\(\mu\)) have higher probabilities,
while values further away have exponentially decreasing probabilities.
This shape is what makes the normal distribution symmetric, with a
single peak at the mean.

The function \(\exp(-x^2)\) falls off rapidly as \(|x|\) increases,
which results in a smooth, continuous decline from the peak at the mean.
This property is crucial because it reflects how, in many natural
phenomena, values tend to cluster around an average, with extreme values
being much less common. This elegant behavior is what makes the normal
distribution so special and why it is so frequently used in statistical
modeling.

\hypertarget{non-existence-of-an-analytical-anti-derivative}{%
\section{Non-existence of an Analytical
Anti-Derivative}\label{non-existence-of-an-analytical-anti-derivative}}

Interestingly, the \textbf{normal distribution function does not have an
anti-derivative} that can be expressed in closed form. This means that
the area under the curve (which represents the cumulative probability)
cannot be solved using traditional analytic integration techniques.
Instead, it is computed numerically or looked up using statistical
tables.

The integral of the normal distribution is given by:

\[
\int_{-\infty}^{\infty} f(x | \mu, \sigma^2) \, dx = 1
\]

However, there is no elementary function that represents this integral.
This is why the \textbf{error function (erf)} is introduced in
mathematics to help approximate these values:

\[
\int e^{-x^2} \, dx = \frac{\sqrt{\pi}}{2} \, \text{erf}(x) + C
\]

The lack of an analytical solution means that in practice, probabilities
for a normal distribution are often calculated using \textbf{numerical
methods} or \textbf{precomputed tables}. This characteristic of the
normal distribution makes it an interesting function from a mathematical
standpoint, as it combines simplicity of form with complexity in
integration.

\hypertarget{central-limit-theorem}{%
\section{Central Limit Theorem}\label{central-limit-theorem}}

The Central Limit Theorem (CLT) is a fundamental concept in probability
theory that explains why the normal distribution is so prevalent in
nature and in machine learning. The CLT states that when independent
random variables are added together, their properly normalized sum tends
to form a normal distribution, regardless of the original distribution
of the variables. This remarkable property applies as long as the number
of variables is sufficiently large and they have finite variances.

Mathematically, if we have a set of random variables , each with mean
and variance , the sum or average of these variables will tend towards a
normal distribution as becomes large. The formal expression is:

where denotes convergence in distribution, and represents the standard
normal distribution.

The CLT helps explain why the normal distribution is observed so often
in practice. Many complex processes can be thought of as the sum of many
small, independent effects. Whether it's measurement errors, human
heights, or even financial market fluctuations, these effects combine to
form a distribution that is approximately normal.

\hypertarget{importance-in-machine-learning}{%
\subsection{Importance in Machine
Learning}\label{importance-in-machine-learning}}

The Central Limit Theorem has significant implications in machine
learning. It provides the foundation for many statistical techniques and
justifies the assumption of normality in models. For example:

Modeling Errors: In regression analysis, the residuals (errors) are
often assumed to be normally distributed. This assumption allows us to
derive confidence intervals and perform hypothesis testing.

Feature Engineering: When aggregating data, such as calculating the mean
of multiple features or observations, the resulting values tend to be
normally distributed, making it easier to apply techniques that assume
normality.

Sampling Distributions: The CLT allows us to make inferences about
population parameters from sample data. Many machine learning algorithms
rely on sampling and estimation, and the CLT ensures that the
distribution of the sample mean approximates normality, which simplifies
analysis and interpretation.

The CLT thus serves as a bridge between randomness and order, allowing
us to apply the powerful tools of the normal distribution even in cases
where the underlying data might not be normally distributed. It is this
ability to generalize and predict outcomes that makes the normal
distribution so central to both statistics and machine learning.

\hypertarget{applications-in-machine-learning}{%
\section{Applications in Machine
Learning}\label{applications-in-machine-learning}}

The normal distribution plays a vital role in numerous machine learning
algorithms and concepts. Its presence is seen in everything from model
assumptions to data transformations. Below are some of the key areas
where the normal distribution is commonly applied:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gaussian Naive Bayes
\end{enumerate}

Gaussian Naive Bayes is a classification algorithm based on Bayes'
theorem. It assumes that the features follow a normal distribution,
which allows the model to calculate probabilities efficiently. The
assumption of normality simplifies the calculations, enabling rapid
classification even with high-dimensional data. This assumption works
well for many real-world datasets, making Gaussian Naive Bayes a popular
choice for problems like spam detection and document classification.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Linear Regression Error Terms
\end{enumerate}

In linear regression, the error terms (residuals) are often assumed to
be normally distributed. This assumption allows us to make statistical
inferences about the parameters of the regression model, such as
constructing confidence intervals and conducting hypothesis tests. If
the residuals are approximately normal, we can apply powerful
statistical tools to evaluate model fit and make predictions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Weight Initialization in Neural Networks
\end{enumerate}

When training neural networks, the weights are often initialized using a
normal distribution. For example, in the Xavier initialization method,
weights are drawn from a normal distribution with a mean of zero and a
variance that depends on the number of input and output nodes. This
helps ensure that the neurons start with a diverse range of values,
which prevents issues such as all neurons producing the same output.
Proper initialization is crucial for efficient training, and using the
normal distribution helps keep the gradients within a reasonable range
during backpropagation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Generative Models
\end{enumerate}

The normal distribution is also used in generative models, such as
Gaussian Mixture Models (GMMs), which assume that the data is generated
from a mixture of several Gaussian distributions. GMMs are widely used
in clustering problems, where they help to model the underlying
distribution of the data and assign probabilities to different clusters.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Feature Scaling and Data Transformation
\end{enumerate}

In many machine learning algorithms, it is beneficial for features to
follow a normal distribution. Methods such as StandardScaler in
scikit-learn standardize features by removing the mean and scaling to
unit variance, resulting in a distribution with a mean of 0 and a
standard deviation of 1. This process is especially important for
algorithms that are sensitive to the scale of the input features, such
as support vector machines (SVMs) and gradient descent optimization.

Importance of the Normal Distribution in Machine Learning

The normal distribution is not only a convenient assumption but also a
useful tool in machine learning. Its prevalence in real-world phenomena
and its mathematical properties make it indispensable for building,
evaluating, and optimizing models. Whether it's in simplifying
calculations through Gaussian Naive Bayes, making statistical inferences
in linear regression, initializing neural networks effectively, or
clustering data in GMMs, the normal distribution is at the core of
numerous machine learning practices. By understanding and leveraging the
normal distribution, practitioners can better model uncertainties and
improve the robustness of their machine learning solutions.

\hypertarget{suggested-additional-content-for-the-chapter}{%
\section{Suggested Additional Content for the
Chapter}\label{suggested-additional-content-for-the-chapter}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Historical Context}: Add a brief history of the normal
  distribution, perhaps mentioning Carl Friedrich Gauss, who contributed
  to its development, and why it is often called the Gaussian
  distribution.
\item
  \textbf{Visualization}: Add a visualization of the normal distribution
  with varying means and standard deviations to help illustrate how
  changes in \(\mu\) and \(\sigma\) affect the shape of the curve.
\end{enumerate}

\newpage

\hypertarget{z-score}{%
\chapter{Z-Score}\label{z-score}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $z = \frac{x - \mu}{\sigma}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-3-Zscore.jpeg}
    \caption*{\Large Take a portion of the probability with the Z-Score}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Z-Score}{def3} 

\begin{align}
z = \frac{x - \mu}{\sigma}
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$x$] The \textbf{random variable}, representing the value within the distribution that is being standardized.
    \vspace{0.5\baselineskip}
    \item[$\mu$] The \textbf{mean} of the distribution, which is the average value and the point around which data tends to cluster.
    \vspace{0.5\baselineskip}
    \item[$\sigma$] The \textbf{standard deviation} of the distribution, indicating the spread or dispersion of values around the mean. A larger $\sigma$ suggests more variability.
    \vspace{0.5\baselineskip}
    \item[$z$] The \textbf{Z-Score}, which represents how many standard deviations a particular $x$ value is from the mean $\mu$. It is a measure of relative position within the distribution.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: The Z-score represents the number of standard
deviations a data point is from the mean.

\textbf{Description}: It is used to standardize data points within a
dataset, making comparisons between different distributions possible.

\textbf{Importance in ML}: Z-scores are crucial in feature scaling and
normalization, allowing different features to be compared and helping
gradient-based algorithms converge faster by ensuring all features have
a similar scale.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-2}{%
\section*{What is Behind the
Equation}\label{what-is-behind-the-equation-2}}
\addcontentsline{toc}{section}{What is Behind the Equation}

The Z-score, also known as the standard score, is a measure that
describes the position of a value relative to the mean of a dataset, in
units of the standard deviation. The Z-score calculation is particularly
useful in the context of the normal distribution, allowing us to
determine how far a particular value \(x\) lies from the mean \(\mu\)
when expressed in terms of the distribution's standard deviation
\(\sigma\). The Z-score is expressed by the formula:

\[
 z = \frac{x - \mu}{\sigma}
\]

This metric is instrumental in transforming individual data points into
a universal scale, where positive Z-scores indicate values above the
mean, and negative Z-scores indicate values below the mean. The Z-score
effectively normalizes different datasets, making comparisons
straightforward.

\hypertarget{applications-in-machine-learning-1}{%
\section{Applications in Machine
Learning}\label{applications-in-machine-learning-1}}

In machine learning, Z-scores are employed for several purposes, ranging
from outlier detection to feature scaling.

\begin{itemize}
\item
  \textbf{Outlier Detection}: Z-scores can help identify outliers, as
  values with extremely high or low Z-scores typically indicate data
  points that lie far from the distribution's average behavior. These
  outliers might signify anomalies or valuable insights that need closer
  examination.
\item
  \textbf{Feature Scaling}: Z-score normalization, also known as
  standardization, is a common feature scaling method. By transforming
  features using Z-scores, they are rescaled to have a mean of 0 and a
  standard deviation of 1. This type of normalization is especially
  valuable in algorithms like logistic regression, k-means clustering,
  and principal component analysis (PCA), where features with different
  ranges can negatively impact model performance.
\item
  \textbf{Standard Normal Table Applications}: The Z-score is also used
  in combination with the standard normal distribution table to compute
  probabilities and p-values for hypothesis testing. In machine
  learning, this is often relevant in model evaluation and statistical
  testing.
\end{itemize}

\hypertarget{typical-range-for-z-scores}{%
\section{Typical Range for Z-Scores}\label{typical-range-for-z-scores}}

The typical range for Z-scores in a standard normal distribution is
between \(-3\) and \(3\). Values beyond this range are considered rare
and are often associated with outliers.

\begin{itemize}
\tightlist
\item
  \textbf{\(|Z| > 3\)}: Typically considered outliers. Values beyond
  three standard deviations are quite unusual in a normal distribution.
\item
  \(|Z| < 1\): Most values are likely within this range and close to the
  mean.
\item
  \(1 \leq |Z| < 2\): These values are still common, although they are
  further away from the mean.
\item
  \(|Z| > 2\): Values in this range start to become unusual, indicating
  either naturally rare observations or data quality issues.
\end{itemize}

The calculation of Z-score can be a valuable indicator of how far a data
point deviates from what is expected, helping differentiate between
normal variance and potential outliers.

\hypertarget{mathematical-notation-and-concept}{%
\section{Mathematical Notation and
Concept}\label{mathematical-notation-and-concept}}

The equation for the Z-score is represented as follows:

\[
z = \frac{x - \mu}{\sigma}
\]

\begin{itemize}
\tightlist
\item
  \textbf{\(x\)}: Represents the actual observed value within the
  dataset.
\item
  \textbf{\(\mu\)}: The mean of the dataset, which represents the
  central tendency around which the dataset is distributed.
\item
  \textbf{\(\sigma\)}: The standard deviation, which gives insight into
  the degree of variability within the data points. It tells us how
  spread out the values are around the mean.
\end{itemize}

The Z-score essentially tells us how many standard deviations away from
the mean a given value \(x\) is, and whether it is to the left (negative
\(z\)) or right (positive \(z\)) of the mean.

\hypertarget{mathematical-insights-area-under-the-normal-curve}{%
\section{Mathematical Insights: Area Under the Normal
Curve}\label{mathematical-insights-area-under-the-normal-curve}}

One of the key properties of the Z-score is how it allows for the
calculation of probabilities from the normal distribution. By converting
a value to its Z-score, we can use the standard normal distribution
(which has a mean of 0 and a standard deviation of 1) to determine the
probability that a value is less than or greater than a given point.

For example, calculating the probability \(P(Z \le z)\) involves
integrating the probability density function of the normal distribution
up to the given Z-score value:

\[
P(Z \le z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt
\]

Since this integral has no closed-form solution, it is typically
evaluated using numerical methods, standard normal distribution tables,
or software. This integral defines the cumulative distribution function
(CDF) of the normal distribution, which is essential in assessing
cumulative probabilities for standard normal variables.

In practice, machine learning models often use pre-calculated tables or
libraries to compute these probabilities for performance metrics,
hypothesis testing, or evaluating the significance of model parameters.

\hypertarget{example-z-score-in-r}{%
\section{Example: Z-Score in R}\label{example-z-score-in-r}}

The Z-score calculation can be easily implemented in R to standardize a
dataset or to identify outliers. Below is a simple example to illustrate
how we can compute Z-scores for a given dataset and visualize the data
distribution using a histogram:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Define the parameters for the normal distribution}
\NormalTok{mean\_value }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{std\_dev }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{z\_score }\OtherTok{\textless{}{-}} \FloatTok{1.25}

\CommentTok{\# Define the cumulative distribution function (CDF) to calculate probability}
\NormalTok{p\_value }\OtherTok{\textless{}{-}} \FunctionTok{pnorm}\NormalTok{(z\_score, }\AttributeTok{mean =}\NormalTok{ mean\_value, }\AttributeTok{sd =}\NormalTok{ std\_dev)}

\CommentTok{\# Generate data for plotting the normal distribution curve}
\NormalTok{x\_values }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{1000}\NormalTok{)}
\NormalTok{y\_values }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(x\_values, }\AttributeTok{mean =}\NormalTok{ mean\_value, }\AttributeTok{sd =}\NormalTok{ std\_dev)}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_values, }\AttributeTok{y =}\NormalTok{ y\_values)}

\CommentTok{\# Plot the normal distribution curve with shaded area under the curve}
\NormalTok{plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_area}\NormalTok{(}\AttributeTok{data =} \FunctionTok{subset}\NormalTok{(data, x }\SpecialCharTok{\textless{}=}\NormalTok{ z\_score), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ z\_score, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Normal Distribution with Z{-}Score of 1.25"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Z"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\CommentTok{\# Print the plot}
\FunctionTok{print}\NormalTok{(plot)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{25MLEqs-V0.0_files/figure-latex/z_score_example-1} 

}

\caption{Default caption for all figures}\label{fig:z_score_example}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print out the results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Z{-}Score:"}\NormalTok{, z\_score, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Z-Score: 1.25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Probability (P{-}value) for Z \textless{}="}\NormalTok{, z\_score, }\StringTok{":"}\NormalTok{, }\FunctionTok{round}\NormalTok{(p\_value }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Probability (P-value) for Z <= 1.25 : 89.44 %
\end{verbatim}

\normalsize

In this code block: * We first generate a dataset of 100 normally
distributed random numbers with a mean of 50 and a standard deviation of
10. * We calculate the Z-scores for each value by subtracting the mean
and dividing by the standard deviation of the dataset. * Finally, we
plot the distribution of the Z-scores using a histogram to visualize
their spread.

This example showcases how the Z-score helps normalize data to a common
scale, making it easier to compare different values and identify
outliers in the dataset.

\newpage

\hypertarget{sigmoid-function}{%
\chapter{Sigmoid Function}\label{sigmoid-function}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $\sigma(x) = \frac{1}{1 + e^{-x}}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-4-Sigmoid.jpeg}
    \caption*{\Large A smooth on or off transition}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}
\begin{a_def_eq}{Sigmoid Function}{def4} 

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$z$] The \textbf{input variable}, which is the linear combination of features and weights in a model. It represents the score calculated before applying the sigmoid transformation and can take any real value.
    \vspace{0.5\baselineskip}
    \item[$\sigma(z)$] The \textbf{sigmoid output}, which is the transformed value of \( z \), mapped between 0 and 1. This output represents the probability that the input belongs to a particular class (e.g., 1 for positive, 0 for negative in binary classification).
    \vspace{0.5\baselineskip}
    \item[$e$] The mathematical constant, approximately equal to 2.718, which forms the base of the natural logarithm. The exponential term \( e^{-z} \) causes the sigmoid function to asymptotically approach 0 for large negative \( z \) and 1 for large positive \( z \).
    \vspace{0.5\baselineskip}
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: The sigmoid function is an activation function
that outputs values between 0 and 1.

\textbf{Description}: It maps any input value into a range between 0 and
1, making it suitable for probability-related tasks.

\textbf{Importance in ML}: The sigmoid function is widely used in
logistic regression and as an activation function in neural networks. It
helps in modeling binary classification problems and introduces
non-linearity into neural models.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-3}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-3}}

The sigmoid function, denoted as \(\sigma(x)\), is an activation
function widely used in machine learning and data science. It maps any
real-valued number to a value between 0 and 1, making it ideal for
scenarios where probability-related interpretations are essential,
especially binary classification problems.

This equation transforms an input \(x\) into an output within the range
of 0 to 1, effectively normalizing the input space. \begin{align}
\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x} = 1 - \sigma(-x)
\end{align}

This is a usefeul identity allows for simplifications in various
mathematical expressions and facilitates gradient-based optimization in
machine learning models. Note that the function is differentiable, which
is crucial for backpropagation in neural networks.

In many fields, particularly in artificial neural networks, the term
sigmoid function usually refers to the logistic function, a specific
type of S-shaped, or ``sigmoid,'' curve. While other functions like the
Gompertz curve or the ogee curve may appear similar because they share
the same general S-shape, these are distinct mathematical functions with
unique properties and applications.

A sigmoid function is characterized by its shape: it begins at a low
value, then increases rapidly in the middle, and finally levels off at a
high value. The logistic function, which is a common example, maps any
real number (from negative to positive infinity) to an output between 0
and 1. This makes it especially useful when working with probabilities
or values that need to be constrained within this range.

Other variations of sigmoid functions, like the hyperbolic tangent
function (tanh ⁡ tanh), also share the S-shaped curve but differ in their
output ranges. The hyperbolic tangent function, for example, produces
outputs between -1 and 1, which may be more suitable in cases where
negative values are meaningful (such as for representing balanced or
symmetric outcomes).

\hypertarget{general-overview-of-applications-of-sigmoid-functions}{%
\section{General Overview of Applications of Sigmoid
Functions}\label{general-overview-of-applications-of-sigmoid-functions}}

\begin{itemize}
\item
  Artificial Neural Networks: Sigmoid functions are widely used as
  activation functions, which means they determine whether a neuron in a
  neural network should be ``activated'' (i.e., pass on its signal). In
  particular, the logistic function is popular because it maps values to
  a range between 0 and 1, simulating the all-or-nothing activation seen
  in biological neurons. This behavior allows neural networks to make
  decisions in a gradual, continuous manner.
\item
  Cumulative Distribution Functions in Statistics: The sigmoid function
  is also useful in statistics, where it can represent the cumulative
  distribution function (CDF) of a probability distribution. In this
  context, the function's smooth, continuous increase models the
  accumulation of probability from one extreme to another, representing
  how likely it is to observe values up to a certain point.
\item
  Probabilistic Interpretation: When the output of the logistic sigmoid
  is interpreted as a probability (from 0 to 1), it enables the function
  to model binary classification problems, where there are only two
  possible outcomes (such as yes/no or true/false). By setting a
  threshold (e.g., 0.5), predictions can be made based on the
  probability that an input belongs to a certain class.
\item
  Invertibility: The logistic sigmoid function is invertible, meaning it
  has an inverse function called the logit function. The logit function
  transforms probabilities (between 0 and 1) back into raw values from
  all real numbers, which can be useful in logistic regression and other
  statistical models that seek to model relationships in probability
  terms.
\end{itemize}

In summary, sigmoid functions, particularly the logistic function, are
versatile tools in both machine learning and statistics due to their
S-shape, bounded output range, and smooth transition, which makes them
ideal for modeling gradual changes in various contexts.

\hypertarget{application-to-machine-learning-sigmoid-function-in-binary-classification}{%
\section{Application to Machine Learning: Sigmoid Function in Binary
Classification}\label{application-to-machine-learning-sigmoid-function-in-binary-classification}}

In binary classification tasks, the sigmoid function is particularly
valuable as it can be used to estimate the probability that a given
input belongs to a certain class. Logistic regression, for example, uses
the sigmoid function to model the probability of a binary outcome, with
predictions being based on whether the probability exceeds a specified
threshold (e.g., 0.5). By mapping outputs to a range between 0 and 1,
the sigmoid function enables interpretable probability scores and smooth
gradients for optimization.

\hypertarget{how-sigmoid-works-in-logistic-regression}{%
\section{How Sigmoid Works in Logistic
Regression}\label{how-sigmoid-works-in-logistic-regression}}

In logistic regression, the sigmoid function serves as the
``activation'' that transforms the output of a linear model into a
probability.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Linear Combination of Features}: First, we calculate a linear
  combination of the input features. For example, in a logistic
  regression model with two features \(x_1\) and \(x_2\), the model
  calculates:

  \begin{align}
  z = w_1 x_1 + w_2 x_2 + b
  \end{align}

  where \(w_1\) and \(w_2\) are weights learned from the data, and \(b\)
  is the bias term. This value \(z\) represents a ``score'' that is not
  yet bounded between 0 and 1.
\item
  \textbf{Applying the Sigmoid Transformation}: Next, we pass this
  linear combination \(z\) through the sigmoid function, which
  transforms it into a probability:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \end{align}

  This output \(\sigma(z)\) now represents the estimated probability
  that the input belongs to the positive class (e.g., ``spam'' or
  ``yes'').
\item
  \textbf{Thresholding for Classification}: Once we have the
  probability, we can decide on a threshold to classify the input. For
  example, if we set a threshold of 0.5, we would classify the input as
  belonging to the positive class if \(\sigma(z) \geq 0.5\), and to the
  negative class otherwise. This threshold is often set based on the
  problem's specific requirements and the balance of classes.
\end{enumerate}

\hypertarget{example-walkthrough}{%
\section{Example Walkthrough}\label{example-walkthrough}}

Imagine you're building a model to detect spam emails based on certain
features, like the presence of certain words or the frequency of
punctuation marks. Here's how the process might work step-by-step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Input Features}: Let's say we have two features:

  \begin{itemize}
  \tightlist
  \item
    \(x_1\): The frequency of the word ``free.''
  \item
    \(x_2\): The number of exclamation marks in the email.
  \end{itemize}
\item
  \textbf{Model Weights}: After training on a dataset, the model might
  assign the following weights:

  \begin{itemize}
  \tightlist
  \item
    \(w_1 = 1.2\) (suggesting ``free'' is an indicator of spam)
  \item
    \(w_2 = 0.8\) (suggesting a high number of exclamation marks also
    correlates with spam)
  \end{itemize}

  Let's assume the bias term \(b\) is -1.5.
\item
  \textbf{Calculate \(z\)}: For an email where \(x_1 = 2\) and
  \(x_2 = 3\) (meaning ``free'' appears twice and there are three
  exclamation marks), we calculate:

  \begin{align}
  z = (1.2 \times 2) + (0.8 \times 3) - 1.5 = 2.4 + 2.4 - 1.5 = 3.3
  \end{align}
\item
  \textbf{Apply Sigmoid to Get Probability}: Now, we pass \(z\) through
  the sigmoid function:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-3.3}} \approx 0.964
  \end{align}

  This probability, 0.964, indicates a high likelihood that the email is
  spam.
\item
  \textbf{Classification Decision}: If our threshold is 0.5, we would
  classify this email as ``spam'' since 0.964 \textgreater{} 0.5. This
  probability-based approach allows us to adjust the threshold based on
  how conservative or lenient we want our classification to be.
\end{enumerate}

\hypertarget{why-sigmoid-is-useful-for-optimization}{%
\section{Why Sigmoid is Useful for
Optimization}\label{why-sigmoid-is-useful-for-optimization}}

The sigmoid function also has smooth gradients, meaning that even small
changes in \(z\) lead to gradual changes in the output probability
\(\sigma(z)\). This smoothness is critical for optimization because it
allows gradient-based algorithms, such as gradient descent, to make
small, continuous updates to model parameters.

To demonstrate the smooth gradient of the sigmoid function
mathematically, we can derive its derivative, which shows how the output
changes with respect to changes in the input \(z\). This derivative will
show that the sigmoid function has a continuous and smooth gradient,
making it suitable for gradient-based optimization methods.

\hypertarget{smooth-gradient-of-the-sigmoid-function}{%
\subsection{Smooth Gradient of the Sigmoid
Function}\label{smooth-gradient-of-the-sigmoid-function}}

To understand why the sigmoid function has smooth gradients, let's
derive its first derivative. We can calculate the derivative of
\(\sigma(z)\) with respect to \(z\), which will tell us how sensitive
the output \(\sigma(z)\) is to small changes in \(z\).

\hypertarget{deriving-the-derivative-of-the-sigmoid-function}{%
\section{Deriving the Derivative of the Sigmoid
Function}\label{deriving-the-derivative-of-the-sigmoid-function}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with the sigmoid function:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \end{align}
\item
  To find \(\frac{d\sigma}{dz}\), we apply the chain rule. Let's rewrite
  the function in a form that will make it easier to differentiate:

  \begin{align}
  \sigma(z) = \left(1 + e^{-z}\right)^{-1}
  \end{align}
\item
  Now, differentiate with respect to \(z\):

  \begin{align}
  \frac{d\sigma}{dz} = -\left(1 + e^{-z}\right)^{-2} \cdot (-e^{-z})
  \end{align}
\item
  Simplify the expression:

  \begin{align}
  \frac{d\sigma}{dz} = \frac{e^{-z}}{\left(1 + e^{-z}\right)^2}
  \end{align}
\item
  Notice that we can rewrite \(e^{-z}\) in terms of \(\sigma(z)\)
  because:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-z}} \Rightarrow e^{-z} = \frac{1 - \sigma(z)}{\sigma(z)}
  \end{align}
\item
  Substitute this back to express \(\frac{d\sigma}{dz}\) in terms of
  \(\sigma(z)\):

  \begin{align}
  \frac{d\sigma}{dz} = \sigma(z)(1 - \sigma(z))
  \end{align}
\end{enumerate}

\hypertarget{interpretation-of-the-derivative}{%
\subsection{Interpretation of the
Derivative}\label{interpretation-of-the-derivative}}

The derivative of the sigmoid function,
\(\frac{d\sigma}{dz} = \sigma(z)(1 - \sigma(z))\), shows us that:

\begin{itemize}
\tightlist
\item
  The gradient is always positive and smooth for all values of \(z\).
\item
  This derivative is largest around \(z = 0\), where the sigmoid curve
  is steepest, and it becomes smaller as \(z\) moves towards positive or
  negative infinity, where the curve flattens out.
\end{itemize}

This smooth gradient is crucial for gradient descent, as it allows the
algorithm to make gradual updates to the weights. Because the derivative
never suddenly changes, the model can adjust parameters smoothly without
sudden jumps, aiding in stable and effective learning.

Thus, the sigmoid's smooth gradient makes it well-suited for
optimization in machine learning.

\hypertarget{typical-ranges-or-values-for-the-sigmoid-function}{%
\section{Typical Ranges or Values for the Sigmoid
Function}\label{typical-ranges-or-values-for-the-sigmoid-function}}

The sigmoid function is bounded by 0 and 1. At \(x = 0\), \(\sigma(x)\)
returns 0.5. As \(x \to \infty\), \(\sigma(x) \to 1\); and as
\(x \to -\infty\), \(\sigma(x) \to 0\). The sigmoid function's
characteristic ``S''-shaped curve makes it particularly suited for
gradual transitions between classes in classification tasks.

\hypertarget{important-mathematical-identities}{%
\section{Important Mathematical
Identities}\label{important-mathematical-identities}}

Some useful identities associated with the sigmoid function include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Derivative of the sigmoid: \begin{align}
  \sigma'(x) = \sigma(x)(1 - \sigma(x))
  \end{align}
\item
  Relationship with the hyperbolic tangent: \begin{align}
  \sigma(x) = \frac{1 + \tanh\left(\frac{x}{2}\right)}{2}
  \end{align}
\end{enumerate}

These identities assist in understanding the behavior of the sigmoid
function in neural networks and other machine learning algorithms.

\newpage

\hypertarget{comparative-functions}{%
\section{Comparative Functions}\label{comparative-functions}}

Several functions exhibit similar ``S''-shaped or smooth transition
behaviors. Here are some of them:

\[
\begin{array}{ll}
-\operatorname{erf}\left(\frac{\sqrt{\pi}}{2} x\right) & -\frac{x}{\sqrt{1+x^2}} \\
-\tanh (x) & -\frac{2}{\pi} \arctan \left(\frac{\pi}{2} x\right) \\
-\frac{2}{\pi} \operatorname{gd}\left(\frac{\pi}{2} x\right) & -\frac{x}{1+|x|}
\end{array}
\]

In addition to the sigmoid function, there are several other functions
that exhibit smooth, ``S''-shaped curves and are used for various
applications in machine learning and mathematics. Below are six of these
comparative functions, along with a brief description of their unique
properties.

\hypertarget{error-function}{%
\subsection{Error Function:}\label{error-function}}

The error function, denoted as \(\operatorname{erf}(x)\), is often used
in probability and statistics, particularly in relation to the normal
distribution. It approximates the integral of the Gaussian function, and
its shape closely resembles that of the sigmoid. The error function has
applications in fields such as signal processing and heat diffusion.

\begin{align}
-\operatorname{erf}\left(\frac{\sqrt{\pi}}{2} x\right)
\end{align}

\hypertarget{reciprocal-square-root}{%
\subsection{Reciprocal Square Root:}\label{reciprocal-square-root}}

This function smooths inputs similarly to the sigmoid but has a
different asymptotic behavior. Unlike the sigmoid, which approaches 1
and 0, this function approaches -1 and 1 as \(x\) tends to \(\infty\)
and \(-\infty\), respectively. It is sometimes used in neural networks
as an alternative activation function.

\begin{align}
-\frac{x}{\sqrt{1+x^2}}
\end{align}

\hypertarget{hyperbolic-tangent}{%
\subsection{Hyperbolic Tangent:}\label{hyperbolic-tangent}}

The hyperbolic tangent function, \(\tanh(x)\), is a popular alternative
to the sigmoid in neural networks because its range is from -1 to 1,
allowing for more balanced gradient flows during backpropagation. It
provides output symmetry, which can sometimes lead to faster training.

\begin{align}
-\tanh(x)
\end{align}

\hypertarget{arctangent}{%
\subsection{Arctangent:}\label{arctangent}}

This function maps inputs to a range between -1 and 1 similarly to the
hyperbolic tangent but has a slightly different curve. The arctangent
function is sometimes used in machine learning models when smaller
gradients or more gradual transitions are preferred.

\begin{align}
\frac{2}{\pi} \arctan \left(\frac{\pi}{2} x\right)
\end{align}

\hypertarget{gudermannian-function}{%
\subsection{Gudermannian Function:}\label{gudermannian-function}}

The Gudermannian function, \(\operatorname{gd}(x)\), provides a link
between circular and hyperbolic functions without involving complex
numbers. This function has applications in geometry and physics,
particularly in mapping problems.

\begin{align}
\frac{2}{\pi} \operatorname{gd}\left(\frac{\pi}{2} x\right)
\end{align}

\hypertarget{inverse-absolute}{%
\subsection{Inverse Absolute:}\label{inverse-absolute}}

This function smoothly scales inputs, but its range only extends from -1
to 1. Its asymptotic properties are particularly useful when a smoother
transition around zero is preferred.

\begin{align}
\frac{x}{1+|x|}
\end{align}

\newpage

\begin{figure}[H]
    \centering
    % Left Image (Sigmoid)
    \begin{minipage}[t]{1.0\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/Sigmoids_comparisons.jpeg}
        \caption{Summary of Alternate Sigmoid Like Curves}
    \end{minipage}
    \label{fig:sigmoid-layout}
\end{figure}

Note: All functions are normalized in such a way that their slope at the
origin is 1

\newpage

\hypertarget{correlation}{%
\chapter{Correlation}\label{correlation}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $\text{Correlation} = \frac{\text{Cov}(X, Y)}{\text{Std}(X) \cdot \text{Std}(Y)}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-5-correlation.jpeg}
    \caption*{\Large $x_i$ and $y_i$ are linearly related}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}
\begin{a_def_eq}{Correlation}{def5} 

\begin{align}\label{r_XY}
\text{Correlation} = \frac{\text{Cov}(X, Y)}{\text{Std}(X) \cdot \text{Std}(Y)}\\
\end{align}
\begin{align}\label{r_pearson}
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{align}

\begin{description}[align=left, labelwidth=2.2cm, labelsep=0em, leftmargin=2.2cm]
    \item[$\text{Cov}(X, Y)$] The \textbf{covariance} between vectors \( X \) and \( Y \), which measures the degree to which the two variables (or vectors) vary together.
    \item[$\text{Std}(V)$] The \textbf{standard deviation of vector \( V \)}, which measures the spread of the elements of \( Y \)  around each mean.
    \item[$\bar{x}$] The \textbf{mean of variable \( X \)}, calculated as \( \frac{1}{n} \sum x_i \), where \( n \) is the number of data points.
    \item[$\bar{y}$] The \textbf{mean of variable \( Y \)}, calculated as \( \frac{1}{n} \sum y_i \), where \( n \) is the number of data points.
    \item[$x_i$] The \textbf{individual data point} in the variable \( X \), where \( i \) indexes each observation in the dataset.
    \item[$y_i$] The \textbf{individual data point} in the variable \( Y \), where \( i \) indexes each observation in the dataset.
\end{description}

\end{a_def_eq}

\hfill\break
\textbf{Introduction}: Correlation measures the strength and direction
of the linear relationship between two variables.

\textbf{Description}: It ranges from -1 to 1, where values close to 1 or
-1 indicate a strong relationship, and values close to 0 indicate no
relationship.

\textbf{Importance in ML}: Correlation analysis helps in feature
selection by identifying which features are related to the target
variable. Strongly correlated features can be used for better
predictions, whereas uncorrelated features can be removed to simplify
models.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-4}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-4}}

Correlation is a statistical measure that describes the strength and
direction of a relationship between two variables. The most common
measure of correlation is Pearson's correlation coefficient, denoted as
\(r\), which ranges from -1 to 1. A positive value indicates a direct
relationship, while a negative value indicates an inverse relationship.
A value of 0 implies no linear relationship.

The equation for Pearson's correlation r coefficient \ref{r_pearson} can
be compared to the normalized version \ref{r_mu}:

\begin{align}\label{r_mu}
r_{\mu} \frac{\sum \left(x_i y_i\right)}{\sqrt{\sum x_i^2 \sum y_i^2}}
\end{align}

Where: - \(x_i\) and \(y_i\) are individual data points of variables
\(x\) and \(y\), - \(\bar{x}\) and \(\bar{y}\) are the means of
variables \(x\) and \(y\), respectively.

\begin{itemize}
\item
  \textbf{Variables}: The two variables, \(x\) and \(y\), whose
  relationship we are measuring. These could be anything from economic
  indicators to user behaviors.
\item
  \textbf{Data Points}: Each pair of values \((x_i, y_i)\) represents an
  observation.
\item
  \textbf{Means}: The mean values of \(x\) and \(y\) are used to
  standardize the data by subtracting them from each data point, which
  helps in normalizing the relationship.
\item
  \(x_i\) and \(y_i\) are individual data points for the two variables
  \(X\) and \(Y\).
\item
  \(\bar{x}\) and \(\bar{y}\) are the means (averages) of the respective
  variables.
\item
  \textbf{Numerator}: The numerator is the \textbf{covariance} between
  \(X\) and \(Y\), which measures how the two variables vary together.
\item
  \textbf{Denominator}: The denominator is the product of the
  \textbf{standard deviations} of \(X\) and \(Y\), which normalizes the
  covariance and ensures that the correlation coefficient lies between
  -1 and 1.
\end{itemize}

The Pearson correlation measures the strength and direction of the
\textbf{linear relationship} between two variables. This is the most
commonly used form of correlation.

\hypertarget{use-case}{%
\subsubsection{Use case:}\label{use-case}}

This equation is used when you want to understand the \textbf{linear
relationship} between two continuous variables and is the formula used
by the \texttt{cor()} function in R with the default method
(\texttt{method\ =\ "pearson"}).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-in-interpretation}{%
\subsubsection{Difference in
Interpretation:}\label{difference-in-interpretation}}

\begin{itemize}
\tightlist
\item
  The pearson equation \ref{r_pearson} measures \textbf{how deviations
  from the mean of each variable are related}, i.e., it normalizes by
  the standard deviations of each variable.
\item
  The second equation \ref{r_mu} compares the raw products of the
  variables, but it does \textbf{not center} the data by subtracting the
  means. This makes the second equation more sensitive to the scales and
  magnitudes of the variables, which is why it's often less preferred
  for calculating correlation in its standard sense.
\end{itemize}

\hypertarget{key-differences}{%
\subsection{Key Differences:}\label{key-differences}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Normalization}:

  \begin{itemize}
  \tightlist
  \item
    The first equation involves centering the data by subtracting the
    means, which is important to account for the relationship of the
    variables in terms of deviations from their respective averages.
  \item
    The second equation does not involve centering the data. It uses the
    raw values of \(x_i\) and \(y_i\), making it sensitive to the
    overall scale and magnitude of the data.
  \end{itemize}
\item
  \textbf{Covariance vs Raw Product}:

  \begin{itemize}
  \tightlist
  \item
    The first equation is essentially computing a normalized
    \textbf{covariance} (divided by the product of the standard
    deviations of \(X\) and \(Y\)).
  \item
    The second equation uses the \textbf{raw product} of the variables,
    which lacks the standardization and may lead to different numerical
    results.
  \end{itemize}
\item
  \textbf{Interpretation}:

  \begin{itemize}
  \tightlist
  \item
    The first equation (Pearson's correlation) measures the strength of
    a \textbf{linear relationship}, normalized by the variance in both
    variables.
  \item
    The second equation is more general but is not commonly used as a
    standard for measuring correlation. It may appear in specific
    contexts, such as \textbf{cosine similarity} when the data is
    already standardized or when normalization is not necessary.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conclusion}{%
\subsection{Conclusion:}\label{conclusion}}

\begin{itemize}
\tightlist
\item
  \textbf{The first equation} \ref{r_peasron} is the \textbf{Pearson
  correlation coefficient}, the standard method for measuring linear
  relationships, normalized by the standard deviations of both
  variables.
\item
  \textbf{The second equation} \ref{r_mu} is a raw form that compares
  the products of the variables and is \textbf{not typically used} for
  Pearson's correlation, as it lacks normalization. It could be seen as
  a raw form of a \textbf{cosine similarity} (for vector comparison) or
  some other variant of correlation, but it's not generally used for
  typical correlation measurements.
\end{itemize}

\hypertarget{historical-context-1}{%
\section{5.3 Historical Context}\label{historical-context-1}}

The concept of correlation was first developed by Sir Francis Galton in
the 19th century. Galton's work laid the foundation for the field of
regression analysis and correlational statistics. The Pearson
correlation, developed by Karl Pearson in 1896, is the most widely used
form of correlation in modern statistics.

\hypertarget{understanding-the-notation}{%
\section{5.4 Understanding the
Notation}\label{understanding-the-notation}}

The formula for Pearson's correlation coefficient involves the concept
of covariance, which measures how much two variables change together.
The denominator in the formula standardizes the covariance by dividing
by the product of the standard deviations of \(x\) and \(y\).

\begin{itemize}
\tightlist
\item
  \textbf{Covariance}: Measures the degree to which two variables move
  together.
\item
  \textbf{Standard Deviation}: Measures the spread of a variable around
  its mean.
\end{itemize}

\hypertarget{applications-in-machine-learning-2}{%
\section{5.5 Applications in Machine
Learning}\label{applications-in-machine-learning-2}}

In machine learning, correlation is used to: - Identify relationships
between input features and target variables, - Check for
multicollinearity, where highly correlated features might skew results,
- Perform feature selection, as correlated features may provide
redundant information.

For instance, if two features are highly correlated, one may be removed
from the model to simplify it without losing predictive power.

\hypertarget{correlation-in-r}{%
\section{5.6 Correlation in R}\label{correlation-in-r}}

To compute the correlation coefficient in R, you can use the built-in
\texttt{cor()} function:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example R code to compute Pearson\textquotesingle{}s correlation coefficient}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{correlation }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(x, y)}
\CommentTok{\# print\{correlation\}}
\FunctionTok{cat}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 5 4 3 2 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(correlation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -1
\end{verbatim}

\normalsize

\newpage

\hypertarget{cosine-similarity}{%
\chapter{Cosine Similarity}\label{cosine-similarity}}

\[
\text{similarity} = \frac{A \cdot B}{\|A\| \|B\|}
\]

\textbf{Introduction}: Cosine similarity measures the cosine of the
angle between two vectors in a multi-dimensional space.

\textbf{Description}: It is commonly used to determine how similar two
documents are, regardless of their size, by comparing their word
vectors.

\textbf{Importance in ML}: Cosine similarity is crucial in text mining
and information retrieval applications. It is often used in clustering
and classification algorithms for textual data, where measuring the
similarity between vectors is needed.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{naive-bayes}{%
\chapter{Naive Bayes}\label{naive-bayes}}

\[
P(y | x_1, \ldots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i | y)}{P(x_1, \ldots, x_n)}
\]

\textbf{Introduction}: Naive Bayes is a probabilistic classifier based
on Bayes' theorem with strong independence assumptions.

\textbf{Description}: It assumes that the presence of a particular
feature in a class is unrelated to the presence of any other feature,
which simplifies computation.

\textbf{Importance in ML}: Naive Bayes is used for classification tasks,
especially in text classification and spam detection. Its simplicity and
efficiency make it a popular choice for high-dimensional datasets.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{maximum-likelihood-estimation-mle}{%
\chapter{Maximum Likelihood Estimation
(MLE)}\label{maximum-likelihood-estimation-mle}}

\[
\arg\max_{\theta} \prod_{i=1}^n P(x_i | \theta)
\]

\textbf{Introduction}: MLE is a method used to estimate the parameters
of a statistical model.

\textbf{Description}: It finds the parameter values that maximize the
likelihood of making the observations given the model.

\textbf{Importance in ML}: MLE is used to train many machine learning
models, including logistic regression and Gaussian mixture models. It
provides a way to fit models to data and make statistical inferences.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{ordinary-least-squares-ols}{%
\chapter{Ordinary Least Squares
(OLS)}\label{ordinary-least-squares-ols}}

\[
\hat{\beta} = (X^T X)^{-1} X^T y
\]

\textbf{Introduction}: OLS is a method for estimating the unknown
parameters in a linear regression model.

\textbf{Description}: It minimizes the sum of squared residuals between
the observed and predicted values.

\textbf{Importance in ML}: OLS is a fundamental technique for regression
analysis. It helps determine the best-fit line for the data, making it
useful for predictive modeling.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{f1-score}{%
\chapter{F1 Score}\label{f1-score}}

\[
F_1 = \frac{2 \cdot P \cdot R}{P + R}
\]

\textbf{Introduction}: The F1 score is a measure of a test's accuracy
that considers both precision (P) and recall (R).

\textbf{Description}: It is the harmonic mean of precision and recall,
providing a single metric to evaluate model performance.

\textbf{Importance in ML}: The F1 score is particularly useful for
imbalanced datasets where the distribution of classes is uneven,
providing a balanced measure of model effectiveness.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{relu-rectified-linear-unit}{%
\chapter{ReLU (Rectified Linear
Unit)}\label{relu-rectified-linear-unit}}

\[
\text{ReLU}(x) = \max(0, x)
\]

\textbf{Introduction}: ReLU is an activation function used in neural
networks.

\textbf{Description}: It outputs the input directly if positive;
otherwise, it outputs zero.

\textbf{Importance in ML}: ReLU is crucial for deep learning as it helps
mitigate the vanishing gradient problem, allowing models to learn faster
and perform better.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{softmax-function}{%
\chapter{Softmax Function}\label{softmax-function}}

\[
P(y = j | z) = \frac{e^{z w_j}}{\sum_{k=1}^K e^{z w_k}}
\]

\textbf{Introduction}: Softmax is an activation function that outputs a
probability distribution over multiple classes.

\textbf{Description}: It converts raw scores into probabilities, which
sum up to one, making it suitable for multi-class classification.

\textbf{Importance in ML}: Softmax is commonly used in the output layer
of neural networks for classification problems involving multiple
classes, making it essential for multi-class models.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{r-squared-r2-score}{%
\chapter{R-squared (R\^{}2) Score}\label{r-squared-r2-score}}

\[
R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\]

\textbf{Introduction}: R-squared is a statistical measure that
represents the proportion of variance in the dependent variable
explained by the independent variables.

\textbf{Description}: It is a goodness-of-fit measure that ranges from 0
to 1, with values closer to 1 indicating a better fit.

\textbf{Importance in ML}: R-squared is used to evaluate the performance
of regression models. It provides insights into how well the model
explains the variance in the target variable.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{mean-squared-error-mse}{%
\chapter{Mean Squared Error (MSE)}\label{mean-squared-error-mse}}

\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]

\textbf{Introduction}: Mean Squared Error is a common loss function used
to measure the average squared difference between predicted and actual
values.

\textbf{Description}: It calculates the squared difference for each
observation and then averages them to provide a metric of model
accuracy.

\textbf{Importance in ML}: MSE is used to evaluate the performance of
regression models. Lower MSE values indicate a better fit, making it
essential for identifying the optimal model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{mean-squared-error-with-l2-regularization-mse-l2-reg}{%
\chapter{Mean Squared Error with L2 Regularization (MSE + L2
Reg)}\label{mean-squared-error-with-l2-regularization-mse-l2-reg}}

\[
\text{MSE}_{\text{regularized}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p \beta_j^2
\]

\textbf{Introduction}: Regularized MSE adds a penalty to the loss
function to prevent overfitting by discouraging overly complex models.

\textbf{Description}: The regularization term \(\lambda \sum \beta_j^2\)
helps keep model parameters small, which leads to simpler models.

\textbf{Importance in ML}: L2 regularization is key in reducing
overfitting by controlling the complexity of the model, ensuring that
the model generalizes well to new data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{eigenvectors-and-eigenvalues}{%
\chapter{Eigenvectors and
Eigenvalues}\label{eigenvectors-and-eigenvalues}}

\[
A v = \lambda v
\]

\textbf{Introduction}: Eigenvectors and eigenvalues are fundamental
concepts in linear algebra used to understand the structure of matrices.

\textbf{Description}: They represent the directions along which linear
transformations act by only scaling the vectors, without changing their
direction.

\textbf{Importance in ML}: Eigenvectors and eigenvalues are used in
dimensionality reduction techniques like PCA (Principal Component
Analysis), which helps reduce the number of features while preserving
essential information.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{entropy}{%
\chapter{Entropy}\label{entropy}}

\[
\text{Entropy} = - \sum_{i} p_i \log_2(p_i)
\]

\textbf{Introduction}: Entropy is a measure of uncertainty or randomness
in a dataset.

\textbf{Description}: It quantifies the impurity in a dataset, making it
a key concept in information theory and decision trees.

\textbf{Importance in ML}: Entropy is used in decision tree algorithms
to decide the best split at each node by measuring the purity of a
dataset. Lower entropy indicates a more homogenous group of samples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{k-means-clustering}{%
\chapter{K-Means Clustering}\label{k-means-clustering}}

\[
\arg\min_S \sum_{k=1}^K \sum_{x \in S_k} \|x - \mu_k\|^2
\]

\textbf{Introduction}: K-Means is an unsupervised learning algorithm
used for clustering data into K distinct groups.

\textbf{Description}: It minimizes the sum of squared distances between
data points and the centroid of their assigned cluster.

\textbf{Importance in ML}: K-Means is a fundamental clustering technique
used in exploratory data analysis and segmentation tasks. It helps
discover patterns and relationships in unlabeled data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{kullback-leibler-kl-divergence}{%
\chapter{Kullback-Leibler (KL)
Divergence}\label{kullback-leibler-kl-divergence}}

\[
D_{KL}(P \| Q) = \sum_{x \in \chi} P(x) \log \frac{P(x)}{Q(x)}
\]

\textbf{Introduction}: KL Divergence is a measure of how one probability
distribution diverges from a second, reference probability distribution.

\textbf{Description}: It is commonly used to measure the difference
between two probability distributions.

\textbf{Importance in ML}: KL Divergence is used in machine learning for
loss functions in models like variational autoencoders. It quantifies
how well the learned distribution approximates the true data
distribution.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{log-loss}{%
\chapter{Log Loss}\label{log-loss}}

\[
-\frac{1}{N} \sum_{i=1}^N \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
\]

\textbf{Introduction}: Log Loss, or logistic loss, is a loss function
used for binary classification tasks.

\textbf{Description}: It measures the performance of a classification
model whose output is a probability value between 0 and 1.

\textbf{Importance in ML}: Log Loss is crucial in evaluating
classification models where the output is a probability. Lower log loss
indicates a more accurate model, especially for probabilistic
predictions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{support-vector-machine-svm-objective}{%
\chapter{Support Vector Machine (SVM)
Objective}\label{support-vector-machine-svm-objective}}

\[
\min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i (w \cdot x_i - b))
\]

\textbf{Introduction}: SVM is a supervised learning model used for
classification and regression tasks.

\textbf{Description}: It finds the hyperplane that best separates
different classes by maximizing the margin between them.

\textbf{Importance in ML}: SVMs are effective for high-dimensional
spaces and are used in classification problems where the decision
boundary is non-linear.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon
\]

\textbf{Introduction}: Linear regression is a simple and widely used
method for predictive analysis.

\textbf{Description}: It models the relationship between a dependent
variable and one or more independent variables by fitting a linear
equation to observed data.

\textbf{Importance in ML}: Linear regression is fundamental for
understanding relationships between variables and is used in predictive
modeling to estimate outcomes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{singular-value-decomposition-svd}{%
\chapter{Singular Value Decomposition
(SVD)}\label{singular-value-decomposition-svd}}

\[
A = U \Sigma V^T
\]

\textbf{Introduction}: SVD is a matrix factorization technique used in
linear algebra.

\textbf{Description}: It decomposes a matrix into three other matrices,
revealing the intrinsic structure of the data.

\textbf{Importance in ML}: SVD is used in dimensionality reduction, data
compression, and noise reduction. It is also a core algorithm behind
recommendation systems.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{lagrange-multiplier}{%
\chapter{Lagrange Multiplier}\label{lagrange-multiplier}}

\[
\max f(x) \;; \; g(x) = 0
\] \[
L(x, \lambda) = f(x) - \lambda \ast g(x)
\]

\textbf{Introduction}: The Lagrange multiplier is a method for finding
the local maxima and minima of a function subject to equality
constraints.

\textbf{Description}: It introduces a new variable, \(\lambda\), which
helps in optimizing a function while considering the constraint.

\textbf{Importance in ML}: Lagrange multipliers are used in optimization
problems with constraints, such as training machine learning models with
regularization terms.

\newpage

\hypertarget{the-human-equation}{%
\chapter{The Human Equation}\label{the-human-equation}}

\[
\textbf{HUMAN}
\]

\textbf{Introduction}:

\textbf{Description}:

\textbf{Importance in ML}:

\newpage

\backmatter

\hypertarget{epilogue---guidance-and-test-materials}{%
\chapter{Epilogue - Guidance and Test
Materials}\label{epilogue---guidance-and-test-materials}}

\hypertarget{applying-musk-rules-to-writing-your-book-on-the-25-key-equations-in-machine-learning}{%
\section*{Applying Musk Rules to Writing Your Book on the ``25 Key
Equations in Machine
Learning''}\label{applying-musk-rules-to-writing-your-book-on-the-25-key-equations-in-machine-learning}}
\addcontentsline{toc}{section}{Applying Musk Rules to Writing Your Book
on the ``25 Key Equations in Machine Learning''}

\hypertarget{always-question-the-requirements}{%
\subsection*{\texorpdfstring{1. \textbf{Always Question the
Requirements}}{1. Always Question the Requirements}}\label{always-question-the-requirements}}
\addcontentsline{toc}{subsection}{1. \textbf{Always Question the
Requirements}}

\begin{itemize}
\tightlist
\item
  \textbf{Book Scope and Audience:} Reevaluate the core purpose and
  audience of the book. Who is your ideal reader? Do you really need to
  cover 25 equations, or would a different number be more impactful? Is
  there any unnecessary content that isn't truly essential to your goal
  of effective teaching and reference?
\item
  \textbf{Key Questions to Ask:} Do each of the sections contribute to
  the core learning objectives? Could some be combined, shortened, or
  presented differently to be clearer and more engaging?
\end{itemize}

\hypertarget{try-very-hard-to-delete-parts-or-processes}{%
\subsection*{\texorpdfstring{2. \textbf{Try Very Hard to Delete Parts or
Processes}}{2. Try Very Hard to Delete Parts or Processes}}\label{try-very-hard-to-delete-parts-or-processes}}
\addcontentsline{toc}{subsection}{2. \textbf{Try Very Hard to Delete
Parts or Processes}}

\begin{itemize}
\tightlist
\item
  \textbf{Delete Unnecessary Content:} Go through each chapter and
  decide if every subsection or detail is truly required. Are there
  parts of the explanations, historical contexts, or code examples that
  could be condensed or cut without losing value?
\item
  \textbf{Focus on Core Concepts:} Sometimes, less is more. Identify and
  focus on the key insights of each equation. For example, rather than
  going deep into every historical development of the normal
  distribution, focus on how the normal distribution is applied directly
  in machine learning.
\end{itemize}

\hypertarget{simplify-or-optimize-but-not-too-early}{%
\subsection*{\texorpdfstring{3. \textbf{Simplify or Optimize, But Not
Too
Early}}{3. Simplify or Optimize, But Not Too Early}}\label{simplify-or-optimize-but-not-too-early}}
\addcontentsline{toc}{subsection}{3. \textbf{Simplify or Optimize, But
Not Too Early}}

\begin{itemize}
\tightlist
\item
  \textbf{Simplification After Completion:} In the initial phase, write
  freely to capture all your thoughts and ideas. Don't worry too much
  about the length or complexity. Once you have a draft, then
  simplify---remove verbose explanations, simplify complex code
  snippets, or summarize complex derivations to keep it engaging.
\item
  \textbf{Iteration in Drafts:} Write each chapter in an exploratory way
  first, then optimize the language and reduce complexity in subsequent
  drafts.
\end{itemize}

\hypertarget{move-faster}{%
\subsection*{\texorpdfstring{4. \textbf{Move
Faster}}{4. Move Faster}}\label{move-faster}}
\addcontentsline{toc}{subsection}{4. \textbf{Move Faster}}

\begin{itemize}
\tightlist
\item
  \textbf{Accelerate Writing with LLMs and Tools:} Use tools like large
  language models (LLMs) to help generate content, summarize, and write
  initial drafts faster. You've already indicated using Mathpix, LaTeX,
  and R Markdown---lean into these tools to keep your process fast and
  flexible.
\item
  \textbf{Set Short-Term Goals:} Use milestones to maintain momentum.
  Instead of working on all 25 chapters at once, work on drafting 5
  chapters in a week. Use techniques like ``pomodoro'' to keep the pace
  fast without burnout.
\end{itemize}

\hypertarget{finally-automate}{%
\subsection*{\texorpdfstring{5. \textbf{Finally:
Automate}}{5. Finally: Automate}}\label{finally-automate}}
\addcontentsline{toc}{subsection}{5. \textbf{Finally: Automate}}

\begin{itemize}
\tightlist
\item
  \textbf{Automate Repetitive Tasks Last:} Once content is ready,
  automate formatting and typesetting. Given that you are using R
  Markdown and LaTeX, continue leveraging those tools for repetitive
  typesetting tasks, but do this after you've streamlined the core
  content.
\item
  \textbf{Automation for Consistency:} Automate consistency checks for
  formulas and code. This could be done with scripts that verify
  equation formatting, syntax checking for code snippets, or tools that
  flag unformatted variables.
\end{itemize}

\hypertarget{specific-strategies-to-speed-up-the-book-creation}{%
\subsection*{\texorpdfstring{\textbf{Specific Strategies to Speed Up the
Book
Creation:}}{Specific Strategies to Speed Up the Book Creation:}}\label{specific-strategies-to-speed-up-the-book-creation}}
\addcontentsline{toc}{subsection}{\textbf{Specific Strategies to Speed
Up the Book Creation:}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Outline Consolidation:}

  \begin{itemize}
  \tightlist
  \item
    Review your current outline and condense or merge sections that
    overlap. Consider if all 25 equations deserve equal weight. You
    could provide detailed explanations for 10-15 equations while giving
    more general summaries for the rest if they are less foundational.
  \end{itemize}
\item
  \textbf{Minimize and Prioritize:}

  \begin{itemize}
  \tightlist
  \item
    Focus on the most impactful content. Given the target audience, ask
    yourself: do they need a historical deep dive, or is the practical
    application what matters most? Emphasize applications, examples, and
    visuals over long theoretical discussions unless absolutely
    necessary.
  \end{itemize}
\item
  \textbf{Visuals and Examples:}

  \begin{itemize}
  \tightlist
  \item
    To make learning engaging, visuals and practical examples are key.
    Automate the creation of code snippets and figures that are
    repetitive. Tools like R and Python scripts can automatically
    generate the visuals needed for each equation, saving considerable
    time.
  \end{itemize}
\item
  \textbf{Leverage Existing Content:}

  \begin{itemize}
  \tightlist
  \item
    Since you are creating this as a reference book, consider using
    existing open-source implementations of the concepts as examples,
    rather than creating everything from scratch. This will save time
    while still providing value.
  \end{itemize}
\item
  \textbf{Peer Review and Feedback Early:}

  \begin{itemize}
  \tightlist
  \item
    Incorporate early feedback to streamline revisions. Share your draft
    with a small group of readers and ask for their feedback on clarity
    and structure. This will help you catch issues early, saving time
    during the editing phase.
  \end{itemize}
\item
  \textbf{Use a Versioned Approach:}

  \begin{itemize}
  \tightlist
  \item
    Set smaller, achievable versions (like v0.3, v0.4, etc.) where each
    iteration refines content. For example, v0.3 might be the initial
    full draft, v0.4 could focus on reducing complexity, and v0.5 could
    involve formatting and visual consistency. This will help you move
    in structured steps rather than getting bogged down in perfecting
    each detail upfront.
  \end{itemize}
\end{enumerate}

\hypertarget{moving-faster-in-practice}{%
\subsection*{Moving Faster, in
Practice:}\label{moving-faster-in-practice}}
\addcontentsline{toc}{subsection}{Moving Faster, in Practice:}

\begin{itemize}
\tightlist
\item
  \textbf{Sprint Write Sections:} Work on chapters in sprints. Set a
  specific time limit for each chapter, such as three days per equation.
  The goal is to create momentum and avoid stagnation.
\item
  \textbf{Lean on a Support System:} Delegate part of the process. If
  possible, outsource parts like copy-editing, formatting, or even
  generating diagrams. The faster you move past the basic draft stage,
  the better you can refine and make the book truly impactful.
\end{itemize}

\hypertarget{conclusion-1}{%
\subsection*{Conclusion:}\label{conclusion-1}}
\addcontentsline{toc}{subsection}{Conclusion:}

The Musk Rules encourage challenging the purpose of everything you're
including, focusing on essential content, and using tools and automation
wisely, but only after the core content is strong. By questioning,
deleting, simplifying, moving fast, and automating thoughtfully, you can
accelerate the process of creating your book without compromising
quality. Focus on the main principles that will make your book a
genuinely valuable reference---clear, effective, and engaging
explanations, visualizations, and examples---while avoiding unnecessary
details that slow you down.

How do these recommendations feel in the context of your goals for this
book? Would you like help refining a specific section or strategy for
moving faster?

\newpage

\hypertarget{temp-_-r-code---python-code---test-area}{%
\section{TEMP \_ R code - Python code - test
area}\label{temp-_-r-code---python-code---test-area}}

\hypertarget{testing-python-integration-temp-section---to-remove-later}{%
\section{testing Python Integration (temp section - to remove
later)}\label{testing-python-integration-temp-section---to-remove-later}}

Create a variable \texttt{x} in the Python session:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Access the Python variable \texttt{x} in an R code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{py}\SpecialCharTok{$}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

Create a new variable \texttt{y} in the Python session using R, and pass
a data frame to \texttt{y}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{py}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

Print the variable \texttt{y} in Python:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## {'speed': [4.0, 4.0, 7.0, 7.0, 8.0, 9.0], 'dist': [2.0, 10.0, 4.0, 22.0, 16.0, 10.0]}
\end{verbatim}

\newpage

\hypertarget{define-numbers-in-r}{%
\section{Define Numbers in R}\label{define-numbers-in-r}}

Let's define the numbers we will use in both R and Python:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a vector of numbers in R}
\NormalTok{define\_numbers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-code-block}{%
\section{R Code Block}\label{r-code-block}}

This is a simple R code block that calculates the sum of 5 numbers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# R code to sum 5 numbers}
\NormalTok{r\_sum }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(define\_numbers)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"The sum of the numbers in R is:"}\NormalTok{, r\_sum))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "The sum of the numbers in R is: 15"
\end{verbatim}

\hypertarget{python-code-to-pass}{%
\section{Python Code To Pass *}\label{python-code-to-pass}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{py}\SpecialCharTok{$}\NormalTok{n2 }\OtherTok{\textless{}{-}}\NormalTok{ define\_numbers}
\end{Highlighting}
\end{Shaded}

\hypertarget{python-code-block}{%
\section{Python Code Block}\label{python-code-block}}

This is a simple Python code block that calculates the sum of 5 numbers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Python code to calculate the sum of numbers defined in R}

\CommentTok{\# Define a list of numbers}
\NormalTok{numbers }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}

\CommentTok{\# Retrieve the numbers passed from R using reticulate\textquotesingle{}s \textquotesingle{}py\textquotesingle{} object}
\NormalTok{total\_sum }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(numbers)}
\NormalTok{total\_sum2 }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(n2)}

\CommentTok{\# Print the result}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The sum of the numbers \textquotesingle{}total sum\textquotesingle{} is: }\SpecialCharTok{\{}\NormalTok{total\_sum}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The sum of the numbers 'total sum' is: 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The sum of the numbers \textquotesingle{}n2\textquotesingle{} is: }\SpecialCharTok{\{}\NormalTok{total\_sum2}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The sum of the numbers 'n2' is: 15.0
\end{verbatim}

\newpage

\listoffigures
\lstlistoflistings

\newpage

\hypertarget{galley-sheet}{%
\chapter*{Galley Sheet}\label{galley-sheet}}
\addcontentsline{toc}{chapter}{Galley Sheet}

\hypertarget{galley-sheet-info}{%
\section{\texorpdfstring{\textbf{GALLEY Sheet
info}}{GALLEY Sheet info}}\label{galley-sheet-info}}

\layout

\hypertarget{galley-sheet-info---ends}{%
\section{\texorpdfstring{\textbf{GALLEY Sheet info -
ends}}{GALLEY Sheet info - ends}}\label{galley-sheet-info---ends}}

\newpage

\hypertarget{references}{%
\chapter{References}\label{references}}

\backmatter
\end{document}
