

# Chapter 7: Cosine Similarity and Its Mathematical Foundations

## 7.1 Introduction to Cosine Similarity

Cosine similarity is a measure used to compute the **cosine of the angle** between two non-zero vectors in an inner product space. It is commonly used to measure the similarity between documents in text mining, recommendation systems, and machine learning tasks. The basic idea is to calculate how similar two vectors are, regardless of their magnitudes, by comparing their directions.

The formula for cosine similarity between two vectors \( \mathbf{A} \) and \( \mathbf{B} \) is:
  
  $$
  \text{similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\| \mathbf{A} \| \| \mathbf{B} \|}
$$
  
  Where:
  - \( \mathbf{A} \cdot \mathbf{B} \) is the **dot product** of vectors \( \mathbf{A} \) and \( \mathbf{B} \),
- \( \| \mathbf{A} \| \) and \( \| \mathbf{B} \| \) are the **norms** (or magnitudes) of the vectors \( \mathbf{A} \) and \( \mathbf{B} \), respectively.

Cosine similarity ranges from -1 to 1:
  - \( 1 \) indicates that the vectors are identical (i.e., they point in the same direction),
- \( 0 \) indicates orthogonality (no similarity),
- \( -1 \) indicates that the vectors are diametrically opposed (pointing in opposite directions).

### 7.2 Cosine Similarity and Vector Norms

Cosine similarity is closely related to the concept of **vector norms**, especially the **Euclidean norm** (L2 norm). 
To understand how cosine similarity works, lets first explore how it is connected to these norms:

#### 7.2.1 L2 Norm (Euclidean Distance)

The **L2 norm** (also known as the Euclidean norm) of a vector \( \mathbf{A} \) is defined as:

$$
\| \mathbf{A} \|_2 = \sqrt{\sum_{i=1}^{n} A_i^2}
$$

Where \( A_i \) represents the components of the vector \( \mathbf{A} \). This norm measures the "length" or magnitude of the vector \( \mathbf{A} \). In the context of cosine similarity, the L2 norm is used to **normalize** the vectors \( \mathbf{A} \) and \( \mathbf{B} \), ensuring that the similarity measure reflects the direction of the vectors, not their magnitude.

#### 7.2.2 Relationship Between Cosine Similarity and L2 Norm

The formula for cosine similarity can be viewed as a **normalized dot product**:

$$
\text{similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\| \mathbf{A} \|_2 \| \mathbf{B} \|_2}
$$

If both vectors are unit vectors (i.e., \( \| \mathbf{A} \|_2 = \| \mathbf{B} \|_2 = 1 \)), then the cosine similarity reduces to the **dot product** of the vectors:

$$
\text{similarity}(\mathbf{A}, \mathbf{B}) = \mathbf{A} \cdot \mathbf{B}
$$

This highlights that **cosine similarity is a normalized measure of the dot product** between vectors. The normalization ensures that the magnitude of the vectors does not influence the similarity score.

#### 7.2.3 Cosine Similarity is Not a Norm

Although cosine similarity involves the concept of norms (specifically the L2 norm), **cosine similarity itself is not a norm**. A norm requires the following properties:
- **Non-negativity**: \( \|\mathbf{A}\| \geq 0 \), with equality if and only if \( \mathbf{A} = 0 \),
- **Homogeneity**: \( \|\alpha \mathbf{A}\| = |\alpha| \|\mathbf{A}\| \) for any scalar \( \alpha \),
- **Triangle inequality**: \( \|\mathbf{A} + \mathbf{B}\| \leq \|\mathbf{A}\| + \|\mathbf{B}\| \).

Cosine similarity does not satisfy the **triangle inequality** and therefore does not qualify as a norm. It is a **similarity measure** that reflects how aligned two vectors are in terms of their direction, rather than their magnitude.

### 7.3 Cosine Similarity as a Distance Metric

Although cosine similarity is not a norm, we can define **cosine distance**, which is a **distance metric**:

$$
\text{Cosine Distance}(\mathbf{A}, \mathbf{B}) = 1 - \text{similarity}(\mathbf{A}, \mathbf{B})
$$

Cosine distance is a valid distance measure because it satisfies the properties of a **metric**:
- Non-negativity: \( \text{Cosine Distance}(\mathbf{A}, \mathbf{B}) \geq 0 \),
- Identity of indiscernibles: \( \text{Cosine Distance}(\mathbf{A}, \mathbf{B}) = 0 \) if and only if \( \mathbf{A} = \mathbf{B} \),
- Symmetry: \( \text{Cosine Distance}(\mathbf{A}, \mathbf{B}) = \text{Cosine Distance}(\mathbf{B}, \mathbf{A}) \),
- Triangle inequality: This property holds for cosine distance, making it a valid metric.

### 7.4 The Role of Cosine Similarity in Machine Learning and Text Mining

Cosine similarity is widely used in **text mining** and **information retrieval** tasks because it provides a robust and scalable method for comparing documents and finding similarities in high-dimensional spaces. Here are a few key applications:

#### 7.4.1 Text Classification and Clustering

In **text classification**, cosine similarity can be used to compare a new document against a set of predefined class vectors, allowing the system to assign a class label based on the closest match. Similarly, in **document clustering**, cosine similarity is used to group documents with similar content into clusters.

#### 7.4.2 Latent Semantic Analysis (LSA)

Cosine similarity is an essential component of **Latent Semantic Analysis (LSA)**, which reduces the dimensionality of text data while preserving its semantic structure. By applying **Singular Value Decomposition (SVD)** to a term-document matrix, LSA captures latent semantic relationships between terms, and cosine similarity is used to measure the similarity between documents in this reduced space.

#### 7.4.3 Word Embeddings

With the rise of **word embeddings** like **Word2Vec**, **GloVe**, and **FastText**, cosine similarity has become an important tool in measuring the **semantic similarity** between words and phrases. Word embeddings map words into continuous vector spaces, and cosine similarity is used to measure the cosine of the angle between these word vectors. Words with similar meanings have high cosine similarity, even if they are not identical.

#### 7.4.4 Semantic Search and Information Retrieval

Cosine similarity is a cornerstone of **semantic search**. Modern search engines and information retrieval systems use cosine similarity to compare the query vector with document vectors, returning results based on both **semantic meaning** and **word overlap**, rather than just keyword matching. This makes search results more contextually relevant.

#### 7.4.5 Recommendation Systems

In **collaborative filtering**, cosine similarity is used to measure the similarity between users or items based on their behavior or preferences. By calculating the cosine similarity between usersâ€™ preferences or item ratings, the system can recommend items that are similar to those the user has liked in the past.

### 7.5 Connections with Other Mathematical Concepts

#### 7.5.1 Cosine Similarity and Euclidean Distance

Cosine similarity is closely related to **Euclidean distance (L2 norm)**, as both can be used to measure similarity or dissimilarity between vectors. Interestingly, cosine similarity and Euclidean distance are connected by the following formula:

$$
\text{Cosine Similarity} = 1 - \frac{\|\mathbf{A} - \mathbf{B}\|_2^2}{\|\mathbf{A}\|_2^2 + \|\mathbf{B}\|_2^2}
$$

This identity shows that cosine similarity can be interpreted in terms of the squared Euclidean distance between two vectors. When cosine similarity is high, the Euclidean distance is low, indicating that the vectors are close in terms of their direction.

#### 7.5.2 Cosine Similarity and Kernel Methods

In the context of **kernel-based methods**, such as **Support Vector Machines (SVM)**, cosine similarity is often used as a **linear kernel**. When working with vectorized data, the cosine of the angle between vectors in high-dimensional space can be interpreted as the kernel function, which measures the similarity between data points.

---

## 7.6 Conclusion

Cosine similarity is a versatile and powerful measure that is deeply tied to the concepts of vector

