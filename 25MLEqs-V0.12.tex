% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12 pt,
  a4paper,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin = 0.9in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\usepackage{lipsum}         % Generates filler text  

\usepackage{tikz}
\usetikzlibrary{
  trees, graphs, arrows.meta, calc, chains, 
  quotes, positioning, shapes.geometric,
  arrows.meta, shapes,backgrounds }
\usepackage{tikz-3dplot}

\usepackage{amsmath}

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\newcommand{\docTitle}{25 ML Fundamental Equations}

% ensures no page fill:
\raggedbottom

% setup special chapter only counter
\newcounter{chonlycounter}
\renewcommand{\thechonlycounter}{\thechapter}

% setup some special branding fonts 
% (this is only used at start of document - for product names )
\usepackage{fontspec}
\usepackage{textcomp}

% Poppins Font
\newfontface\poppins{Poppins}[Path=fonts/, UprightFont=*-Regular, BoldFont=*-Bold]
\newcommand{\textpoppins}[1]{{\poppins #1}}

% FluxRegular Font
\newfontface\fluxreg{FluxRegular}[Path=fonts/,UprightFont=*.otf]
\newcommand{\textfluxreg}[1]{{\fluxreg #1}}

% Lato Font
\newfontface\lato{Lato}[Path=fonts/, UprightFont=*-Regular]
\newcommand{\textlato}[1]{{\lato #1}}

% SourceSans Font
\newfontface\ssp{SourceSansPro}[Path=fonts/, UprightFont=*-Regular]
\newcommand{\textssp}[1]{{\ssp #1}}

% Courier New Font
\newfontface\couriernew{Courier New}
\newcommand{\textcouriernew}[1]{{\couriernew #1}}

% Arial Font
\newfontface\arial{Arial}
\newcommand{\textarial}[1]{{\arial #1}}

% Plantino
\newfontface\palatino{Palatino}
\newcommand{\textpalatino}[1]{{\palatino #1}}

\usepackage{rotating}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{listings}
% make code block listings specific
\renewcommand{\lstlistingname}{Code Listing}

\usepackage{booktabs}
% width formatting on page
\usepackage{changepage}       

% custom colors
\definecolor{dblu}  {HTML}{002db3}
\definecolor{cyan}  {HTML}{99ccff}
\definecolor{ltorng}{HTML}{99ccff}
\definecolor{ltgrn }{HTML}{002db3}

\usepackage{titling}

% margin methods - to adjust margins
% for margins setting -> employ:"\layout"
\usepackage{layout}       
\usepackage{multicol}  
\usepackage{geometry} 

% Set asymmetric page margins (book formatting)
\geometry{
  inner=0.75in,         % Inner margin
  outer=1.0in,          % Outer margin
  bottom=0.6in,         % Bottom margin
  top=0.6in,            % Bottom margin
  includeheadfoot }

% math fonts and coloring
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}        % specific symbols
\usepackage{mathrsfs}
\usepackage{xcolor}         % For coloring text
\usepackage{graphicx}

\definecolor{impactblue}{RGB}{30, 144, 255}
\definecolor{royalimpactblue}{RGB}{65, 105, 225} % Royal Blue, more vivid
\definecolor{navyimpactblue}{RGB}{0, 0, 128} %  avy Blue tone 
 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{float}

% so that weblinks are colored bluw
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

\usepackage{enumitem}
\usepackage{varwidth}
\usepackage[nochapter, tocentry, owncaptions, tablegrid]{vhistory}
\usepackage{tasks}
\usepackage{scrlayer-scrpage}
\numberwithin{equation}{section}
\usepackage{lastpage}
\captionsetup[figure]{font=small,labelfont=small}

% Regular Theorems, definitions, lemmas, proofs
\theoremstyle{plain}      % from `amsthm'
\newtheorem{theorem}{Theorem}

\theoremstyle{definition} % from `amsthm'
\newtheorem{defn}{Definition}

\newtheorem{xmpl}{Example}% from `amsthm'

\theoremstyle{remark}     % from `amsthm'
\newtheorem{remark}{Remark}

\newtheorem{algorithm}{Algorithm} 
\newtheoremstyle{note}    % style name
  {2ex}                     % above space
  {2ex}                     % below space
  {}                        % body font
  {}                        % indent amount
  {\scshape}                % head font
  {.}                       % post head punctuation
  {\newline}                % post head punctuation
  {}                        % head spec

\theoremstyle{note}         % from `amsthm'
\newtheorem{scnote}{Note}   % ?? check this - looks wrong

% Colorbox - Theorems, definitions, lemmas, proofs

\usepackage[most]{tcolorbox}          % does not included 'minted' (issue with rmd) 
\newtheorem{lem}{Lemma}		% from `amsthm'
\newtheorem{pruf}{Proof}		% from `amsthm'

% Colorbox Framed Theorem
\newtcbtheorem[number within=chapter]{a_thm}{Theorem}
{enhanced,frame empty,interior empty,colframe=green!50!white,
  coltitle=green!40!black,fonttitle=\bfseries,colbacktitle=green!15!white,
  borderline={0.5mm}{0mm}{green!15!white},  
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, , 
  breakable, colback=white
}{theo}

% Colorbox Framed Theorem 2
% This is Theorem Box for Key Equations 
% Only indexes the Chapter Number, not Chapt.Section
% Should use only once in a Chapter!!
\newtcbtheorem[use counter=chonlycounter]{a_def_eq}{Key ML Equation}
{enhanced,frame empty,interior empty,colframe=blue!50!white,
  coltitle=blue!20!black,fonttitle=\bfseries,colbacktitle=blue!35!white,
  borderline={0.5mm}{0mm}{blue!50!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, 
  breakable, colback=white
}{def_eq}

% Colorbox Framed Definition
\newtcbtheorem[number within=chapter]{a_def}{Definition}
{enhanced,frame empty,interior empty,colframe=blue!50!white,
  coltitle=blue!40!black,fonttitle=\bfseries,colbacktitle=blue!10!white,
  borderline={0.5mm}{0mm}{blue!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, 
  breakable, colback=white
}{defi}

% Colorbox Framed Example
\newtcbtheorem[number within=chapter]{a_exmp}{Example}
{enhanced,frame empty,interior empty,colframe=cyan!50!white,
  coltitle=cyan!20!black,fonttitle=\bfseries,colbacktitle=cyan!15!white,
  borderline={0.5mm}{0mm}{cyan!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, , 
  breakable, colback=white
}{exmp}

% Colorbox Framed Lemma
\newtcbtheorem[number within=chapter]{a_lemma}{Lemma}
{enhanced,frame empty,interior empty,colframe=cyan!50!white,
  coltitle=cyan!20!black,fonttitle=\bfseries,colbacktitle=cyan!15!white,
  borderline={0.5mm}{0mm}{cyan!15!white},
  attach boxed title to top left={yshift=-2mm},
  boxed title style={boxrule=0.4pt},varwidth boxed title, , 
  breakable, colback=white
}{lema}

% tcolorboxenvironment Lemma 
% ?? needed
\tcolorboxenvironment{lem}{
enhanced jigsaw,colframe=cyan,interior hidden, breakable,before skip=10pt,after skip=10pt 
}
\tcolorboxenvironment{pruf}{			
blanker,breakable,left=5mm,
before skip=10pt,after skip=10pt,
borderline west={1mm}{0pt}{red}
}

\pretitle{
\begin{flushleft} \LARGE
\includegraphics[width=2cm,height=2cm]{pictures/IH2.jpg}
\end{flushleft}
\begin{flushright} \LARGE
}

\posttitle  {\end   {flushright}}
\preauthor  {\begin {flushright}}
\postauthor {\end   {flushright}}
\predate    {\begin {flushright}}
\postdate   {\end   {flushright}}

%Title page messaging
\ohead[]{IH - Machine Learning Reviews}
\ifoot{\hsize=350pt \docTitle\ -- Version \vhCurrentVersion}
\ofoot{\thepage~/~\pageref{LastPage}}
\cfoot[]{}

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={25 Key Equations in Machine Learning},
  pdfauthor={to-ohru iwanami \{the hatch keeper\}},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{25 Key Equations in Machine Learning}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{\textit{Insight Hatch Machine Learning Reviews - Vol 1}}
\author{to-ohru iwanami \{the hatch keeper\}}
\date{\vhCurrentDate\\
version \vhCurrentVersion\\
InsightHatch: ``25MLEqs''\\
\textbf{IH-25MLEqs-v}\textbf{\vhCurrentVersion}}

\begin{document}
\frontmatter
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\setstretch{1.25}
\mainmatter
\hfill\break

\begin{versionhistory}
  \vhEntry{0.00.0}{2024.10.25}{DP}{Created!}
  \vhEntry{0.01.0}{2024.10.25}{DP}{Thereom boxes changes, layout changes}
  \vhEntry{0.03.0}{2024.10.26}{DP}{Buildout Chapter Frames}
  \vhEntry{0.04.0}{2024.11.07}{DP}{Chapter Builds}
  \vhEntry{0.07.0}{2024.11.24}{DP}{Naive Bayes, MLE, LinReg, OLS}
  \vhEntry{0.10.3}{2024.12.12}{DP}{Continue Naive Bayes, Continue MLE}
  \vhEntry{0.12.0}{2025.01.10}{DP}{Cleanup}
\end{versionhistory}
\newpage

\setcounter{table}{0}

\frontmatter

\hypertarget{prefice-and-introduction}{%
\chapter{Prefice and Introduction}\label{prefice-and-introduction}}

This book is a journey through the 25 most important mathematical
equations that underpin modern data science and machine learning. The
idea is not only to learn about each of these equations but also to
further understand the application, background, and illustrate practical
examples. We explore each concept visually and instructively, shedding
light on how these fundamental equations contribute to building
intelligent systems.

Mathematics is the foundation of the incredible advances we see today in
artificial intelligence and machine learning. The purpose of this book
is to provide a learning guide that is both informative and visually
compelling, bringing out the underlying beauty and utility of these
powerful component mathematical tools. From optimization algorithms like
Gradient Descent to classification techniques like Naive Bayes, and from
measures of information like Entropy to clustering algorithms like
K-Means, this book aims to provide an accessible yet thorough
explanation of how these concepts work and why they are essential.

Each chapter focuses on one equation, starting with an introduction,
followed by a detailed description, supporting side discussions are
provided where appropriate, also provided are examples of how it is used
in practice. Whether you're an aspiring data scientist, a seasoned
engineer, or just curious about the mathematics that drives intelligent
technology, this book will provide you with a solid understanding of
these essential tools and how they interact to solve complex problems.
Visual aids, illustrative examples, and in-depth explanations will help
demystify each topic, making learning both engaging and enjoyable. The
author takes some liberty on occassion and presents sideline discussions
of related material, with the hope of not making this book a handbook of
sorts but more of a \textbf{group of essays} on supporting related
topics which help the reader to see how the chapter equation is
intertwined with other key points or concepts.

Each section is followed by questions and homework problems. These
problems are selected based on their popularity and effectiveness in
enhancing understanding. The goal is to provide exercises that solidify
the reader's comprehension of each topic. An answer key is provided at
the end of the book to facilitate learning and self-assessment. Solving
(or attempting to solve, as well as reviewing the answer keys), these
problems is helpful to get ever further examples and insight under a
practioner's belt.

Many of the examples and problems use Python and R to provide practical
insights. Code blocks in Python and R are used throughout to illustrate
the concepts discussed. The entire book, is posted on the author's
GitHub repository for easy access and reproducibility.

Whenever possible, all examples that originate from a particular source
are acknowledged by citation and recognition of the original author. If
any citation or attribution is missed, the author kindly requests
feedback so that proper corrections can be made.

\hypertarget{some-notes-on-the-typesetting-framework-and-tools}{%
\subsection*{Some notes on the typesetting framework and
tools}\label{some-notes-on-the-typesetting-framework-and-tools}}
\addcontentsline{toc}{subsection}{Some notes on the typesetting
framework and tools}

With the current advent of ML based LLMs and the availability of
typesetting \LaTeX , we combine tools like
\textbf{\large{\textpoppins{Mathpix}}} (the \LaTeX   equation generating
snipping tool) and \textcouriernew{R Markdown} driven by
\textbf{\large{\textarial{Pandoc}}} and
\textbf{\large{\textpalatino{knitr}}} to essentially ``GET IT DONE''.
Obviously your mileage may vary; we find this tool set perfect for
typesetting combined with computational analysis. Why? Well you have
nearly full \LaTeX  capability with access to R \&
\textbf{\large{\textfluxreg{Python}}} (via
\textbf{\large{\textlato{reticulate}}}) programming code blocks and also
the ability to bridge to
\text{\large{\textssp{MATHEMATICA}}}\textsuperscript{\textregistered} if
needed.

This tool suite (under RStudio) leaves the door open to a wider world of
analysis and computation. While RStudio has it's warts, it does allow
for an IDE approach which leverages typesetting and computation
framework (through R, \textbf{\large{\textfluxreg{Python}}} and
\text{\large{\textssp{MATHEMATICA}}}\textsuperscript{\textregistered})
that makes it a worthy Swiss Army Knife.

The alternative is also VS Code, which is great, with mixed mode
\textcouriernew{R Markdown} \& \LaTeX  computational environment is
pretty flexible. Which ever you use - RStudio or VS Code, combined
typesetting with inline comput

?? Quarto ?? \ldots{} need a fork here ??

\hypertarget{the-complimentary-nod-to-ml-llm-tools}{%
\subsection*{The complimentary nod to ML LLM
tools}\label{the-complimentary-nod-to-ml-llm-tools}}
\addcontentsline{toc}{subsection}{The complimentary nod to ML LLM tools}

Most of this compilation and summary work would not be possible without
the advent of LLMs and new found ability to build concise summaries of
difficult mathematical concepts. The time savings is basically a gift
which save more grey hairs and time flipping pages of numerous cross
references. Actually we want to spend time ``learning'' and not
``flipping and skimming'' to find the properly defined statement which
is of utility in our endeavor. So for that - we lean on LLMs to be more
productive, and help clearly convey underlying concepts. This is not to
be lazy, each section must be scrutinized for accuracy and consistency.
The speed gains allow for a very large group of examples and highly
detailed presentations.

\hypertarget{notes-to-usage}{%
\subsection*{Notes to usage}\label{notes-to-usage}}
\addcontentsline{toc}{subsection}{Notes to usage}

We give full credit for any original works that are used herein to reach
the stated objective. At this point this is not a financial pursuit we
do not provide any license related respect to any previous publisher. Go
figure. Why would we take such a stand - this is educational open source
material. We strongly recommend you take the time to do some background
research on any referenced authors.\\

\emph{We truly hope you enjoy the tour, as much as we have enjoyed our
journey to write it\ldots{}}

to-ohru iwanami, 2024, somewhere in asia

\hypertarget{the-25-uhh-24}{%
\chapter{The 25 (uhh 24)}\label{the-25-uhh-24}}

How did we arrive at the 25 (uhh 24) most important equations? Well,
it's partly a matter of educated opinion, partly a rough statistical
average of what people talk about most when they're exhilarated by the
magic of machine learning, and partly---let's be real, you should just
listen to me (fully joking here). These equations are more than just
mathematical tools; they represent milestones that have shaped the
evolution of artificial intelligence, each one contributing a brick or
building block in the formidable wall of progress we've built over the
decades.

Take, for instance, Gradient Descent --- a cornerstone of optimization,
whose principles date back to the early 19th century and the work of
Adrien-Marie Legendre and Carl Friedrich Gauss in minimizing residuals.
The modern incarnation of Gradient Descent, essential for training deep
neural networks, only emerged in the mid-20th century, revitalized by
researchers trying to teach machines to ``learn.'' Fast forward to the
1960s, when Frank Rosenblatt was developing the perceptron, it became
evident that iterative optimization methods like Gradient Descent could
unlock the potential of early neural networks.

Then we have Bayes' theorem, after Reverend Thomas Bayes, who developed
his idea of conditional probability back in the 18th century. Although
it lay relatively dormant for years, it became a key breakthrough in the
1950s when Alan Turing and others applied Bayesian methods to
codebreaking during World War II. Today, its cousin---Naive Bayes --- is
still a powerful tool for classification, especially for natural
language processing, allowing us to create chatbots and email spam
filters. This dormant theorem found new life, propelling us toward a
world of probabilistic understanding in machines.

Singular Value Decomposition (SVD), another member of our elite list,
found its fame in the 1990s when it was used for information retrieval
in the famous Latent Semantic Analysis (LSA) algorithm---paving the way
for how we handle and understand textual data. The underlying
mathematics, discovered by Eugenio Beltrami and others in the 19th
century, helps us not only reduce dimensionality but also reveal the
hidden relationships between concepts in data.

Fast forward to today to the ReLU (Rectified Linear Unit) function.
While it seems so simple, this piece of mathematical elegance emerged as
a game-changer for deep learning in the 2010s, thanks to the work of
Geoffrey Hinton and his colleagues. Before ReLU, neural networks
struggled with the vanishing gradient problem, limiting their ability to
learn effectively. By simply transforming negative inputs to zero and
retaining positive ones, ReLU helped neural networks go deeper and
deeper (pun-ch!), giving birth to the deep learning revolution we know
today.

Each of these equations is a testament to the evolution of knowledge, a
story of breakthroughs spanning centuries. Together, they form a bridge
from the deterministic calculus of Newton and Leibniz to the
probabilistic dreams of Bayes and the practical algorithms of today's AI
pioneers. Think of them as the greatest hits of mathematics for data
scientists---carefully curated, occasionally debated, and ultimately
distilled into the essence of what drives machine learning today. This
collection captures a human legacy of curiosity and ingenuity, guiding
you on your intellectual journey through the ever-growing forest of
machine learning---one equation, one insight at a time.

As we reflect on these 1 through 24 equations, and on to 25, one cannot
ignore the human spirit that threads through each mathematical symbol
and formula. Behind every breakthrough is a mind driven by curiosity, a
determination to see beyond the obvious, to turn abstraction into
discovery. Each equation, in its own way, captures a moment where human
thought transcended barriers---whether it was a problem of optimization,
uncertainty, or understanding the nature of learning itself. Yet, even
as these 24 pillars of insight stand tall, there remains one more --- a
keystone that binds them all, born not out of calculation alone but from
the essence of what it means to explore, to question, and to create.
This final entry goes beyond numbers and symbols, embracing the very
source of every theorem, every insight, and every spark of ingenuity. So
as you journey onward, remember---there's always one more equation to
unveil, one that's been with us all along.

\mainmatter

\renewcommand{\chaptername}{Equation}

\hypertarget{gradient-descent}{%
\chapter{Gradient Descent}\label{gradient-descent}}

\huge

\[
\color{dblu}{\theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)}
\] \normalsize

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-1-gradient_descent.jpeg}
    \caption*{\Large Finding a local minimum with Gradient Descent}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Gradient Descent}{def1} 

\begin{align}
\large{\theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)}
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$\theta_j$] The current value of the \textbf{parameter vector} at iteration $j$, which represents the current estimate of the model parameters.
    \vspace{0.5\baselineskip}
    \item[$\theta_{j+1}$] The updated value of the \textbf{parameter vector} for the next iteration, which results from applying the gradient step to the current parameters.
    \vspace{0.5\baselineskip}
    \item[$\alpha$] The \textbf{learning rate}, which is a positive scalar determining the size of the step to take in the direction of the negative gradient.
    \vspace{0.5\baselineskip}
    \item[$J$] The \textbf{cost function}, which is the function being minimized by adjusting the parameter vector $\theta$. It measures the difference between predicted values and actual values.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: Gradient Descent is an optimization algorithm
used to minimize the cost function by iteratively moving in the
direction of the steepest descent as defined by the negative of the
gradient.

\textbf{Description}: In machine learning, gradient descent is used to
update the parameters of the model, \(\theta\), to reduce the difference
between the predicted and actual outcomes.

\textbf{Importance in ML}: Gradient Descent is foundational for training
machine learning models, particularly in neural networks and linear
regression. It helps in finding optimal parameters by iteratively
reducing the error, making it crucial for model accuracy.\\

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation}{%
\section{What is behind the
equation}\label{what-is-behind-the-equation}}

\hypertarget{structure-of-the-parameter-vector-theta}{%
\section{\texorpdfstring{Structure of the Parameter Vector
\(\theta\)}{Structure of the Parameter Vector \textbackslash theta}}\label{structure-of-the-parameter-vector-theta}}

The parameter vector, typically denoted as \(\theta\), contains all the
adjustable parameters or weights of the model that you want to optimize
in order to minimize the cost function \(J(\theta)\). Depending on the
type of model, the structure of \(\theta\) can vary:

\begin{itemize}
\item
  \textbf{Linear Regression}:

  \begin{align}
  \theta = 
  \begin{bmatrix}
  \theta_0 \\
  \theta_1 \\
  \vdots \\
  \theta_n
  \end{bmatrix}
  \end{align}
\end{itemize}

In linear regression, \(\theta\) is often a column vector of weights
where each \(\theta_i\) corresponds to the weight associated with
feature \(x_i\). \(\theta_0\) is often referred to as the bias term or
intercept.

\begin{itemize}
\item
  \textbf{Logistic Regression / Neural Networks}: In these models,
  \(\theta\) can have more dimensions. For a neural network, the
  parameter vector could be a collection of weight matrices for
  different layers, such as:

  \begin{align}
  \theta = \{ W_1, W_2, \ldots, W_L, b_1, b_2, \ldots, b_L \}
  \end{align}

  where \(W_i\) and \(b_i\) are weights and biases associated with layer
  \(i\) of the network.
\item
  \textbf{Deep Learning Models}: The parameter vector is much more
  complex, often containing multiple matrices and vectors representing
  weights and biases for each layer in a deep neural network.
\end{itemize}

In general, \(\theta\) is a vector that can be expressed as:

\begin{align}
\theta = (\theta_1, \theta_2, \dots, \theta_n)^T
\end{align}

where \(n\) is the number of features or neurons, depending on the model
type.

\hypertarget{typical-value-of-the-learning-rate-alpha}{%
\section{\texorpdfstring{Typical Value of the Learning Rate
\(\alpha\)}{Typical Value of the Learning Rate \textbackslash alpha}}\label{typical-value-of-the-learning-rate-alpha}}

The learning rate \(\alpha\) controls the step size when updating the
parameters during gradient descent. Choosing a proper value for
\(\alpha\) is crucial for the convergence of the algorithm. Here are
some general guidelines:

\begin{itemize}
\item
  \textbf{Typical Range}: A typical value for the learning rate lies
  between \(0.001\) and \(0.1\). Values outside this range can either
  lead to a slow convergence or an unstable training process.
\item
  \textbf{Considerations}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Too Small}: If \(\alpha\) is too small, gradient descent
    will take very small steps towards the optimum, resulting in a very
    slow convergence process.
  \item
    \textbf{Too Large}: If \(\alpha\) is too large, gradient descent
    might overshoot the minimum or even diverge, causing the cost
    function to oscillate or increase.
  \item
    \textbf{Adaptive Learning Rates}: Some advanced algorithms like
    \textbf{Adam} use adaptive learning rates which adjust automatically
    as training progresses.
  \end{itemize}
\end{itemize}

In practice, tuning the learning rate often involves trial and error or
the use of techniques like \textbf{learning rate schedules} or
\textbf{grid search} to find the best value for a specific problem.

\hypertarget{math-behind-the-gradient-nabla-jtheta_j}{%
\section{\texorpdfstring{Math Behind the Gradient
\(\nabla J(\theta_j)\)}{Math Behind the Gradient \textbackslash nabla J(\textbackslash theta\_j)}}\label{math-behind-the-gradient-nabla-jtheta_j}}

The term \(\nabla J(\theta_j)\) is the \textbf{gradient} of the cost
function \(J(\theta)\) evaluated at \(\theta_j\). Let's break it down:

\begin{itemize}
\item
  \textbf{Gradient Definition}: The gradient of a function is a vector
  of partial derivatives with respect to each parameter in \(\theta\).
  In the case of \(J(\theta)\), the gradient \(\nabla J(\theta_j)\)
  tells us how much the cost function changes when we make an
  infinitesimally small change to each component of \(\theta\).
  Mathematically, for a parameter vector
  \(\theta_j = (\theta_1, \theta_2, \dots, \theta_n)^T\):

  \[
  \nabla J(\theta_j) = \begin{bmatrix}
  \frac{\partial J}{\partial \theta_1} \Big|_{\theta_j} \\
  \frac{\partial J}{\partial \theta_2} \Big|_{\theta_j} \\
  \vdots \\
  \frac{\partial J}{\partial \theta_n} \Big|_{\theta_j}
  \end{bmatrix}
  \]

  Each partial derivative \(\frac{\partial J}{\partial \theta_i}\)
  represents the rate of change of the cost function with respect to
  parameter \(\theta_i\).
\item
  \textbf{Intuition}: The gradient \(\nabla J(\theta_j)\) points in the
  direction of the \textbf{steepest ascent} of the cost function \(J\).
  In gradient descent, we want to \textbf{minimize} \(J(\theta)\), so we
  move in the opposite direction, which is why the update rule is:

  \[
  \theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(\nabla J(\theta_j)\) represents the slope or direction in which
    \(J(\theta)\) increases most quickly.
  \item
    By subtracting \(\alpha \nabla J(\theta_j)\), we effectively move in
    the direction of steepest \textbf{descent}, hence reducing
    \(J(\theta)\).
  \end{itemize}
\item
  \textbf{Computing the Gradient}: In practice, the gradient
  \(\nabla J(\theta_j)\) is computed using \textbf{differentiation}. For
  different cost functions, the gradient takes different forms:

  \begin{itemize}
  \tightlist
  \item
    For \textbf{linear regression} with a \textbf{mean squared error
    (MSE)} cost function, the gradient is relatively simple and involves
    the residuals (errors) between predictions and actual values.
  \item
    For \textbf{neural networks}, computing the gradient involves
    \textbf{backpropagation}, which is a process of applying the chain
    rule of calculus to calculate the gradients efficiently for each
    layer.
  \end{itemize}
\end{itemize}

In short, \(\nabla J(\theta_j)\) gives us the necessary information to
adjust \(\theta\) in a way that reduces the error. The learning rate
\(\alpha\) then determines how big the step should be in this direction.

\newpage

\hypertarget{gradient-descent-in-r}{%
\section{Gradient Descent in R}\label{gradient-descent-in-r}}

Gradient Descent is a foundational optimization algorithm used to
iteratively minimize cost functions. Here, we apply the gradient descent
method to find the local minimum of a specific polynomial function. The
function used is a quartic polynomial
\(P(x) = x^4 - 6x^3 + 11x^2 - 6x\). Our goal is to observe how the
gradient descent algorithm updates our parameter over successive
iterations to converge towards a minimum point of the polynomial.

The following R code demonstrates how to implement gradient descent for
this polynomial, including plotting the descent path and function value
across iterations. This practical example aims to give you a clear
understanding of how gradient descent operates on real functions, using
R for illustration. Note that gd\_result is the dataframe result fed to
ggplot.\\

\begin{lstlisting}[caption={\raggedright Gradient Descent: Over a Polynomial}, label={lst:GradDec1}]
\end{lstlisting}
\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Define the new polynomial function and its derivative}
\NormalTok{polynomial\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{return}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{4} \SpecialCharTok{{-}} \DecValTok{8} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{3} \SpecialCharTok{+} \DecValTok{18} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{11} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Derivative of the new polynomial function}
\NormalTok{polynomial\_derivative }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{return}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{24} \SpecialCharTok{*}\NormalTok{ x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{36} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{11}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Gradient Descent Algorithm}
\NormalTok{gradient\_descent }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{learning\_rate =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{iterations =} \DecValTok{1000}\NormalTok{, }\AttributeTok{start =} \DecValTok{3}\NormalTok{) \{}
\NormalTok{  x }\OtherTok{\textless{}{-}}\NormalTok{ start  }\CommentTok{\# Starting point}
  
  \CommentTok{\# Store x values and function values for plotting}
\NormalTok{  x\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(iterations)}
\NormalTok{  function\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(iterations)}
  
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{iterations) \{}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{polynomial\_derivative}\NormalTok{(x)}
\NormalTok{    x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{{-}}\NormalTok{ learning\_rate }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{    x\_values[i] }\OtherTok{\textless{}{-}}\NormalTok{ x}
\NormalTok{    function\_values[i] }\OtherTok{\textless{}{-}} \FunctionTok{polynomial\_function}\NormalTok{(x)}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Iteration =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{iterations, x\_values, function\_values))}
\NormalTok{\}}


\CommentTok{\# Run the Gradient Descent with specific parameters}
\NormalTok{learning\_rate }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\NormalTok{iterations }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{start\_point }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{gd\_result }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_descent}\NormalTok{(learning\_rate, iterations, start\_point)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{lstlisting}[caption={\raggedright Gradient Descent: Objective Function vs. Iteration}, label={lst:GradDec2}]
\end{lstlisting}
\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(gd\_result, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Iteration, }\AttributeTok{y =}\NormalTok{ function\_values)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Gradient Descent on Polynomial Function"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Iteration"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Objective Function Value: f(x)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{11}\NormalTok{)  }\CommentTok{\# Adjust the size value as needed}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/gradient-descent-iteration-plot1-1} 

}

\caption{Gradient Descent on Polynomial Function - Iteration vs Function Value}\label{fig:gradient-descent-iteration-plot1}
\end{figure}
\normalsize

\newpage
\begin{lstlisting}[caption={\raggedright Gradient Descent: Plot the Path of Descent}, label={lst:GradDec3}]
\end{lstlisting}
\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Additional Plot: Trace the Path of Gradient Descent on the Polynomial}
\NormalTok{x\_range }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(gd\_result}\SpecialCharTok{$}\NormalTok{x\_values) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\FunctionTok{max}\NormalTok{(gd\_result}\SpecialCharTok{$}\NormalTok{x\_values) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{500}\NormalTok{)}
\NormalTok{polynomial\_values }\OtherTok{\textless{}{-}} \FunctionTok{polynomial\_function}\NormalTok{(x\_range)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_range, }\AttributeTok{y =}\NormalTok{ polynomial\_values), }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ gd\_result, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_values, }\AttributeTok{y =}\NormalTok{ function\_values), }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Gradient Descent Path on Polynomial Function"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"f(x)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{11}\NormalTok{)  }\CommentTok{\# Adjust the size value as needed}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/gradient-descent-iteration-plot2-1} 

}

\caption{Gradient Descent on Polynomial Function - Trace the Path on the Polynomial}\label{fig:gradient-descent-iteration-plot2}
\end{figure}
\normalsize

\newpage

\hypertarget{linking-gradient-descent-to-our-polynomial-example}{%
\section{Linking Gradient Descent to Our Polynomial
Example}\label{linking-gradient-descent-to-our-polynomial-example}}

The equation we used to explain gradient descent is:

\[
\theta_{j+1} = \theta_j - \alpha \nabla J(\theta_j)
\]

\hypertarget{step-by-step-breakdown}{%
\subsection{Step-by-Step Breakdown}\label{step-by-step-breakdown}}

The fundamental idea behind gradient descent is to iteratively adjust
the model's parameters, \(\theta\), in a direction that reduces the cost
function, \(J(\theta)\). The parameter update is governed by the
\textbf{gradient} of the cost function (or loss function),
\(\nabla J(\theta)\).

In the above equation:

\begin{itemize}
\tightlist
\item
  \(\theta_{j}\) represents the parameter(s) at step \(j\), and
  \(\theta_{j+1}\) is the updated parameter at step \(j+1\).
\item
  \(\alpha\) is the \textbf{learning rate}, which controls how large
  each step is in the descent.
\item
  \(\nabla J(\theta_j)\) is the \textbf{gradient} of the cost function
  with respect to the parameters, evaluated at \(\theta_j\). It gives
  the direction of the steepest ascent, and since we want to minimize
  the function, we move in the opposite direction, hence the negative
  sign.
\end{itemize}

\hypertarget{gradient-descent-applied-to-the-polynomial}{%
\subsection{Gradient Descent Applied to the
Polynomial}\label{gradient-descent-applied-to-the-polynomial}}

In our R code example, we have a \textbf{polynomial function} defined
as:

\[
P(x) = x^4 - 6x^3 + 11x^2 - 6x
\]

Our goal is to minimize this polynomial function, which represents our
\textbf{cost function}, \(J(x)\). Here, \(x\) plays the role of our
parameter, \(\theta\), that we want to adjust iteratively using gradient
descent.

The \textbf{derivative} of the polynomial is:

\[
P'(x) = 4x^3 - 18x^2 + 22x - 6
\]

In the gradient descent algorithm, this derivative represents the
\textbf{gradient} of the cost function with respect to our parameter,
\(x\). This means that \(\nabla J(x) = P'(x)\). The iterative update
step becomes:

\[
x_{j+1} = x_j - \alpha P'(x_j)
\]

Where: - \(x_j\) is the current value of the parameter (similar to
\(\theta_j\)). - \(\alpha\) is the learning rate that we set in the R
code. - \(P'(x_j)\) is the gradient of the polynomial at the current
point.

\hypertarget{bringing-it-full-circle}{%
\subsection{Bringing It Full Circle}\label{bringing-it-full-circle}}

In our R code, we used \textbf{gradient descent} to minimize the
polynomial function by starting at an initial point (\(x = 3\)) and
repeatedly updating \(x\) using the gradient descent rule:

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{{-}}\NormalTok{ learning\_rate }\SpecialCharTok{*}\NormalTok{ grad}
\end{Highlighting}
\end{Shaded}

\normalsize

This corresponds exactly to the equation:

\[
x_{j+1} = x_j - \alpha P'(x_j)
\]

As each iteration proceeds, \(x\) gets closer and closer to a point
where the gradient is zero (i.e., a local minimum or a stationary
point). In this particular case, the polynomial \(P(x)\) has one real
minimum, and the gradient descent algorithm helps us converge towards
it.

\hypertarget{visual-connection}{%
\subsection{Visual Connection}\label{visual-connection}}

The \textbf{first plot} in our example (function value vs.~iteration)
shows how the value of the polynomial decreases over time as gradient
descent proceeds. The \textbf{second plot} (gradient descent path on the
polynomial curve) visually shows how \(x\) moves along the polynomial
curve, getting closer and closer to the minimum.

This linkage between the theoretical equation of gradient descent and
the practical implementation of minimizing the polynomial demonstrates
how gradient descent serves as a universal tool to solve optimization
problems, whether in machine learning or even in simple polynomial
functions like the one used in our example.

Gradient descent works by always taking steps in the direction that most
rapidly reduces the cost function---allowing us to find optimal
parameters, minimize errors, or reach local minima. It is an essential
part of training many machine learning models, and the concept remains
consistent regardless of the specific problem context.

\normalsize
\setstretch{1.25}

\newpage

\hypertarget{additonal-ideas-to-explore}{%
\section{Additonal ideas to explore}\label{additonal-ideas-to-explore}}

Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, or
concepts like Momentum and Learning Rate Scheduling.

We could also explore more code examples, such as:

Adding stopping criteria to our gradient descent R code, like a
tolerance for change in cost function.

Visualizing the convergence of the gradient descent with multiple
starting points.

Implementing different cost functions and comparing their optimization
trajectories. Let me know which direction you'd like to explore, and
I'll get started!

\newpage

\hypertarget{normal-distribution}{%
\chapter{Normal Distribution}\label{normal-distribution}}

\(f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\)

\huge

\[
\color{dblu}{f(x\! \mid\! \mu, \sigma^2) = \large{\frac{1}{\sigma \sqrt{2\pi}}}\; e^{-\frac{(x-\mu)^2}{2 \sigma^2}}}
\] \normalsize

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-2-normal_distribution.jpeg}
    \caption*{\Large Normal Distribution, Variance and Mean}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Normal Distribution}{def2} 

\begin{align}
 f(x | \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$x$] The \textbf{random variable} for which the probability density function is being calculated. It represents the value within the distribution.
    \vspace{0.5\baselineskip}
    \item[$\mu$] The \textbf{mean} of the distribution, which represents the center or "average" value around which the data clusters.
    \vspace{0.5\baselineskip}
    \item[$\sigma$] The \textbf{standard deviation} of the distribution, indicating how spread out the values are around the mean. A larger $\sigma$ means more spread, while a smaller $\sigma$ means values are more tightly clustered.
    \vspace{0.5\baselineskip}
    \item[$\sigma^2$] The \textbf{variance} of the distribution, which is the square of the standard deviation. It provides a measure of the dispersion of the distribution.
    \vspace{0.5\baselineskip}
    \item[$f(x | \mu, \sigma^2)$] The \textbf{probability density function (PDF)} for the normal distribution, which provides the likelihood of $x$ occurring given the parameters $\mu$ and $\sigma^2$.
    \vspace{0.5\baselineskip}
    \item[$\exp$] The \textbf{exponential function}, which ensures that the PDF value falls off symmetrically from the mean $\mu$. It plays a key role in modeling the bell-shaped curve characteristic of the normal distribution.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: The normal distribution, also called the Gaussian
distribution, is a probability distribution that is symmetric about the
mean.

\textbf{Description}: It represents how data tends to cluster around a
central point. The parameters \(\mu\) and \(\sigma^2\) represent the
mean and variance, respectively.

\textbf{Importance in ML}: The normal distribution is used in many ML
algorithms, especially in probabilistic models and hypothesis testing.
Assumptions of normality often simplify the mathematics of learning
models and are vital in Bayesian networks.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-1}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-1}}

The normal distribution equation represents a probability density
function (PDF) that models how data points are distributed around a
central value (the mean, \(\mu\)). This equation is essential in
statistics because it describes a common pattern found in natural
phenomena, from heights of people to errors in measurements. The
bell-shaped curve is a striking visual, representing how values tend to
cluster around the mean, with fewer observations occurring as we move
further from this center. The beauty of the normal distribution lies in
its symmetry and the way it characterizes many real-world datasets,
making it a cornerstone in both statistics and machine learning.

\newpage

\hypertarget{the-players}{%
\section{The Players}\label{the-players}}

\begin{figure}[htbp]
    \centering
    % Left Image (Normal Distribution Summary)
    \begin{minipage}[t]{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/normal_distribution.jpeg}
        \caption{Summary of Normal Distribution with PDF and CDF}
    \end{minipage}
    \hfill
    % Right Images (Carl Friedrich Gauss and Pierre-Simon Laplace)
    \begin{minipage}[t]{0.40\textwidth}
        \vspace{-38em} % Negative space to align the right images better with the left
        \centering
        \includegraphics[width=\textwidth, height=0.3\textheight]{pictures/gauss.jpeg}
        \caption{Carl Friedrich Gauss}
        \vspace{1em} % Adds some spacing between the two right images
        \includegraphics[width=\textwidth, height=0.3\textheight]{pictures/laplace.jpeg}
        \caption{Pierre-Simon Laplace}
    \end{minipage}

    \label{fig:normal-distribution-layout}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Normal Distribution}:
  {[}\href{https://en.wikipedia.org/wiki/Normal_distribution}{Normal Distribution on Wikipedia}{]}
\item
  \textbf{Carl Friedrich Gauss}:
  {[}\href{https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss}{Carl Friedrich Gauss on Wikipedia}{]}
\item
  \textbf{Pierre-Simon Laplace}:
  {[}\href{https://en.wikipedia.org/wiki/Pierre-Simon_Laplace}{Pierre-Simon Laplace on Wikipedia}{]}
  \newpage
\end{enumerate}

\hypertarget{historical-context}{%
\section{Historical Context}\label{historical-context}}

The normal distribution, often referred to as the Gaussian distribution,
has a rich history rooted in the development of probability theory and
statistics. The distribution is named after the German mathematician
Carl Friedrich Gauss, who used it extensively in his work on astronomy
and measurement errors in the early 19th century. Gauss formalized the
idea that errors in measurements tend to follow a symmetric pattern
around the true value, leading to the bell-shaped curve we now associate
with the normal distribution.

However, the concept of the normal distribution predates Gauss and can
be traced back to the work of Abraham de Moivre, an 18th-century French
mathematician. De Moivre first derived the normal distribution as an
approximation to the binomial distribution when the number of trials
becomes very large. His work laid the foundation for what would later be
formalized and popularized by Gauss.

The normal distribution became particularly significant due to the
Central Limit Theorem, which states that the sum of many independent
random variables, regardless of their original distribution, tends to
follow a normal distribution. This theorem, proven by mathematicians
such as Pierre-Simon Laplace, helped establish the normal distribution
as a fundamental tool in statistics and the natural sciences. Today, it
is widely used not only because of its mathematical properties but also
because it naturally arises in numerous real-world situations, making it
one of the most important and recognizable distributions in probability
and statistics.

\hypertarget{relative-standard-deviation-rsd}{%
\section{Relative Standard Deviation
(RSD)}\label{relative-standard-deviation-rsd}}

In the context of the normal distribution, the spread of data points is
characterized by the standard deviation (\(\sigma\)). A useful concept
derived from this is the \textbf{Relative Standard Deviation (RSD)},
which is expressed as a percentage of the mean:

\[
\text{RSD} = \left( \frac{\sigma}{\mu} \right) \times 100
\]

\textbf{Typical RSD Values}: The RSD provides insight into how variable
the data is relative to its mean. A low RSD (e.g., below 10\%) indicates
that the data points are closely clustered around the mean, whereas a
higher RSD suggests more dispersion. In many real-world datasets, RSDs
ranging between 5\% and 20\% are common, depending on the type of
measurement and its inherent variability. RSD helps give a standardized
view of spread that is independent of the scale of the data, which is
particularly useful when comparing the variability between datasets.

\hypertarget{understanding-the-notation-fx-mu-sigma2}{%
\section{\texorpdfstring{Understanding the Notation
\(f(x | \mu, \sigma^2)\)}{Understanding the Notation f(x \textbar{} \textbackslash mu, \textbackslash sigma\^{}2)}}\label{understanding-the-notation-fx-mu-sigma2}}

The notation \(f(x | \mu, \sigma^2)\) represents the \textbf{probability
density function} of a normal distribution given the parameters \(\mu\)
(mean) and \(\sigma^2\) (variance). This notation indicates a
\textbf{conditional relationship}, where the probability density
function \(f(x)\) is dependent on the parameters \(\mu\) and
\(\sigma^2\).

In other words, \(\mu\) and \(\sigma^2\) define the specific
characteristics of the distribution (where it is centered and how spread
out it is), while \(x\) represents the variable for which the
probability is being calculated. This notation highlights that the
distribution is characterized by these parameters, and the output
\(f(x)\) tells us how likely it is to observe a particular value of
\(x\) under that distribution.

\hypertarget{the-nature-of-the-exponential-function-in-normal-distribution}{%
\section{The Nature of the Exponential Function in Normal
Distribution}\label{the-nature-of-the-exponential-function-in-normal-distribution}}

A key component of the normal distribution equation is the
\textbf{exponential function}:

\[
\exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right)
\]

This part of the function gives the normal distribution its famous
\textbf{bell shape}. The negative squared term in the exponent ensures
that values closer to the mean (\(\mu\)) have higher probabilities,
while values further away have exponentially decreasing probabilities.
This shape is what makes the normal distribution symmetric, with a
single peak at the mean.

The function \(\exp(-x^2)\) falls off rapidly as \(|x|\) increases,
which results in a smooth, continuous decline from the peak at the mean.
This property is crucial because it reflects how, in many natural
phenomena, values tend to cluster around an average, with extreme values
being much less common. This elegant behavior is what makes the normal
distribution so special and why it is so frequently used in statistical
modeling.

\hypertarget{non-existence-of-an-analytical-anti-derivative}{%
\section{Non-existence of an Analytical
Anti-Derivative}\label{non-existence-of-an-analytical-anti-derivative}}

Interestingly, the \textbf{normal distribution function does not have an
anti-derivative} that can be expressed in closed form. This means that
the area under the curve (which represents the cumulative probability)
cannot be solved using traditional analytic integration techniques.
Instead, it is computed numerically or looked up using statistical
tables.

The integral of the normal distribution is given by:

\[
\int_{-\infty}^{\infty} f(x | \mu, \sigma^2) \, dx = 1
\]

However, there is no elementary function that represents this integral.
This is why the \textbf{error function (erf)} is introduced in
mathematics to help approximate these values:

\[
\int e^{-x^2} \, dx = \frac{\sqrt{\pi}}{2} \, \text{erf}(x) + C
\]

The lack of an analytical solution means that in practice, probabilities
for a normal distribution are often calculated using \textbf{numerical
methods} or \textbf{precomputed tables}. This characteristic of the
normal distribution makes it an interesting function from a mathematical
standpoint, as it combines simplicity of form with complexity in
integration.

\hypertarget{central-limit-theorem}{%
\section{Central Limit Theorem}\label{central-limit-theorem}}

The Central Limit Theorem (CLT) is a fundamental concept in probability
theory that explains why the normal distribution is so prevalent in
nature and in machine learning. The CLT states that when independent
random variables are added together, their properly normalized sum tends
to form a normal distribution, regardless of the original distribution
of the variables. This remarkable property applies as long as the number
of variables is sufficiently large and they have finite variances.

Mathematically, if we have a set of random variables , each with mean
and variance , the sum or average of these variables will tend towards a
normal distribution as becomes large. The formal expression is:

where denotes convergence in distribution, and represents the standard
normal distribution.

The CLT helps explain why the normal distribution is observed so often
in practice. Many complex processes can be thought of as the sum of many
small, independent effects. Whether it's measurement errors, human
heights, or even financial market fluctuations, these effects combine to
form a distribution that is approximately normal.

\hypertarget{importance-in-machine-learning}{%
\subsection{Importance in Machine
Learning}\label{importance-in-machine-learning}}

The Central Limit Theorem has significant implications in machine
learning. It provides the foundation for many statistical techniques and
justifies the assumption of normality in models. For example:

Modeling Errors: In regression analysis, the residuals (errors) are
often assumed to be normally distributed. This assumption allows us to
derive confidence intervals and perform hypothesis testing.

Feature Engineering: When aggregating data, such as calculating the mean
of multiple features or observations, the resulting values tend to be
normally distributed, making it easier to apply techniques that assume
normality.

Sampling Distributions: The CLT allows us to make inferences about
population parameters from sample data. Many machine learning algorithms
rely on sampling and estimation, and the CLT ensures that the
distribution of the sample mean approximates normality, which simplifies
analysis and interpretation.

The CLT thus serves as a bridge between randomness and order, allowing
us to apply the powerful tools of the normal distribution even in cases
where the underlying data might not be normally distributed. It is this
ability to generalize and predict outcomes that makes the normal
distribution so central to both statistics and machine learning.

\hypertarget{applications-in-machine-learning}{%
\section{Applications in Machine
Learning}\label{applications-in-machine-learning}}

The normal distribution plays a vital role in numerous machine learning
algorithms and concepts. Its presence is seen in everything from model
assumptions to data transformations. Below are some of the key areas
where the normal distribution is commonly applied:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gaussian Naive Bayes
\end{enumerate}

Gaussian Naive Bayes is a classification algorithm based on Bayes'
theorem. It assumes that the features follow a normal distribution,
which allows the model to calculate probabilities efficiently. The
assumption of normality simplifies the calculations, enabling rapid
classification even with high-dimensional data. This assumption works
well for many real-world datasets, making Gaussian Naive Bayes a popular
choice for problems like spam detection and document classification.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Linear Regression Error Terms
\end{enumerate}

In linear regression, the error terms (residuals) are often assumed to
be normally distributed. This assumption allows us to make statistical
inferences about the parameters of the regression model, such as
constructing confidence intervals and conducting hypothesis tests. If
the residuals are approximately normal, we can apply powerful
statistical tools to evaluate model fit and make predictions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Weight Initialization in Neural Networks
\end{enumerate}

When training neural networks, the weights are often initialized using a
normal distribution. For example, in the Xavier initialization method,
weights are drawn from a normal distribution with a mean of zero and a
variance that depends on the number of input and output nodes. This
helps ensure that the neurons start with a diverse range of values,
which prevents issues such as all neurons producing the same output.
Proper initialization is crucial for efficient training, and using the
normal distribution helps keep the gradients within a reasonable range
during backpropagation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Generative Models
\end{enumerate}

The normal distribution is also used in generative models, such as
Gaussian Mixture Models (GMMs), which assume that the data is generated
from a mixture of several Gaussian distributions. GMMs are widely used
in clustering problems, where they help to model the underlying
distribution of the data and assign probabilities to different clusters.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Feature Scaling and Data Transformation
\end{enumerate}

In many machine learning algorithms, it is beneficial for features to
follow a normal distribution. Methods such as StandardScaler in
scikit-learn standardize features by removing the mean and scaling to
unit variance, resulting in a distribution with a mean of 0 and a
standard deviation of 1. This process is especially important for
algorithms that are sensitive to the scale of the input features, such
as support vector machines (SVMs) and gradient descent optimization.

Importance of the Normal Distribution in Machine Learning

The normal distribution is not only a convenient assumption but also a
useful tool in machine learning. Its prevalence in real-world phenomena
and its mathematical properties make it indispensable for building,
evaluating, and optimizing models. Whether it's in simplifying
calculations through Gaussian Naive Bayes, making statistical inferences
in linear regression, initializing neural networks effectively, or
clustering data in GMMs, the normal distribution is at the core of
numerous machine learning practices. By understanding and leveraging the
normal distribution, practitioners can better model uncertainties and
improve the robustness of their machine learning solutions.

\hypertarget{suggested-additional-content-for-the-chapter}{%
\section{Suggested Additional Content for the
Chapter}\label{suggested-additional-content-for-the-chapter}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Historical Context}: Add a brief history of the normal
  distribution, perhaps mentioning Carl Friedrich Gauss, who contributed
  to its development, and why it is often called the Gaussian
  distribution.
\item
  \textbf{Visualization}: Add a visualization of the normal distribution
  with varying means and standard deviations to help illustrate how
  changes in \(\mu\) and \(\sigma\) affect the shape of the curve.
\end{enumerate}

\newpage

\hypertarget{z-score}{%
\chapter{Z-Score}\label{z-score}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $z = \frac{x - \mu}{\sigma}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-3-Zscore.jpeg}
    \caption*{\Large Take a portion of the probability with the Z-Score}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Z-Score}{def3} 

\begin{align}
z = \frac{x - \mu}{\sigma}
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$x$] The \textbf{random variable}, representing the value within the distribution that is being standardized.
    \vspace{0.5\baselineskip}
    \item[$\mu$] The \textbf{mean} of the distribution, which is the average value and the point around which data tends to cluster.
    \vspace{0.5\baselineskip}
    \item[$\sigma$] The \textbf{standard deviation} of the distribution, indicating the spread or dispersion of values around the mean. A larger $\sigma$ suggests more variability.
    \vspace{0.5\baselineskip}
    \item[$z$] The \textbf{Z-Score}, which represents how many standard deviations a particular $x$ value is from the mean $\mu$. It is a measure of relative position within the distribution.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: The Z-score represents the number of standard
deviations a data point is from the mean.

\textbf{Description}: It is used to standardize data points within a
dataset, making comparisons between different distributions possible.

\textbf{Importance in ML}: Z-scores are crucial in feature scaling and
normalization, allowing different features to be compared and helping
gradient-based algorithms converge faster by ensuring all features have
a similar scale.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-2}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-2}}

The Z-score, also known as the standard score, is a measure that
describes the position of a value relative to the mean of a dataset, in
units of the standard deviation. The Z-score calculation is particularly
useful in the context of the normal distribution, allowing us to
determine how far a particular value \(x\) lies from the mean \(\mu\)
when expressed in terms of the distribution's standard deviation
\(\sigma\). The Z-score is expressed by the formula:

\[
 z = \frac{x - \mu}{\sigma}
\]

This metric is instrumental in transforming individual data points into
a universal scale, where positive Z-scores indicate values above the
mean, and negative Z-scores indicate values below the mean. The Z-score
effectively normalizes different datasets, making comparisons
straightforward.

\hypertarget{applications-in-machine-learning-1}{%
\section{Applications in Machine
Learning}\label{applications-in-machine-learning-1}}

In machine learning, Z-scores are employed for several purposes, ranging
from outlier detection to feature scaling.

\begin{itemize}
\item
  \textbf{Outlier Detection}: Z-scores can help identify outliers, as
  values with extremely high or low Z-scores typically indicate data
  points that lie far from the distribution's average behavior. These
  outliers might signify anomalies or valuable insights that need closer
  examination.
\item
  \textbf{Feature Scaling}: Z-score normalization, also known as
  standardization, is a common feature scaling method. By transforming
  features using Z-scores, they are rescaled to have a mean of 0 and a
  standard deviation of 1. This type of normalization is especially
  valuable in algorithms like logistic regression, k-means clustering,
  and principal component analysis (PCA), where features with different
  ranges can negatively impact model performance.
\item
  \textbf{Standard Normal Table Applications}: The Z-score is also used
  in combination with the standard normal distribution table to compute
  probabilities and p-values for hypothesis testing. In machine
  learning, this is often relevant in model evaluation and statistical
  testing.
\end{itemize}

\hypertarget{typical-range-for-z-scores}{%
\section{Typical Range for Z-Scores}\label{typical-range-for-z-scores}}

The typical range for Z-scores in a standard normal distribution is
between \(-3\) and \(3\). Values beyond this range are considered rare
and are often associated with outliers.

\begin{itemize}
\tightlist
\item
  \textbf{\(|Z| > 3\)}: Typically considered outliers. Values beyond
  three standard deviations are quite unusual in a normal distribution.
\item
  \(|Z| < 1\): Most values are likely within this range and close to the
  mean.
\item
  \(1 \leq |Z| < 2\): These values are still common, although they are
  further away from the mean.
\item
  \(|Z| > 2\): Values in this range start to become unusual, indicating
  either naturally rare observations or data quality issues.
\end{itemize}

The calculation of Z-score can be a valuable indicator of how far a data
point deviates from what is expected, helping differentiate between
normal variance and potential outliers.

\hypertarget{mathematical-notation-and-concept}{%
\section{Mathematical Notation and
Concept}\label{mathematical-notation-and-concept}}

The equation for the Z-score is represented as follows:

\[
z = \frac{x - \mu}{\sigma}
\]

\begin{itemize}
\tightlist
\item
  \textbf{\(x\)}: Represents the actual observed value within the
  dataset.
\item
  \textbf{\(\mu\)}: The mean of the dataset, which represents the
  central tendency around which the dataset is distributed.
\item
  \textbf{\(\sigma\)}: The standard deviation, which gives insight into
  the degree of variability within the data points. It tells us how
  spread out the values are around the mean.
\end{itemize}

The Z-score essentially tells us how many standard deviations away from
the mean a given value \(x\) is, and whether it is to the left (negative
\(z\)) or right (positive \(z\)) of the mean.

\hypertarget{mathematical-insights-area-under-the-normal-curve}{%
\section{Mathematical Insights: Area Under the Normal
Curve}\label{mathematical-insights-area-under-the-normal-curve}}

One of the key properties of the Z-score is how it allows for the
calculation of probabilities from the normal distribution. By converting
a value to its Z-score, we can use the standard normal distribution
(which has a mean of 0 and a standard deviation of 1) to determine the
probability that a value is less than or greater than a given point.

For example, calculating the probability \(P(Z \le z)\) involves
integrating the probability density function of the normal distribution
up to the given Z-score value:

\[
P(Z \le z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt
\]

Since this integral has no closed-form solution, it is typically
evaluated using numerical methods, standard normal distribution tables,
or software. This integral defines the cumulative distribution function
(CDF) of the normal distribution, which is essential in assessing
cumulative probabilities for standard normal variables.

In practice, machine learning models often use pre-calculated tables or
libraries to compute these probabilities for performance metrics,
hypothesis testing, or evaluating the significance of model parameters.

\hypertarget{example-z-score-in-r}{%
\section{Example: Z-Score in R}\label{example-z-score-in-r}}

The Z-score calculation can be easily implemented in R to standardize a
dataset or to identify outliers. Below is a simple example to illustrate
how we can compute Z-scores for a given dataset and visualize the data
distribution using a histogram:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Define the parameters for the normal distribution}
\NormalTok{mean\_value }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{std\_dev }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{z\_score }\OtherTok{\textless{}{-}} \FloatTok{1.25}

\CommentTok{\# Define the cumulative distribution function (CDF) to calculate probability}
\NormalTok{p\_value }\OtherTok{\textless{}{-}} \FunctionTok{pnorm}\NormalTok{(z\_score, }\AttributeTok{mean =}\NormalTok{ mean\_value, }\AttributeTok{sd =}\NormalTok{ std\_dev)}

\CommentTok{\# Generate data for plotting the normal distribution curve}
\NormalTok{x\_values }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{1000}\NormalTok{)}
\NormalTok{y\_values }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(x\_values, }\AttributeTok{mean =}\NormalTok{ mean\_value, }\AttributeTok{sd =}\NormalTok{ std\_dev)}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_values, }\AttributeTok{y =}\NormalTok{ y\_values)}

\CommentTok{\# Plot the normal distribution curve with shaded area under the curve}
\NormalTok{plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_area}\NormalTok{(}\AttributeTok{data =} \FunctionTok{subset}\NormalTok{(data, x }\SpecialCharTok{\textless{}=}\NormalTok{ z\_score), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ z\_score, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Normal Distribution with Z{-}Score of 1.25"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Z"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\CommentTok{\# Print the plot}
\FunctionTok{print}\NormalTok{(plot)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/z_score_example-1} 

}

\caption{Default caption for all figures}\label{fig:z_score_example}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Print out the results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Z{-}Score:"}\NormalTok{, z\_score, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Z-Score: 1.25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Probability (P{-}value) for Z \textless{}="}\NormalTok{, z\_score, }\StringTok{":"}\NormalTok{, }\FunctionTok{round}\NormalTok{(p\_value }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Probability (P-value) for Z <= 1.25 : 89.44 %
\end{verbatim}

\normalsize

In this code block: * We first generate a dataset of 100 normally
distributed random numbers with a mean of 50 and a standard deviation of
10. * We calculate the Z-scores for each value by subtracting the mean
and dividing by the standard deviation of the dataset. * Finally, we
plot the distribution of the Z-scores using a histogram to visualize
their spread.

This example showcases how the Z-score helps normalize data to a common
scale, making it easier to compare different values and identify
outliers in the dataset.

\newpage

\hypertarget{sigmoid-function}{%
\chapter{Sigmoid Function}\label{sigmoid-function}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $\sigma(x) = \frac{1}{1 + e^{-x}}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-4-Sigmoid.jpeg}
    \caption*{\Large A smooth on or off transition}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}
\begin{a_def_eq}{Sigmoid Function}{def4} 

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$z$] The \textbf{input variable}, which is the linear combination of features and weights in a model. It represents the score calculated before applying the sigmoid transformation and can take any real value.
    \vspace{0.5\baselineskip}
    \item[$\sigma(z)$] The \textbf{sigmoid output}, which is the transformed value of \( z \), mapped between 0 and 1. This output represents the probability that the input belongs to a particular class (e.g., 1 for positive, 0 for negative in binary classification).
    \vspace{0.5\baselineskip}
    \item[$e$] The mathematical constant, approximately equal to 2.718, which forms the base of the natural logarithm. The exponential term \( e^{-z} \) causes the sigmoid function to asymptotically approach 0 for large negative \( z \) and 1 for large positive \( z \).
    \vspace{0.5\baselineskip}
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: The sigmoid function is an activation function
that outputs values between 0 and 1.

\textbf{Description}: It maps any input value into a range between 0 and
1, making it suitable for probability-related tasks.

\textbf{Importance in ML}: The sigmoid function is widely used in
logistic regression and as an activation function in neural networks. It
helps in modeling binary classification problems and introduces
non-linearity into neural models.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-3}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-3}}

The sigmoid function, denoted as \(\sigma(x)\), is an activation
function widely used in machine learning and data science. It maps any
real-valued number to a value between 0 and 1, making it ideal for
scenarios where probability-related interpretations are essential,
especially binary classification problems.

This equation transforms an input \(x\) into an output within the range
of 0 to 1, effectively normalizing the input space. \begin{align}
\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x} = 1 - \sigma(-x)
\end{align}

This is a usefeul identity allows for simplifications in various
mathematical expressions and facilitates gradient-based optimization in
machine learning models. Note that the function is differentiable, which
is crucial for backpropagation in neural networks.

In many fields, particularly in artificial neural networks, the term
sigmoid function usually refers to the logistic function, a specific
type of S-shaped, or ``sigmoid,'' curve. While other functions like the
Gompertz curve or the ogee curve may appear similar because they share
the same general S-shape, these are distinct mathematical functions with
unique properties and applications.

A sigmoid function is characterized by its shape: it begins at a low
value, then increases rapidly in the middle, and finally levels off at a
high value. The logistic function, which is a common example, maps any
real number (from negative to positive infinity) to an output between 0
and 1. This makes it especially useful when working with probabilities
or values that need to be constrained within this range.

Other variations of sigmoid functions, like the hyperbolic tangent
function (tanh ⁡ tanh), also share the S-shaped curve but differ in their
output ranges. The hyperbolic tangent function, for example, produces
outputs between -1 and 1, which may be more suitable in cases where
negative values are meaningful (such as for representing balanced or
symmetric outcomes).

\hypertarget{general-overview-of-applications-of-sigmoid-functions}{%
\section{General Overview of Applications of Sigmoid
Functions}\label{general-overview-of-applications-of-sigmoid-functions}}

\begin{itemize}
\item
  Artificial Neural Networks: Sigmoid functions are widely used as
  activation functions, which means they determine whether a neuron in a
  neural network should be ``activated'' (i.e., pass on its signal). In
  particular, the logistic function is popular because it maps values to
  a range between 0 and 1, simulating the all-or-nothing activation seen
  in biological neurons. This behavior allows neural networks to make
  decisions in a gradual, continuous manner.
\item
  Cumulative Distribution Functions in Statistics: The sigmoid function
  is also useful in statistics, where it can represent the cumulative
  distribution function (CDF) of a probability distribution. In this
  context, the function's smooth, continuous increase models the
  accumulation of probability from one extreme to another, representing
  how likely it is to observe values up to a certain point.
\item
  Probabilistic Interpretation: When the output of the logistic sigmoid
  is interpreted as a probability (from 0 to 1), it enables the function
  to model binary classification problems, where there are only two
  possible outcomes (such as yes/no or true/false). By setting a
  threshold (e.g., 0.5), predictions can be made based on the
  probability that an input belongs to a certain class.
\item
  Invertibility: The logistic sigmoid function is invertible, meaning it
  has an inverse function called the logit function. The logit function
  transforms probabilities (between 0 and 1) back into raw values from
  all real numbers, which can be useful in logistic regression and other
  statistical models that seek to model relationships in probability
  terms.
\end{itemize}

In summary, sigmoid functions, particularly the logistic function, are
versatile tools in both machine learning and statistics due to their
S-shape, bounded output range, and smooth transition, which makes them
ideal for modeling gradual changes in various contexts.

\hypertarget{application-to-machine-learning-sigmoid-function-in-binary-classification}{%
\section{Application to Machine Learning: Sigmoid Function in Binary
Classification}\label{application-to-machine-learning-sigmoid-function-in-binary-classification}}

In binary classification tasks, the sigmoid function is particularly
valuable as it can be used to estimate the probability that a given
input belongs to a certain class. Logistic regression, for example, uses
the sigmoid function to model the probability of a binary outcome, with
predictions being based on whether the probability exceeds a specified
threshold (e.g., 0.5). By mapping outputs to a range between 0 and 1,
the sigmoid function enables interpretable probability scores and smooth
gradients for optimization.

\hypertarget{how-sigmoid-works-in-logistic-regression}{%
\section{How Sigmoid Works in Logistic
Regression}\label{how-sigmoid-works-in-logistic-regression}}

In logistic regression, the sigmoid function serves as the
``activation'' that transforms the output of a linear model into a
probability.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Linear Combination of Features}: First, we calculate a linear
  combination of the input features. For example, in a logistic
  regression model with two features \(x_1\) and \(x_2\), the model
  calculates:

  \begin{align}
  z = w_1 x_1 + w_2 x_2 + b
  \end{align}

  where \(w_1\) and \(w_2\) are weights learned from the data, and \(b\)
  is the bias term. This value \(z\) represents a ``score'' that is not
  yet bounded between 0 and 1.
\item
  \textbf{Applying the Sigmoid Transformation}: Next, we pass this
  linear combination \(z\) through the sigmoid function, which
  transforms it into a probability:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \end{align}

  This output \(\sigma(z)\) now represents the estimated probability
  that the input belongs to the positive class (e.g., ``spam'' or
  ``yes'').
\item
  \textbf{Thresholding for Classification}: Once we have the
  probability, we can decide on a threshold to classify the input. For
  example, if we set a threshold of 0.5, we would classify the input as
  belonging to the positive class if \(\sigma(z) \geq 0.5\), and to the
  negative class otherwise. This threshold is often set based on the
  problem's specific requirements and the balance of classes.
\end{enumerate}

\hypertarget{example-walkthrough}{%
\section{Example Walkthrough}\label{example-walkthrough}}

Imagine you're building a model to detect spam emails based on certain
features, like the presence of certain words or the frequency of
punctuation marks. Here's how the process might work step-by-step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Input Features}: Let's say we have two features:

  \begin{itemize}
  \tightlist
  \item
    \(x_1\): The frequency of the word ``free.''
  \item
    \(x_2\): The number of exclamation marks in the email.
  \end{itemize}
\item
  \textbf{Model Weights}: After training on a dataset, the model might
  assign the following weights:

  \begin{itemize}
  \tightlist
  \item
    \(w_1 = 1.2\) (suggesting ``free'' is an indicator of spam)
  \item
    \(w_2 = 0.8\) (suggesting a high number of exclamation marks also
    correlates with spam)
  \end{itemize}

  Let's assume the bias term \(b\) is -1.5.
\item
  \textbf{Calculate \(z\)}: For an email where \(x_1 = 2\) and
  \(x_2 = 3\) (meaning ``free'' appears twice and there are three
  exclamation marks), we calculate:

  \begin{align}
  z = (1.2 \times 2) + (0.8 \times 3) - 1.5 = 2.4 + 2.4 - 1.5 = 3.3
  \end{align}
\item
  \textbf{Apply Sigmoid to Get Probability}: Now, we pass \(z\) through
  the sigmoid function:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-3.3}} \approx 0.964
  \end{align}

  This probability, 0.964, indicates a high likelihood that the email is
  spam.
\item
  \textbf{Classification Decision}: If our threshold is 0.5, we would
  classify this email as ``spam'' since 0.964 \textgreater{} 0.5. This
  probability-based approach allows us to adjust the threshold based on
  how conservative or lenient we want our classification to be.
\end{enumerate}

\hypertarget{why-sigmoid-is-useful-for-optimization}{%
\section{Why Sigmoid is Useful for
Optimization}\label{why-sigmoid-is-useful-for-optimization}}

The sigmoid function also has smooth gradients, meaning that even small
changes in \(z\) lead to gradual changes in the output probability
\(\sigma(z)\). This smoothness is critical for optimization because it
allows gradient-based algorithms, such as gradient descent, to make
small, continuous updates to model parameters.

To demonstrate the smooth gradient of the sigmoid function
mathematically, we can derive its derivative, which shows how the output
changes with respect to changes in the input \(z\). This derivative will
show that the sigmoid function has a continuous and smooth gradient,
making it suitable for gradient-based optimization methods.

\hypertarget{smooth-gradient-of-the-sigmoid-function}{%
\subsection{Smooth Gradient of the Sigmoid
Function}\label{smooth-gradient-of-the-sigmoid-function}}

To understand why the sigmoid function has smooth gradients, let's
derive its first derivative. We can calculate the derivative of
\(\sigma(z)\) with respect to \(z\), which will tell us how sensitive
the output \(\sigma(z)\) is to small changes in \(z\).

\hypertarget{deriving-the-derivative-of-the-sigmoid-function}{%
\section{Deriving the Derivative of the Sigmoid
Function}\label{deriving-the-derivative-of-the-sigmoid-function}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with the sigmoid function:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \end{align}
\item
  To find \(\frac{d\sigma}{dz}\), we apply the chain rule. Let's rewrite
  the function in a form that will make it easier to differentiate:

  \begin{align}
  \sigma(z) = \left(1 + e^{-z}\right)^{-1}
  \end{align}
\item
  Now, differentiate with respect to \(z\):

  \begin{align}
  \frac{d\sigma}{dz} = -\left(1 + e^{-z}\right)^{-2} \cdot (-e^{-z})
  \end{align}
\item
  Simplify the expression:

  \begin{align}
  \frac{d\sigma}{dz} = \frac{e^{-z}}{\left(1 + e^{-z}\right)^2}
  \end{align}
\item
  Notice that we can rewrite \(e^{-z}\) in terms of \(\sigma(z)\)
  because:

  \begin{align}
  \sigma(z) = \frac{1}{1 + e^{-z}} \Rightarrow e^{-z} = \frac{1 - \sigma(z)}{\sigma(z)}
  \end{align}
\item
  Substitute this back to express \(\frac{d\sigma}{dz}\) in terms of
  \(\sigma(z)\):

  \begin{align}
  \frac{d\sigma}{dz} = \sigma(z)(1 - \sigma(z))
  \end{align}
\end{enumerate}

\hypertarget{interpretation-of-the-derivative}{%
\subsection{Interpretation of the
Derivative}\label{interpretation-of-the-derivative}}

The derivative of the sigmoid function,
\(\frac{d\sigma}{dz} = \sigma(z)(1 - \sigma(z))\), shows us that:

\begin{itemize}
\tightlist
\item
  The gradient is always positive and smooth for all values of \(z\).
\item
  This derivative is largest around \(z = 0\), where the sigmoid curve
  is steepest, and it becomes smaller as \(z\) moves towards positive or
  negative infinity, where the curve flattens out.
\end{itemize}

This smooth gradient is crucial for gradient descent, as it allows the
algorithm to make gradual updates to the weights. Because the derivative
never suddenly changes, the model can adjust parameters smoothly without
sudden jumps, aiding in stable and effective learning.

Thus, the sigmoid's smooth gradient makes it well-suited for
optimization in machine learning.

\hypertarget{typical-ranges-or-values-for-the-sigmoid-function}{%
\section{Typical Ranges or Values for the Sigmoid
Function}\label{typical-ranges-or-values-for-the-sigmoid-function}}

The sigmoid function is bounded by 0 and 1. At \(x = 0\), \(\sigma(x)\)
returns 0.5. As \(x \to \infty\), \(\sigma(x) \to 1\); and as
\(x \to -\infty\), \(\sigma(x) \to 0\). The sigmoid function's
characteristic ``S''-shaped curve makes it particularly suited for
gradual transitions between classes in classification tasks.

\hypertarget{important-mathematical-identities}{%
\section{Important Mathematical
Identities}\label{important-mathematical-identities}}

Some useful identities associated with the sigmoid function include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Derivative of the sigmoid: \begin{align}
  \sigma'(x) = \sigma(x)(1 - \sigma(x))
  \end{align}
\item
  Relationship with the hyperbolic tangent: \begin{align}
  \sigma(x) = \frac{1 + \tanh\left(\frac{x}{2}\right)}{2}
  \end{align}
\end{enumerate}

These identities assist in understanding the behavior of the sigmoid
function in neural networks and other machine learning algorithms.

\newpage

\hypertarget{comparative-functions}{%
\section{Comparative Functions}\label{comparative-functions}}

Several functions exhibit similar ``S''-shaped or smooth transition
behaviors. Here are some of them:

\[
\begin{array}{ll}
-\operatorname{erf}\left(\frac{\sqrt{\pi}}{2} x\right) & -\frac{x}{\sqrt{1+x^2}} \\
-\tanh (x) & -\frac{2}{\pi} \arctan \left(\frac{\pi}{2} x\right) \\
-\frac{2}{\pi} \operatorname{gd}\left(\frac{\pi}{2} x\right) & -\frac{x}{1+|x|}
\end{array}
\]

In addition to the sigmoid function, there are several other functions
that exhibit smooth, ``S''-shaped curves and are used for various
applications in machine learning and mathematics. Below are six of these
comparative functions, along with a brief description of their unique
properties.

\hypertarget{error-function}{%
\subsection{Error Function:}\label{error-function}}

The error function, denoted as \(\operatorname{erf}(x)\), is often used
in probability and statistics, particularly in relation to the normal
distribution. It approximates the integral of the Gaussian function, and
its shape closely resembles that of the sigmoid. The error function has
applications in fields such as signal processing and heat diffusion.

\begin{align}
-\operatorname{erf}\left(\frac{\sqrt{\pi}}{2} x\right)
\end{align}

\hypertarget{reciprocal-square-root}{%
\subsection{Reciprocal Square Root:}\label{reciprocal-square-root}}

This function smooths inputs similarly to the sigmoid but has a
different asymptotic behavior. Unlike the sigmoid, which approaches 1
and 0, this function approaches -1 and 1 as \(x\) tends to \(\infty\)
and \(-\infty\), respectively. It is sometimes used in neural networks
as an alternative activation function.

\begin{align}
-\frac{x}{\sqrt{1+x^2}}
\end{align}

\hypertarget{hyperbolic-tangent}{%
\subsection{Hyperbolic Tangent:}\label{hyperbolic-tangent}}

The hyperbolic tangent function, \(\tanh(x)\), is a popular alternative
to the sigmoid in neural networks because its range is from -1 to 1,
allowing for more balanced gradient flows during backpropagation. It
provides output symmetry, which can sometimes lead to faster training.

\begin{align}
-\tanh(x)
\end{align}

\hypertarget{arctangent}{%
\subsection{Arctangent:}\label{arctangent}}

This function maps inputs to a range between -1 and 1 similarly to the
hyperbolic tangent but has a slightly different curve. The arctangent
function is sometimes used in machine learning models when smaller
gradients or more gradual transitions are preferred.

\begin{align}
\frac{2}{\pi} \arctan \left(\frac{\pi}{2} x\right)
\end{align}

\hypertarget{gudermannian-function}{%
\subsection{Gudermannian Function:}\label{gudermannian-function}}

The Gudermannian function, \(\operatorname{gd}(x)\), provides a link
between circular and hyperbolic functions without involving complex
numbers. This function has applications in geometry and physics,
particularly in mapping problems.

\begin{align}
\frac{2}{\pi} \operatorname{gd}\left(\frac{\pi}{2} x\right)
\end{align}

\hypertarget{inverse-absolute}{%
\subsection{Inverse Absolute:}\label{inverse-absolute}}

This function smoothly scales inputs, but its range only extends from -1
to 1. Its asymptotic properties are particularly useful when a smoother
transition around zero is preferred.

\begin{align}
\frac{x}{1+|x|}
\end{align}

\newpage

\begin{figure}[htbp]
    \centering
    % Left Image (Sigmoid)
    \begin{minipage}[t]{1.0\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/Sigmoids_comparisons.jpeg}
        \caption{Summary of Alternate Sigmoid Like Curves}
    \end{minipage}
    \label{fig:sigmoid-layout}
\end{figure}

Note: All functions are normalized in such a way that their slope at the
origin is 1

\newpage

\hypertarget{correlation}{%
\chapter{Correlation}\label{correlation}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $\text{Correlation} = \frac{\text{Cov}(X, Y)}{\text{Std}(X) \cdot \text{Std}(Y)}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-5-correlation.jpeg}
    \caption*{\Large $x_i$ and $y_i$ are linearly related}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}
\begin{a_def_eq}{Correlation}{def5} 

\begin{align}\label{r_XY}
\text{Correlation} = \frac{\text{Cov}(X, Y)}{\text{Std}(X) \cdot \text{Std}(Y)}\\
\end{align}
\begin{align}\label{r_pearson}
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\end{align}

\begin{description}[align=left, labelwidth=2.2cm, labelsep=0em, leftmargin=2.2cm]
    \item[$\text{Cov}(X, Y)$] The \textbf{covariance} between vectors \( X \) and \( Y \), which measures the degree to which the two variables (or vectors) vary together.
    \item[$\text{Std}(V)$] The \textbf{standard deviation of vector \( V \)}, which measures the spread of the elements of \( Y \)  around each mean.
    \item[$\bar{x}$] The \textbf{mean of variable \( X \)}, calculated as \( \frac{1}{n} \sum x_i \), where \( n \) is the number of data points.
    \item[$\bar{y}$] The \textbf{mean of variable \( Y \)}, calculated as \( \frac{1}{n} \sum y_i \), where \( n \) is the number of data points.
    \item[$x_i$] The \textbf{individual data point} in the variable \( X \), where \( i \) indexes each observation in the dataset.
    \item[$y_i$] The \textbf{individual data point} in the variable \( Y \), where \( i \) indexes each observation in the dataset.
\end{description}

\end{a_def_eq}

\hfill\break
\textbf{Introduction}: Correlation measures the strength and direction
of the linear relationship between two variables.

\textbf{Description}: It ranges from -1 to 1, where values close to 1 or
-1 indicate a strong relationship, and values close to 0 indicate no
relationship.

\textbf{Importance in ML}: Correlation analysis helps in feature
selection by identifying which features are related to the target
variable. Strongly correlated features can be used for better
predictions, whereas uncorrelated features can be removed to simplify
models.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-4}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-4}}

Correlation is a statistical measure that describes the strength and
direction of a relationship between two variables. The most common
measure of correlation is Pearson's correlation coefficient, denoted as
\(r\), which ranges from -1 to 1. A positive value indicates a direct
relationship, while a negative value indicates an inverse relationship.
A value of 0 implies no linear relationship.

The equation for Pearson's correlation r coefficient \ref{r_pearson} can
be compared to the normalized version \ref{r_mu}:

\begin{align}\label{r_mu}
r_{\mu} \frac{\sum \left(x_i y_i\right)}{\sqrt{\sum x_i^2 \sum y_i^2}}
\end{align}

Where: - \(x_i\) and \(y_i\) are individual data points of variables
\(x\) and \(y\), - \(\bar{x}\) and \(\bar{y}\) are the means of
variables \(x\) and \(y\), respectively.

\begin{itemize}
\item
  \textbf{Variables}: The two variables, \(x\) and \(y\), whose
  relationship we are measuring. These could be anything from economic
  indicators to user behaviors.
\item
  \textbf{Data Points}: Each pair of values \((x_i, y_i)\) represents an
  observation.
\item
  \textbf{Means}: The mean values of \(x\) and \(y\) are used to
  standardize the data by subtracting them from each data point, which
  helps in normalizing the relationship.
\item
  \(x_i\) and \(y_i\) are individual data points for the two variables
  \(X\) and \(Y\).
\item
  \(\bar{x}\) and \(\bar{y}\) are the means (averages) of the respective
  variables.
\item
  \textbf{Numerator}: The numerator is the \textbf{covariance} between
  \(X\) and \(Y\), which measures how the two variables vary together.
\item
  \textbf{Denominator}: The denominator is the product of the
  \textbf{standard deviations} of \(X\) and \(Y\), which normalizes the
  covariance and ensures that the correlation coefficient lies between
  -1 and 1.
\end{itemize}

The Pearson correlation measures the strength and direction of the
\textbf{linear relationship} between two variables. This is the most
commonly used form of correlation.

\hypertarget{use-case}{%
\subsubsection{Use case:}\label{use-case}}

This equation is used when you want to understand the \textbf{linear
relationship} between two continuous variables and is the formula used
by the \texttt{cor()} function in R with the default method
(\texttt{method\ =\ "pearson"}).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{difference-in-interpretation}{%
\subsubsection{Difference in
Interpretation:}\label{difference-in-interpretation}}

\begin{itemize}
\tightlist
\item
  The pearson equation \ref{r_pearson} measures \textbf{how deviations
  from the mean of each variable are related}, i.e., it normalizes by
  the standard deviations of each variable.
\item
  The second equation \ref{r_mu} compares the raw products of the
  variables, but it does \textbf{not center} the data by subtracting the
  means. This makes the second equation more sensitive to the scales and
  magnitudes of the variables, which is why it's often less preferred
  for calculating correlation in its standard sense.
\end{itemize}

\hypertarget{key-differences}{%
\subsection{Key Differences:}\label{key-differences}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Normalization}:

  \begin{itemize}
  \tightlist
  \item
    The first equation involves centering the data by subtracting the
    means, which is important to account for the relationship of the
    variables in terms of deviations from their respective averages.
  \item
    The second equation does not involve centering the data. It uses the
    raw values of \(x_i\) and \(y_i\), making it sensitive to the
    overall scale and magnitude of the data.
  \end{itemize}
\item
  \textbf{Covariance vs Raw Product}:

  \begin{itemize}
  \tightlist
  \item
    The first equation is essentially computing a normalized
    \textbf{covariance} (divided by the product of the standard
    deviations of \(X\) and \(Y\)).
  \item
    The second equation uses the \textbf{raw product} of the variables,
    which lacks the standardization and may lead to different numerical
    results.
  \end{itemize}
\item
  \textbf{Interpretation}:

  \begin{itemize}
  \tightlist
  \item
    The first equation (Pearson's correlation) measures the strength of
    a \textbf{linear relationship}, normalized by the variance in both
    variables.
  \item
    The second equation is more general but is not commonly used as a
    standard for measuring correlation. It may appear in specific
    contexts, such as \textbf{cosine similarity} when the data is
    already standardized or when normalization is not necessary.
  \end{itemize}
\end{enumerate}

\hypertarget{final-point-summary-pearson-vs-normed-at}{%
\subsection{Final point summary Pearson vs normed
at}\label{final-point-summary-pearson-vs-normed-at}}

\begin{itemize}
\tightlist
\item
  \textbf{The first equation} \ref{r_pearson} is the \textbf{Pearson
  correlation coefficient}, the standard method for measuring linear
  relationships, normalized by the standard deviations of both
  variables.
\item
  \textbf{The second equation} \ref{r_mu} is a raw form that compares
  the products of the variables and is \textbf{not typically used} for
  Pearson's correlation, as it lacks normalization. It could be seen as
  a raw form of a \textbf{cosine similarity} (for vector comparison) or
  some other variant of correlation, but it's not generally used for
  typical correlation measurements.
\end{itemize}

\hypertarget{historical-context-1}{%
\section{Historical Context}\label{historical-context-1}}

The concept of correlation was first developed by Sir Francis Galton in
the 19th century. Galton's work laid the foundation for the field of
regression analysis and correlational statistics. The Pearson
correlation, developed by Karl Pearson in 1896, is the most widely used
form of correlation in modern statistics.

\hypertarget{understanding-the-notation}{%
\section{Understanding the Notation}\label{understanding-the-notation}}

The formula for Pearson's correlation coefficient involves the concept
of covariance, which measures how much two variables change together.
The denominator in the formula standardizes the covariance by dividing
by the product of the standard deviations of \(x\) and \(y\).

\begin{itemize}
\tightlist
\item
  \textbf{Covariance}: Measures the degree to which two variables move
  together.
\item
  \textbf{Standard Deviation}: Measures the spread of a variable around
  its mean.
\end{itemize}

\hypertarget{applications-in-machine-learning-2}{%
\section{Applications in Machine
Learning}\label{applications-in-machine-learning-2}}

In machine learning, correlation is used to: - Identify relationships
between input features and target variables, - Check for
multicollinearity, where highly correlated features might skew results,
- Perform feature selection, as correlated features may provide
redundant information.

For instance, if two features are highly correlated, one may be removed
from the model to simplify it without losing predictive power.

\hypertarget{correlation-in-r}{%
\section{Correlation in R}\label{correlation-in-r}}

To compute the correlation coefficient in R, you can use the built-in
\texttt{cor()} function:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example R code to compute Pearson\textquotesingle{}s correlation coefficient}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{correlation }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(x, y)}
\CommentTok{\# print\{correlation\}}
\FunctionTok{cat}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 5 4 3 2 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(correlation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -1
\end{verbatim}

\normalsize

\newpage

\hypertarget{scatter-plots-with-linear-overlays}{%
\section{Scatter Plots with Linear
Overlays}\label{scatter-plots-with-linear-overlays}}

In this extension of the correlation chapter, we will demonstrate how to
visualize the relationship between two variables using scatter plots and
add a linear regression line to the plots. We will use two different
sets of scatter data for illustration.

\hypertarget{example-1-scatter-data-with-linear-overlay-n_pdfmu-sigma}{%
\subsection{\texorpdfstring{Example 1: Scatter Data with Linear Overlay,
\(N_{pdf}(\mu, \sigma)\)}{Example 1: Scatter Data with Linear Overlay, N\_\{pdf\}(\textbackslash mu, \textbackslash sigma)}}\label{example-1-scatter-data-with-linear-overlay-n_pdfmu-sigma}}

\[
N_{pdf}(\mu,\sigma) = N_{pdf}(\mu=1,\sigma=1)
\]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example 1: Create scatter data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)  }\CommentTok{\# For reproducibility}

\CommentTok{\# Generate example data}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# 100 random data points from a normal distribution}
\NormalTok{y1 }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Linear relationship with some noise}

\CommentTok{\# Plotting the scatter plot with a linear regression line}
\FunctionTok{plot}\NormalTok{(x1, y1, }\AttributeTok{main =} \StringTok{"Scatter Plot with Linear Overlay Ex1"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"X1"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y1"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Add linear regression line}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/example1-1} 

}

\caption{Default caption for all figures}\label{fig:example1}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate Pearson\textquotesingle{}s correlation coefficient}
\NormalTok{r1 }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(x1, y1)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Pearson\textquotesingle{}s Correlation for Example 1: "}\NormalTok{, r1, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Pearson's Correlation for Example 1:  0.9775592
\end{verbatim}

\normalsize

\hypertarget{example-1-scatter-data-with-linear-overlay-and-some-noise}{%
\subsection{Example 1: Scatter Data with Linear Overlay and some
noise}\label{example-1-scatter-data-with-linear-overlay-and-some-noise}}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example 2: Create another set of scatter data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{24}\NormalTok{)  }\CommentTok{\# For reproducibility}

\CommentTok{\# Generate another set of example data}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)  }\CommentTok{\# 100 random data points from a different normal distribution}
\NormalTok{y2 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ x2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Another linear relationship with noise}

\CommentTok{\# Plotting the scatter plot with a linear regression line}
\FunctionTok{plot}\NormalTok{(x2, y2, }\AttributeTok{main =} \StringTok{"Scatter Plot with Linear Overlay Ex2"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"X2"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y2"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x2), }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Add linear regression line}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/example2-1} 

}

\caption{Default caption for all figures}\label{fig:example2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate Pearson\textquotesingle{}s correlation coefficient}
\NormalTok{r2 }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(x2, y2)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Pearson\textquotesingle{}s Correlation for Example 2: "}\NormalTok{, r2, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Pearson's Correlation for Example 2:  0.783414
\end{verbatim}

\normalsize

\newpage

\hypertarget{cosine-similarity}{%
\chapter{Cosine Similarity}\label{cosine-similarity}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $S_C(A, B):=\cos (\theta)=\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-6-cosine_similarity.jpeg}
    \caption*{\Large You're comparative angle and measure}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}
\begin{a_def_eq}{Cosine Similarity}{def6} 

\begin{align}\label{cosine_sim_full}
\text { cosine similarity }=S_C(A, B):=\cos (\theta)=\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}=\frac{\sum_{i=1}^n a_i b_i}{\sqrt{\sum_{i=1}^n a_i^2} \cdot \sqrt{\sum_{i=1}^n b_i^2}}
\end{align}

\begin{description}[align=left, labelwidth=2.2cm, labelsep=0em, leftmargin=2.2cm] 
    \item[$A$] The \textbf{first vector} \( A \), which represents the features (such as word frequencies or embedding values) of the first document or data point. 
    \vspace{0.5\baselineskip}
    \item[$B$] The \textbf{second vector} \( B \), which represents the features of the second document or data point. 
    \vspace{0.5\baselineskip}
    \item[$A \cdot B$] The \textbf{dot product} of vectors \( A \) and \( B \), which computes the sum of the products of corresponding components of the two vectors. This measures how aligned the vectors are in the same direction.
    \vspace{0.5\baselineskip}
    \item[$\|A\|$] The \textbf{norm of vector \( A \)}, calculated as \( \|A\| = \sqrt{\sum A_i^2} \), which measures the magnitude (length) of vector \( A \).
    \vspace{0.5\baselineskip}
    \item[$\|B\|$] The \textbf{norm of vector \( B \)}, calculated as \( \|B\| = \sqrt{\sum B_i^2} \), which measures the magnitude (length) of vector \( B \).
    \vspace{0.5\baselineskip}
    \item[$\text{similarity}$] The \textbf{cosine similarity} between vectors \( A \) and \( B \), which quantifies the cosine of the angle between the vectors, effectively measuring the directional alignment or similarity between them.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: Cosine similarity measures the cosine of the
angle between two vectors in a multi-dimensional space.

\textbf{Description}: It is commonly used to determine how similar two
documents are, regardless of their size, by comparing their word
vectors.

\textbf{Importance in ML}: Cosine similarity is crucial in text mining
and information retrieval applications. It is often used in clustering
and classification algorithms for textual data, where measuring the
similarity between vectors is needed.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-5}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-5}}

\hypertarget{cosine-similarity-technically-speaking}{%
\subsection{Cosine Similarity Technically
speaking}\label{cosine-similarity-technically-speaking}}

Cosine similarity \ref{cosine_sim_full} measures the cosine of the angle
between two vectors \(\mathbf{A}\) and \(\mathbf{B}\) in a
multi-dimensional space. The equation for cosine similarity is given by:

Where: - \(\mathbf{A} \cdot \mathbf{B}\) is the dot product of vectors
\(\mathbf{A}\) and \(\mathbf{B}\), - \(\| \mathbf{A} \|\) and
\(\| \mathbf{B} \|\) are the magnitudes (or norms) of vectors
\(\mathbf{A}\) and \(\mathbf{B}\).

This formula results in a value between -1 and 1, where: - 1 indicates
the vectors are identical (or point in the same direction), - 0
indicates the vectors are orthogonal (i.e., no similarity), - -1
indicates the vectors are diametrically opposed (i.e., completely
dissimilar).

Cosine similarity is a measure used to compute the \textbf{cosine of the
angle} between two non-zero vectors in an inner product space. It is
commonly used to measure the similarity between documents in text
mining, recommendation systems, and machine learning tasks. The basic
idea is to calculate how similar two vectors are, regardless of their
magnitudes, by comparing their directions.

Where: - \(\mathbf{A} \cdot \mathbf{B}\) is the \textbf{dot product} of
vectors \(\mathbf{A}\) and \(\mathbf{B}\), - \(\| \mathbf{A} \|\) and
\(\| \mathbf{B} \|\) are the \textbf{norms} (or magnitudes) of the
vectors \(\mathbf{A}\) and \(\mathbf{B}\), respectively.

Cosine similarity ranges from -1 to 1: - \(1\) indicates that the
vectors are identical (i.e., they point in the same direction), - \(0\)
indicates orthogonality (no similarity), - \(-1\) indicates that the
vectors are diametrically opposed (pointing in opposite directions).

\begin{itemize}
\item
  \textbf{Dot Product}: The dot product \(A \cdot B\),
  \(A \cdot B = \sum_{i=1}^{n} a_i b_i\), is calculated as the sum of
  the products of corresponding elements in the vectors. It measures how
  much two vectors point in the same direction. Mathematically, the dot
  product is represented as: Where \(a_i\) and \(b_i\) represent the
  individual components of vectors \(A\) and \(B\), respectively. This
  formula sums up the element-wise product of the two vectors.
\item
  \textbf{Magnitude}: The magnitude of a vector \(A\) is calculated as
  \(\|A\| = \sqrt{\sum_{i=1}^{n} a_i^2}\), where \(a_i\) represents the
  individual components of vector \(A\). The magnitude measures the
  length of the vector.
\item
  \textbf{Cosine of the Angle}: The cosine of the angle between the
  vectors is the core of cosine similarity. If the vectors are pointing
  in the same direction, the cosine value will be 1, indicating high
  similarity.
\end{itemize}

\hypertarget{cosine-similarity-mathematically-speaking}{%
\subsection{Cosine Similarity Mathematically
speaking}\label{cosine-similarity-mathematically-speaking}}

Mathematically, the cosine similarity between two vectors \(\mathbf{A}\)
and \(\mathbf{B}\) is given by:

\[
\text{similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}
\]

Where: - \(\mathbf{A} \cdot \mathbf{B}\) is the dot product of vectors
\(\mathbf{A}\) and \(\mathbf{B}\), - \(\|\mathbf{A}\|\) and
\(\|\mathbf{B}\|\) are the norms (magnitudes) of the vectors.

Now, considering linear separability, for two classes of data to be
linearly separable, the vectors corresponding to those classes should be
\textbf{distinct} in terms of their direction. This can be formalized as
follows:

\begin{itemize}
\item
  If the vectors for the two classes are aligned (i.e., pointing in the
  same direction), their cosine similarity will be close to \textbf{1},
  indicating that the classes are \textbf{highly similar} and not
  linearly separable.
\item
  If the vectors are \textbf{orthogonal}, their cosine similarity will
  be \textbf{0}, implying that they are \textbf{completely different}
  and hence easily separable by a linear classifier.
\end{itemize}

To represent this mathematically, if we have two vectors
\(\mathbf{A}_1\) and \(\mathbf{A}_2\) representing the two classes, the
cosine similarity between them can be used as follows:

\[
\mathbf{A}_1 \cdot \mathbf{A}_2 = \|\mathbf{A}_1\| \|\mathbf{A}_2\| \cos(\theta)
\]

Where: - \(\theta\) is the angle between the two vectors.

If the vectors are \textbf{orthogonal} (i.e., \(\theta = 90^\circ\)),
then:

\[
\cos(90^\circ) = 0 \quad \Rightarrow \quad \mathbf{A}_1 \cdot \mathbf{A}_2 = 0
\]

This means that the cosine similarity is 0, and the two classes are
perfectly separable using a linear classifier.

For the vectors to be \textbf{linearly separable}, a good condition is
that the cosine similarity between the class vectors should be
\textbf{low} or \textbf{zero}. If the cosine similarity is high (close
to 1), it means that the vectors are pointing in similar directions, and
the classes might not be easily separable by a linear boundary.

\textbf{In summary:} \textbf{In summary:}

\begin{itemize}
\tightlist
\item
  \textbf{Cosine similarity close to 1}: Vectors are pointing in the
  same direction, classes are \textbf{not separable}.
\item
  \textbf{Cosine similarity close to 0}: Vectors are orthogonal, classes
  are \textbf{easily separable}.
\end{itemize}

This is crucial for algorithms like \textbf{SVMs} and \textbf{Logistic
Regression}, which rely on the geometric properties of the data for
finding the decision boundary.

\newpage

\hypertarget{historical-context-2}{%
\section{Historical Context}\label{historical-context-2}}

Cosine similarity has become a cornerstone of text mining and Natural
Language Processing (NLP) due to its ability to compare documents and
their word distributions. Its origins and application milestones are
tied closely to developments in both computational linguistics and
information retrieval systems. Below is an expanded historical overview
of cosine similarity, focusing on its key milestones in the evolution of
machine learning, text mining, and NLP.

\hypertarget{the-historical-development-of-cosine-similarity}{%
\subsection{The Historical Development of Cosine
Similarity}\label{the-historical-development-of-cosine-similarity}}

\begin{itemize}
\tightlist
\item
  Early Foundations: Vector Space Models
\end{itemize}

Cosine similarity owes much of its origin to the Vector Space Model
(VSM), a mathematical framework developed in the 1950s and 1960s for
representing text documents as vectors in a multi-dimensional space.

1950s-1960s: Early work in information retrieval (IR) focused on using
Boolean models and term frequency approaches to determine document
relevance. However, these models lacked the ability to handle the
nuances of natural language. The Vector Space Model (VSM), first
popularized by Gerard Salton and colleagues at Cornell University in the
late 1960s, brought about a shift in how text was represented. In VSM, a
document was transformed into a vector, where each dimension represented
the frequency of a term or a word in that document. This allowed for
documents to be compared based on their word distributions, thus setting
the stage for using similarity measures.

\begin{itemize}
\tightlist
\item
  The Birth of Cosine Similarity in Information Retrieval (1970s-1980s)
\end{itemize}

The concept of cosine similarity, specifically as a measure of the
angular distance between two document vectors, emerged as a way to
quantify the similarity between documents without being affected by
their lengths.

1970s: Salton and McGill in their seminal book Introduction to Modern
Information Retrieval (1983) formalized the use of cosine similarity as
part of the Vector Space Model. The key insight was that the cosine of
the angle between two document vectors provided a measure of how similar
the documents were, regardless of their lengths or word counts. This
normalized measure became the foundation for retrieval models in IR
systems. 1980s-1990s: Cosine similarity began to dominate the field of
information retrieval, where the goal was to rank documents in response
to queries. The Vector Space Model, with cosine similarity as its
central metric, was widely used in database search engines and document
retrieval systems. During this period, IR research emphasized the
importance of efficiently comparing documents in large datasets using
similarity measures like cosine similarity. The Rise of Natural Language
Processing (NLP) and Text Mining (1990s-2000s)

With the rise of computational linguistics and NLP, cosine similarity
found itself at the heart of various machine learning and text analysis
applications. The key development in this period was the shift from
purely mathematical and statistical approaches to semantics-driven
models in language processing.

\begin{itemize}
\tightlist
\item
  Cosine Similarity into the 1990s:
\end{itemize}

As large datasets and corpora of text became available (such as the
Reuters-21578 dataset for news classification), cosine similarity was
used to measure the similarity between documents and query-document
pairs in information retrieval. This time also marked the rise of TF-IDF
(Term Frequency-Inverse Document Frequency), a weighting scheme that was
commonly combined with cosine similarity to enhance document comparison
by weighting terms based on their importance. Late 1990s-2000s: Latent
Semantic Analysis (LSA), introduced by Deerwester et al.~in 1990,
expanded the role of cosine similarity. LSA used singular value
decomposition (SVD) to reduce the dimensionality of term-document
matrices, with cosine similarity serving as the central metric to
identify semantic similarity between documents. This was particularly
useful for handling synonymy and polysemy (words having multiple
meanings).

The development of word embeddings (early models like Latent Dirichlet
Allocation (LDA), and later Word2Vec, GloVe, and FastText) built upon
the concepts introduced by LSA, enabling more nuanced similarity
measures for word and document comparisons. Cosine Similarity in Modern
Machine Learning (2010s-present)

With the advent of deep learning and large-scale natural language models
like BERT and GPT, cosine similarity continues to be widely used in
modern NLP tasks, but with significant advancements.

\begin{itemize}
\tightlist
\item
  Cosine similarity in the birth of the ML age 2010s:
\end{itemize}

Cosine similarity remains one of the most widely used metrics in
document clustering, topic modeling, and recommendation systems. The
advent of word embeddings like Word2Vec (by Tomas Mikolov et al.~at
Google) revolutionized cosine similarity, as it enabled semantic word
vector representations rather than just term frequency counts.

Word2Vec and GloVe create dense vector representations of words, and
cosine similarity became a powerful way to compare these dense vectors
and measure the similarity of word meanings. This was a significant leap
from counting word occurrences to capturing the contextual relationships
between words, documents, or even entire corpora.

Deep Learning Integration:

In modern applications, transformer-based models such as BERT, GPT, and
T5 leverage pre-trained embeddings that can be compared using cosine
similarity for tasks like semantic textual similarity and document
ranking. These models capture fine-grained meanings of words and
sentences, and cosine similarity is often used to compare the output of
these models for tasks such as document retrieval, question answering,
and text classification.

\begin{itemize}
\tightlist
\item
  Cosine Similarity in Information Retrieval and Search Engines
  (2010s-present)
\end{itemize}

The field of search engines has seen a continual refinement of cosine
similarity, especially in the era of semantic search:

Query-Document Matching: Traditional search engines used cosine
similarity to rank documents by relevance to a given query. With the
advent of semantic search, search engines now use more sophisticated
embeddings (e.g., from BERT or T5) to map both queries and documents
into a shared semantic space, where cosine similarity is used to measure
their relevance, not just based on keyword overlap but on the underlying
meaning of the content.

Reinforcement Learning and Ranking: Modern search engines like Google,
Bing, and Yahoo still use cosine similarity in ranking documents,
although they have become more sophisticated with reinforcement
learning-based approaches that adjust ranking according to user
interactions. The query understanding has evolved from simple keyword
matching to contextual semantic understanding powered by neural
networks.

\begin{itemize}
\tightlist
\item
  Milestones in the Application of Cosine Similarity in ML
\end{itemize}

Early Applications in Information Retrieval (1970s-1980s): Cosine
similarity was first used in information retrieval to compare documents
based on term frequency vectors. Introduction of LSA (1990s): LSA
expanded the role of cosine similarity, allowing for the comparison of
documents in a semantic space.

Word Embeddings (2000s-present): Cosine similarity became integral in
word embeddings (e.g., Word2Vec, GloVe), which represent words in
continuous vector spaces, enabling the comparison of words and documents
at a semantic level. Transformer Models (2018-present): Cosine
similarity continues to be used in modern semantic search engines,
document clustering, and semantic textual similarity tasks, powered by
state-of-the-art transformer-based language models like BERT and GPT.

\hypertarget{cosine-similarity---the-basics-and-the-usage}{%
\section{Cosine Similarity - The basics and the
usage}\label{cosine-similarity---the-basics-and-the-usage}}

Cosine similarity is a fascinating measure, and there are a few
interesting mathematical identities, properties, and quirks related to
it. Here are some of them:

\hypertarget{cosine-similarity-and-the-angle-between-vectors}{%
\subsection{Cosine Similarity and the Angle Between
Vectors:}\label{cosine-similarity-and-the-angle-between-vectors}}

Cosine similarity essentially measures the cosine of the angle between
two vectors in multidimensional space. This brings us to some
interesting geometric properties: - Zero Similarity: If the cosine
similarity is 0 , it implies that the angle between the vectors is
\(90^{\circ}\) (i.e., the vectors are orthogonal). In text mining, this
would mean two documents have no terms in common. - Negative Similarity:
A negative cosine similarity indicates that the vectors are pointing in
opposite directions (i.e., the angle is greater than \(90^{\circ}\) but
less than \(180^{\circ}\) ). - Unit Vectors: If the vectors \(A\) and
\(B\) are normalized to unit vectors (i.e., \(\|A\|=\|B\|=1\) ), then
cosine similarity becomes equivalent to the dot product \(A \cdot B\).
This property makes cosine similarity a normalized version of the dot
product, and it's particularly useful in highdimensional spaces like
text data, where magnitude can vary widely across documents.

\hypertarget{cosine-similarity-and-the-inner-product}{%
\subsection{Cosine Similarity and the Inner
Product:}\label{cosine-similarity-and-the-inner-product}}

Cosine similarity is directly related to the inner product (or dot
product) of vectors.

\begin{itemize}
\tightlist
\item
  When you normalize the vectors (making them unit vectors), cosine
  similarity becomes the inner product itself. This is why cosine
  similarity is sometimes referred to as a normalized form of the dot
  product.
\item
  In vector spaces, the inner product captures both the magnitude and
  direction of the vectors, and cosine similarity isolates only the
  directional similarity by normalizing out the magnitudes.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  **Connection with the Euclidean Distance:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Interestingly, cosine similarity is connected to Euclidean distance
  through a well-known identity:
\end{itemize}

\[
\text { Cosine Similarity }=1-\frac{\|A-B\|^2}{\|A\|^2+\|B\|^2}= \frac{A \cdot B}{\|A\|\|B\|}
\]

This identity shows that the cosine similarity can be interpreted in
terms of the squared Euclidean distance between the vectors. High cosine
similarity means low Euclidean distance, and vice versa. This provides a
duality between these two commonly used measures of similarity in
machine learning.

\newpage

\hypertarget{cosine-similarity-and-linear-separability}{%
\section{Cosine Similarity and Linear
Separability}\label{cosine-similarity-and-linear-separability}}

Let's begin by recalling the formulation of linear separability from the
matrix formulation and connecting it to cosine similarity: 1. Linear
Separability Condition: For a set of data points \(\mathbf{x}_i\) with
labels \(y_i\), the condition for linear separability is:

\[
y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right)>0, \quad \forall i=1,2, \ldots, n
\]

This condition means that all data points are correctly classified by
the hyperplane, which is determined by w and \(b\). 2. Cosine Similarity
and Separation: The cosine similarity between two vectors
\(\mathbf{x}_i\) and \(\mathbf{w}\) (where w is the normal vector to the
hyperplane) is given by:

\[
\operatorname{similarity}\left(\mathbf{x}_i, \mathbf{w}\right)=\frac{\mathbf{w}^T \mathbf{x}_{\mathbf{i}}}{\|\mathbf{w}\|\left\|\mathbf{x}_i\right\|}
\]

\begin{itemize}
\tightlist
\item
  When cosine similarity is high (close to 1 ), the vector
  \(\mathbf{x}_i\) is pointing in a similar direction to \(\mathbf{w}\),
  and the data point \(\mathbf{x}_i\) is not well separated from other
  points in the feature space.
\item
  When cosine similarity is low (close to 0 ), the vectors are
  orthogonal, indicating that the points from different classes are
  well-separated, and the decision boundary (hyperplane) is more likely
  to be effective at separating the two classes.
\item
  Linear Separability and Margin Maximization: In Support Vector
  Machines (SVM), the goal is to find the optimal hyperplane that
  maximizes the margin between the two classes. The margin is the
  perpendicular distance from the closest data points (the support
  vectors) to the hyperplane. The SVM optimization problem is formulated
  as:
\end{itemize}

\[
\text { Maximize } \frac{2}{\|\mathbf{w}\|}
\]

Subject to the constraint:

\[
y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right) \geq 1, \quad \forall i
\]

This ensures that the data points are classified correctly with the
maximum margin, which is geometrically related to the angle between the
weight vector \(\mathbf{w}\) and the data points \(\mathbf{x}_i\).

\hypertarget{how-cosine-similarity-relates-to-the-margin-in-svm}{%
\subsection{How Cosine Similarity Relates to the Margin in
SVM:}\label{how-cosine-similarity-relates-to-the-margin-in-svm}}

The margin in SVM is inversely proportional to the magnitude of the
weight vector w. A larger margin corresponds to a lower value of
\textbar w \textbar\textbar. Since cosine similarity measures the
alignment between the vectors, it directly relates to the angle between
the weight vector \(\mathbf{w}\) and the data vectors \(\mathbf{x}_i\) :

\begin{itemize}
\tightlist
\item
  A large cosine similarity means the vectors are pointing in the same
  direction, making it harder for the classifier to separate the classes
  with a hyperplane.
\item
  A low cosine similarity (especially close to 0 ) means the vectors are
  more orthogonal, making the data points more separable.
\end{itemize}

\hypertarget{full-mathematical-representa-tion-connecting-linear-separability-and-cosine-similarity}{%
\subsection{Full Mathematical Representa tion Connecting Linear
Separability and Cosine
Similarity:}\label{full-mathematical-representa-tion-connecting-linear-separability-and-cosine-similarity}}

To summarize the relationship between cosine similarity and linear
separability, we have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cosine similarity between the weight vector \(\mathbf{w}\) and each
  data point \(\mathbf{x}_i\):
\end{enumerate}

\[
\operatorname{similarity}\left(\mathbf{x}_i, \mathbf{w}\right)=\frac{\mathbf{w}^T \mathbf{x}_i}{\|\mathbf{w}\|\left\|\mathbf{x}_i\right\|}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For linear separability, the data points must satisfy the condition:
\end{enumerate}

\[
y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right)>0, \quad \forall i=1,2, \ldots, n
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The margin between the classes is maximized when the cosine similarity
  between the weight vector \(\mathbf{w}\) and the data points
  \(\mathbf{x}_i\) is low (i.e., the vectors are orthogonal):
\end{enumerate}

\[
\text { Maximize margin: } \frac{2}{\|\mathbf{w}\|}
\]

This leads to a large margin when cosine similarity is small, ensuring
that the data points are easily separable by the hyperplane.

Thus, the cosine similarity between the weight vector \(\mathbf{w}\) and
the data points \(\mathbf{x}_i\) plays a key role in determining the
separability of the data. When cosine similarity is low, it indicates
that the data is well separated and linearly separable.

\newpage

\hypertarget{cosine-similarity-with-sparse-vectors}{%
\subsection{Cosine Similarity with Sparse
Vectors:}\label{cosine-similarity-with-sparse-vectors}}

\begin{itemize}
\tightlist
\item
  One interesting property of cosine similarity is its robustness in
  high-dimensional, sparse data (such as text data). In sparse
  high-dimensional spaces, many of the components of the vectors are
  zero, and the cosine similarity reduces to comparing the non-zero
  entries of the vectors. This makes it very useful in scenarios like
  text analysis or recommendation systems, where documents or items are
  represented by high-dimensional sparse vectors, and many entries in
  these vectors are zero.
\end{itemize}

\hypertarget{cosine-similarity-as-a-measure-of-clustering-quality}{%
\subsection{Cosine Similarity as a Measure of Clustering
Quality:}\label{cosine-similarity-as-a-measure-of-clustering-quality}}

\begin{itemize}
\tightlist
\item
  Cosine similarity is widely used in clustering algorithms, such as
  k-means and k-medoids, especially in text clustering. It helps measure
  how well different data points (such as documents) are grouped
  together. Interestingly, when cosine similarity is used in clustering:
\item
  Cosine similarity doesn't rely on absolute magnitude: Unlike Euclidean
  distance, it doesn't care about the vector magnitudes, so documents of
  different lengths or sizes can still be compared meaningfully.
\item
  Cluster Centers in Text Mining: When performing clustering with text
  data, the center of a cluster (the centroid) might not be a point in
  the vector space, but it's computed as the mean of cosine similarities
  with respect to all other points in the cluster.
\end{itemize}

\hypertarget{cosine-similarity-and-vector-scaling}{%
\subsection{Cosine Similarity and Vector
Scaling:}\label{cosine-similarity-and-vector-scaling}}

\begin{itemize}
\tightlist
\item
  Scaling the vectors (by multiplying them by a constant) does not
  affect the cosine similarity. This is because cosine similarity is a
  function of the angle between vectors, not their magnitude. Hence, if
  two vectors point in the same direction but have different magnitudes,
  their cosine similarity will be 1. This is a useful property in NLP,
  where scaling the vector components doesn't change their relative
  orientation.
\end{itemize}

\hypertarget{cosine-similarity-as-a-measure-of-clustering-quality-1}{%
\subsection{Cosine Similarity as a Measure of Clustering
Quality:}\label{cosine-similarity-as-a-measure-of-clustering-quality-1}}

\begin{itemize}
\tightlist
\item
  Cosine similarity is widely used in clustering algorithms, such as
  k-means and k-medoids, especially in text clustering. It helps measure
  how well different data points (such as documents) are grouped
  together. Interestingly, when cosine similarity is used in clustering:
\item
  Cosine similarity doesn't rely on absolute magnitude: Unlike Euclidean
  distance, it doesn't care about the vector magnitudes, so documents of
  different lengths or sizes can still be compared meaningfully.
\item
  Cluster Centers in Text Mining: When performing clustering with text
  data, the center of a cluster (the centroid) might not be a point in
  the vector space, but it's computed as the mean of cosine similarities
  with respect to all other points in the cluster.
\end{itemize}

\hypertarget{cosine-similarity-and-vector-scaling-1}{%
\subsection{Cosine Similarity and Vector
Scaling:}\label{cosine-similarity-and-vector-scaling-1}}

\begin{itemize}
\tightlist
\item
  Scaling the vectors (by multiplying them by a constant) does not
  affect the cosine similarity. This is because cosine similarity is a
  function of the angle between vectors, not their magnitude. Hence, if
  two vectors point in the same direction but have different magnitudes,
  their cosine similarity will be 1. This is a useful property in NLP,
  where scaling the vector components doesn't change their relative
  orientation.
\end{itemize}

\hypertarget{cosine-similarity-as-a-measure-of-word-similarity-in-nlp}{%
\subsection{Cosine Similarity as a Measure of Word Similarity in
NLP:}\label{cosine-similarity-as-a-measure-of-word-similarity-in-nlp}}

\begin{itemize}
\tightlist
\item
  In NLP, word embeddings like Word2Vec and GloVe represent words as
  vectors. Cosine similarity is often used to measure semantic
  similarity between words based on their vector representations. Words
  with similar meanings (like ``king'' and ``queen'') will have high
  cosine similarity, even though they are not identical in form.
\end{itemize}

\hypertarget{cosine-similarity-as-an-angular-measure}{%
\subsection{Cosine Similarity as an Angular
Measure:}\label{cosine-similarity-as-an-angular-measure}}

\begin{itemize}
\tightlist
\item
  The cosine similarity is essentially the cosine of the angle between
  the two vectors. If the two vectors are exactly the same, the cosine
  similarity is 1 (indicating a \(0^{\circ}\) angle). If they are
  perpendicular, the cosine similarity is 0 (indicating a \(90^{\circ}\)
  angle). If they are opposite, the cosine similarity is -1 (indicating
  a \(180^{\circ}\) angle).
\item
  This geometric interpretation is useful in visualizing vector spaces.
  For instance, when comparing documents, vectors pointing in the same
  direction are highly similar, and vectors pointing in opposite
  directions are highly dissimilar.
\end{itemize}

\hypertarget{cosine-similarity-and-textual-data}{%
\subsection{Cosine Similarity and Textual
Data:}\label{cosine-similarity-and-textual-data}}

\begin{itemize}
\tightlist
\item
  In many information retrieval systems (like search engines), documents
  are represented by TF-IDF vectors (Term Frequency-Inverse Document
  Frequency). Cosine similarity is used to measure the relevance of a
  document to a query by comparing the query vector to the document
  vectors. Interestingly, because of the high-dimensional and sparse
  nature of TF-IDF vectors, cosine similarity becomes one of the most
  efficient ways to compare documents, avoiding issues that might arise
  with traditional distance measures like Euclidean distance.
\end{itemize}

\hypertarget{cosine-similarity-and-regularization}{%
\subsection{Cosine Similarity and
Regularization:}\label{cosine-similarity-and-regularization}}

\begin{itemize}
\tightlist
\item
  Cosine similarity also plays a role in some machine learning models,
  especially those used for regularization. For instance, in sparse
  representations (such as Lasso regression or feature selection in
  text), cosine similarity can be used to measure how closely related a
  feature is to another, helping in the regularization process by
  identifying similar features or words.
\end{itemize}

\hypertarget{relation-to-spherical-geometry}{%
\section{Relation to Spherical
Geometry:}\label{relation-to-spherical-geometry}}

Cosine similarity is deeply connected to spherical geometry. In fact, it
is the spherical distance (or the angular distance) between two points
on the unit sphere. The closer two vectors are in the direction they
point, the smaller the angle between them, and therefore, the higher the
cosine similarity. - This connection becomes evident when vectors are
normalized (as unit vectors) and placed on the surface of a unit sphere.
In this setting, the cosine of the angle between the vectors directly
gives the cosine of the spherical angle between them, which is central
in many algorithms that perform clustering and classification based on
angles, such as kmeans clustering on normalized data.

\hypertarget{cosine-similarity-and-kernel-methods}{%
\section{Cosine Similarity and Kernel
Methods:}\label{cosine-similarity-and-kernel-methods}}

Cosine similarity can also be seen as a kernel function in the context
of machine learning. In the context of support vector machines (SVMs)
and other kernel-based learning methods, cosine similarity can serve as
a linear kernel for high-dimensional vector spaces. - The linear kernel
in SVM is essentially equivalent to calculating the cosine similarity
between data points when the data is centered and normalized. Thus, it
provides a way to map the data into a high-dimensional space without
explicitly calculating the dot product in that space.

\hypertarget{cosine-similarity-and-information-retrieval}{%
\section{Cosine Similarity and Information
Retrieval:}\label{cosine-similarity-and-information-retrieval}}

In information retrieval (IR), cosine similarity is used to compare the
vector representations of documents. The documents are typically
represented in terms of their TF-IDF (Term Frequency-Inverse Document
Frequency) vectors, and cosine similarity is used to measure how similar
a query is to a document. - In this context, cosine similarity measures
the angular distance between the query vector and document vectors.
Despite differences in the document lengths, the cosine similarity
focuses on the relative frequency of words, making it especially useful
for content based recommendation systems and document clustering.

\hypertarget{cosine-similarity-and-the-norms-of-vectors}{%
\section{Cosine Similarity and the Norms of
Vectors:}\label{cosine-similarity-and-the-norms-of-vectors}}

Cosine similarity is inherently related to vector norms. In particular,
Euclidean distance (L.2 norm) is closely tied to cosine similarity, but
cosine similarity itself is not a norm in the traditional sense. Let's
break this down: - Euclidean Norm (L2 norm): The L.2 norm (also called
the Euclidean norm) of a vector
\(\mathbf{A}=\left[a_1, a_2, \ldots, a_n\right]\) is defined as:

\[
\|\mathbf{A}\| 2=\sqrt{a_1^2+a_2^2+\ldots+a_n^2}
\]

It measures the ``length'' or magnitude of the vector. - Cosine
Similarity: Cosine similarity between two vectors \(\mathbf{A}\) and
\(\mathbf{B}\) is given by:

\[
\operatorname{similarity}(\mathbf{A}, \mathbf{B})=\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|_2\|\mathbf{B}\|_2}
\]

Notice that cosine similarity involves normalizing the vectors by their
L2 norms. In this case, cosine similarity is related to the cosine of
the angle between the vectors, which is a directional measure rather
than a magnitude measure. 2. Cosine Similarity vs.~Norms:

Cosine similarity itself is not a norm, but it is closely related to the
concept of a normalized inner product. - A norm is a function that
satisfies the following properties: - Non-negativity:
\(\|\mathbf{A}\| \geq 0\) with equality if and only if \(\mathbf{A}=0\),
- Homogeneity: \(\|\alpha \mathbf{A}\|=|\alpha| \mid \mathbf{A} \|\) for
any scalar \(\alpha\), - Triangle inequality:
\(\|\mathbf{A}+\mathbf{B}\| \leq\|\mathbf{A}\|+\|\mathbf{B}\|\).

Cosine similarity does not satisfy the triangle inequality or the other
properties needed to qualify as a norm. Therefore, cosine similarity is
not a norm. Instead, it's a measure of similarity between vectors that
is normalized to account for the lengths of the vectors, so that only
the angle between them influences the result.

\hypertarget{can-cosine-similarity-vs-what-can-be-used-as-a-norm}{%
\section{Can Cosine Similarity vs What can be used as a
Norm?}\label{can-cosine-similarity-vs-what-can-be-used-as-a-norm}}

Cosine similarity itself cannot be used as a norm, because, as
mentioned, it doesn't satisfy the necessary properties of a norm.
However, cosine distance, which is defined as:

\[
\operatorname{distance}(\mathbf{A}, \mathbf{B})=1-\operatorname{similarity}(\mathbf{A}, \mathbf{B})=1-\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|_2\left\|_{\mathbf{B}}\right\|_2}
\]

can serve as a distance measure in a space where similarity is measured
by cosine. While cosine similarity is not a norm, cosine distance
satisfies the triangle inequality and other properties of a metric,
making it a valid distance measure.

Thus, cosine distance is a valid mathematical structure to measure
dissimilarity between vectors, and although it isn't a norm itself, it's
related to vector distances in a space defined by cosine similarity. 4.
Connections with Other Norms:

While cosine similarity is related to the L2 (Euclidean) norm, it also
appears in the context of other norms. Let's explore a few: - L1 Norm
(Manhattan Norm): The L1 norm of a vector
\(\mathbf{A}=\left[a_1, a_2, \ldots, a_n\right]\) is defined as:

\[
\|\mathbf{A}\| 1=\left|a_1\right|+\left|a_2\right|+\ldots+\left|a_n\right|
\]

\begin{itemize}
\tightlist
\item
  The cosine similarity measure is independent of the magnitude of the
  vectors but still considers their direction. In some applications, we
  could use cosine similarity in conjunction with the L1 norm to measure
  the similarity between high-dimensional sparse vectors (common in text
  analysis) where non-zero entries are few and far between.
\item
  L \(\infty\) Norm (Maximum Norm): The L \(\infty\) norm (also called
  the Chebyshev norm) is given by:
\end{itemize}

\[
\|\mathbf{A}\|_{\infty}=\max \left(\left|a_1\right|,\left|a_2\right|, \ldots,\left|a_n\right|\right)
\]

\begin{itemize}
\tightlist
\item
  Although the L \(\infty\) norm measures the largest element in the
  vector, it doesn't directly impact cosine similarity, but it's worth
  noting that the interaction between cosine similarity and other norms
  can become relevant in scenarios such as image retrieval or
  clustering, where the components of the vectors may represent pixel
  intensities or other quantities.
\end{itemize}

\hypertarget{cosine-distance}{%
\section{Cosine Distance:}\label{cosine-distance}}

\hypertarget{cosine-distance-is-defined-as}{%
\subsection{Cosine distance is defined
as:}\label{cosine-distance-is-defined-as}}

\[
\text { Cosine Distance }(\mathbf{A}, \mathbf{B})=1-\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}
\]

This is the angular distance between two vectors in a high-dimensional
space, and it ranges from 0 (when the vectors are identical) to 2 (when
the vectors are diametrically opposed). The closer the cosine distance
is to 0 , the more similar the vectors are.

\hypertarget{can-cosine-similarity-be-used-as-a-norm-cosine-distance-as-a-metric}{%
\subsection{Can Cosine Similarity be Used as a Norm? (Cosine Distance as
a
Metric)}\label{can-cosine-similarity-be-used-as-a-norm-cosine-distance-as-a-metric}}

While cosine similarity cannot be a norm due to the reasons above, the
cosine distance, which is defined as:

\[
\operatorname{Cosine} \operatorname{Distance}(\mathbf{A}, \mathbf{B})=1-\operatorname{similarity}(\mathbf{A}, \mathbf{B})=1-\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|_2\left\|_{\mathbf{B}}\right\|_2}
\]

can be used as a distance metric. Cosine distance satisfies the
necessary properties of a distance measure: 1. Non-negativity: Cosine
Distance \((\mathbf{A}, \mathbf{B}) \geq 0\) with equality if and only
if \(\mathbf{A}=\mathbf{B}\). 2. Symmetry: Cosine Distance
\((\mathbf{A}, \mathbf{B})=\) Cosine Distance
\((\mathbf{B}, \mathbf{A})\). 3. Identity of indiscernibles: Cosine
Distance \((\mathbf{A}, \mathbf{B})=0\) if and only if
\(\mathbf{A}=\mathbf{B}\). 4. Triangle Inequality: This property holds
for cosine distance, meaning it satisfies the conditions to be a valid
distance metric.

However, cosine distance is a metric (not a norm), and while it is
related to the angle between vectors, it is not directly related to the
magnitude of the vectors.

Using Cosine Distance as a Norm (Modified Version) While cosine
similarity cannot directly be used as a norm, there are ways to modify
or adapt concepts like cosine distance for scenarios where norms are
needed. However, this requires a modification to the usual properties of
cosine similarity.

Transforming Cosine Similarity into a Norm-like Measure: 1. Unit Vectors
(Normalization):

In many applications, vectors are normalized to unit length, meaning
\(\|\mathbf{A}\|_2=1\) and \(\|\mathbf{B}\|_2=1\). In this case, cosine
similarity becomes a direct measure of how aligned or opposite the
vectors are. The modified ``norm'' is then based on the angular distance
between the vectors. However, this is still angular distance, not a true
norm, because the triangle inequality may not hold in all cases.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Scaling with Cosine Distance:
\end{enumerate}

To make cosine distance behave more like a norm, you would need to scale
the vectors in such a way that cosine distance follows the norm
properties. For example, if we scale the vectors before calculating the
cosine distance and then apply a normalization strategy, we could create
an approximation of a norm-like structure. However, this would still
require further adjustments to make it truly behave as a norm in all
cases.

\hypertarget{why-cosine-similarity-is-not-a-norm}{%
\subsection{Why Cosine Similarity is Not a
Norm}\label{why-cosine-similarity-is-not-a-norm}}

Cosine similarity itself is not a norm due to the following reasons: 1.
Non-negativity: \(A\) norm must satisfy the condition that it is
non-negative and equal to 0 only when the vector is the zero vector
(i.e., \(\|\mathbf{A}\|=0\) if and only if \(\mathbf{A}=\mathbf{0}\) ).
- Cosine similarity ranges from -1 to 1, and it can be negative when the
vectors are diametrically opposed, which violates the non-negativity
property required for norms. Specifically, cosine similarity measures
the cosine of the angle between vectors, and negative values represent
an angle greater than \(90^{\circ}\) (opposite directions), which is
conceptually different from the magnitude measure required for norms. 2.
Homogeneity: A norm must also satisfy the property of homogeneity, where
scaling a vector by a scalar should scale the norm by the absolute value
of that scalar:

\[
\|\alpha \mathbf{A}\|=|\alpha|\|\mathbf{A}\|
\]

\begin{itemize}
\tightlist
\item
  Cosine similarity doesn't satisfy this property because it is based on
  the angle between vectors, and scaling the vectors doesn't change the
  angle. However, norms require the scaling of the vector to scale the
  resulting value. This means cosine similarity doesn't respond to
  scaling in the way norms are supposed to.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Triangle Inequality: A norm must satisfy the triangle inequality,
  which states:
\end{enumerate}

\[
\|\mathbf{A}+\mathbf{B}\| \leq\|\mathbf{A}\|+\|\mathbf{B}\|
\]

\begin{itemize}
\tightlist
\item
  Cosine similarity does not satisfy this property. Cosine similarity is
  only concerned with the angle between vectors and does not scale
  linearly with the magnitude of vectors, meaning it doesn't adhere to
  the triangle inequality required by norms.
\end{itemize}

Because of these issues, cosine similarity is not a norm in the strict
mathematical sense.

\hypertarget{modified-norm-in-specialized-contexts}{%
\subsection{Modified Norm in Specialized
Contexts:}\label{modified-norm-in-specialized-contexts}}

in some specialized contexts, especially those involving unit vectors on
the unit sphere, the cosine distance can be informally referred to as an
angular norm or spherical norm. Here's why: - Unit Vectors: When vectors
are normalized to have a length of 1 (i.e., unit vectors), the cosine
similarty becomes a direct measure of how aligned or opposite the
vectors are. The distance between these vectors is angular because it
reflects the angle between them. - Spherical Geometry: In spherical
geometry, the distance between points on the surface of a sphere is
often computed using the cosine of the angle between them. in this
context, cosine distance (or cosine similarity) could be informally
referred to as an angular distance or spherical norm. These are not true
norms in the mathematical sense, but the terminology is used to describe
the nature of the distance being calculated on the surface of a sphere.

\newpage

\hypertarget{triangle-inequality-for-cosine-similarity}{%
\section{Triangle inequality for cosine
similarity}\label{triangle-inequality-for-cosine-similarity}}

The ordinary triangle inequality for angles (i.e., arc lengths on a unit
hypersphere) gives us that

\[
|\angle A C-\angle C B| \leq \angle A B \leq \angle A C+\angle C B .
\]

Because the cosine function decreases as an angle in \([0, \pi]\)
radians increases, the sense of these inequalities is reversed when we
take the cosine of each value:

\[
\cos (\angle A C-\angle C B) \geq \cos (\angle A B) \geq \cos (\angle A C+\angle C B) .
\]

Using the cosine addition and subtraction formulas, these two
inequalities can be written in terms of the original cosines,

\[
\begin{aligned}
& \cos (A, C) \cdot \cos (C, B)+\sqrt{\left(1-\cos (A, C)^2\right) \cdot\left(1-\cos (C, B)^2\right)} \geq \cos (A, B) \\
& \cos (A, B) \geq \cos (A, C) \cdot \cos (C, B)-\sqrt{\left(1-\cos (A, C)^2\right) \cdot\left(1-\cos (C, B)^2\right)}
\end{aligned}
\]

This form of the triangle inequality can be used to bound the minimum
and maximum similarity of two objects A and B if the similarities to a
reference object C is already known. This is used for example in metric
data indexing, but has also been used to accelerate spherical
\(k\)-means clustering the same way the Euclidean triangle inequality
has been used to accelerate regular k-means.

\hypertarget{soft-cosine-measure}{%
\subsection{Soft cosine measure}\label{soft-cosine-measure}}

A soft cosine or (``soft'' similarity) between two vectors considers
similarities between pairs of features. The traditional cosine
similarity considers the vector space model (VSM) features as
independent or completely different, while the soft cosine measure
proposes considering the similarity of features in VSM, which help
generalize the concept of cosine (and soft cosine) as well as the idea
of (soft) similarity.

For example, in the field of natural language processing (NLP) the
similarity among features is quite intuitive. Features such as words,
\(n\)-grams, or syntactic \(n\)-grams can be quite similar, though
formally they are considered as different features in the VSM. For
example, words ``play'' and ``game'' are different words and thus mapped
to different points in VSM; yet they are semantically related. In case
of \(n\)-grams or syntactic \(n\)-grams, Levenshtein distance can be
applied (in fact, Levenshtein distance can be applied to words as well).

For calculating soft cosine, the matrix \(\mathbf{s}\) is used to
indicate similarity between features. It can be calculated through
Levenshtein distance, WordNet similarity, or other similarity measures.
Then we just multiply by this matrix. Given two \(N\)-dimension vectors
\(a\) and \(b\), the soft cosine similarity is calculated as follows:

\[
\operatorname{soft} \_^{\operatorname{cosine}}(a, b)=\frac{\sum_{i, j}^N s_{i j} a_i b_j}{\sqrt{\sum_{i, j}^N s_{i j} a_i a_j} \sqrt{\sum_{i, j}^N s_{i j} b_i b_j}},
\]

where \(s_{i j}=\operatorname{similarity}\left(\right.\) feature \(_i\),
feature \(_j\) ). If there is no similarity between features (
\(s_{i i}=1, s_{i j}=0\) for \(i \neq j\) ), the given equation is
equivalent to the conventional cosine similarity formula.

The time complexity of this measure is quadratic, which makes it
applicable to real-world tasks. Note that the complexity can be reduced
to subquadratic. An efficient implementation of such soft cosine
similarity is included in the Gensim open source library.

\newpage

\hypertarget{cosine-similarity-applications-in-machine-learning}{%
\section{Cosine Similarity Applications in Machine
Learning}\label{cosine-similarity-applications-in-machine-learning}}

Cosine similarity plays a crucial role in various machine learning
tasks, particularly in text mining and information retrieval: -
\textbf{Text Classification}: It can be used to compare a document's
word vector with a predefined class vector, allowing for the
classification of documents based on similarity. - \textbf{Clustering}:
In clustering, cosine similarity is used to measure the similarity
between data points (documents, images, etc.), helping algorithms group
similar items together. - \textbf{Recommendation Systems}: In
collaborative filtering for recommendation systems, cosine similarity is
used to measure the similarity between users or items based on their
preferences or behaviors.

\hypertarget{cosine-similarity-in-r}{%
\section{Cosine Similarity in R}\label{cosine-similarity-in-r}}

Here's how to calculate the cosine similarity between two vectors using
R:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example vectors}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{)}

\CommentTok{\# Function to calculate cosine similarity}
\NormalTok{cosine\_similarity }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(A, B) \{}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{(A }\SpecialCharTok{*}\NormalTok{ B) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(A}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(B}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))))}
\NormalTok{\}}

\CommentTok{\# Calculate and print cosine similarity}
\NormalTok{similarity }\OtherTok{\textless{}{-}} \FunctionTok{cosine\_similarity}\NormalTok{(A, B)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Cosine Similarity between A and B: "}\NormalTok{, similarity, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cosine Similarity between A and B:  0.9970545
\end{verbatim}

\normalsize

\newpage

\hypertarget{cosine-similarity-in-r-3d-plot}{%
\section{Cosine Similarity in R,
3D-plot}\label{cosine-similarity-in-r-3d-plot}}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Example 3D vectors}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)  }\CommentTok{\# Vector A}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)  }\CommentTok{\# Vector B}

\CommentTok{\# Function to calculate cosine similarity}
\NormalTok{cosine\_similarity }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(A, B) \{}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{(A }\SpecialCharTok{*}\NormalTok{ B) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(A}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(B}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))))}
\NormalTok{\}}

\CommentTok{\# Calculate cosine similarity}
\NormalTok{similarity }\OtherTok{\textless{}{-}} \FunctionTok{cosine\_similarity}\NormalTok{(A, B)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Cosine Similarity between A and B: "}\NormalTok{, similarity, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Cosine Similarity between A and B:  0.9746318
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prepare data for ggplot}
\NormalTok{vector\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, A[}\DecValTok{1}\NormalTok{], }\DecValTok{0}\NormalTok{, B[}\DecValTok{1}\NormalTok{]), }\CommentTok{\# X{-}coordinates}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, A[}\DecValTok{2}\NormalTok{], }\DecValTok{0}\NormalTok{, B[}\DecValTok{2}\NormalTok{]), }\CommentTok{\# Y{-}coordinates}
  \AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, A[}\DecValTok{3}\NormalTok{], }\DecValTok{0}\NormalTok{, B[}\DecValTok{3}\NormalTok{]), }\CommentTok{\# Z{-}coordinates}
  \AttributeTok{group =} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"B"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Convert data to 2D projection for plotting in ggplot}
\NormalTok{vector\_data\_2d }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ vector\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{/}\NormalTok{ (vector\_data}\SpecialCharTok{$}\NormalTok{z }\SpecialCharTok{+} \DecValTok{3}\NormalTok{),  }\CommentTok{\# Simple projection}
  \AttributeTok{y =}\NormalTok{ vector\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{/}\NormalTok{ (vector\_data}\SpecialCharTok{$}\NormalTok{z }\SpecialCharTok{+} \DecValTok{3}\NormalTok{),  }\CommentTok{\# Simple projection}
  \AttributeTok{group =}\NormalTok{ vector\_data}\SpecialCharTok{$}\NormalTok{group}
\NormalTok{)}

\CommentTok{\# Create the 2D plot using ggplot}
\FunctionTok{ggplot}\NormalTok{(vector\_data\_2d, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{color =}\NormalTok{ group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xend =} \DecValTok{0}\NormalTok{, }\AttributeTok{yend =} \DecValTok{0}\NormalTok{), }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"3D Cosine Similarity between Vectors A and B"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"X"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_fixed}\NormalTok{(}\AttributeTok{ratio =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Fix aspect ratio for proper projection}
\end{Highlighting}
\end{Shaded}

\begin{figure}[htbp]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/cosine_similarity_example_2-1} 

}

\caption{Default caption for all figures}\label{fig:cosine_similarity_example_2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Save the plot as a PDF}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"cosine\_similarity\_2d\_plot.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize
\newpage

\hypertarget{tikz-output---cosine-similarity}{%
\subsection{Tikz Output - cosine
similarity}\label{tikz-output---cosine-similarity}}

\begin{tikzpicture}

% Define 3D coordinates for the vectors
\coordinate (A) at (1, 2, 3);  % Vector A (1, 2, 3)
\coordinate (B) at (4, 5, 6);  % Vector B (4, 5, 6)
\coordinate (O) at (0, 0, 0);  % Origin (0, 0, 0)

% Draw the vectors as lines
\draw[->, thick, blue] (O) -- (A) node[midway, below] {$\mathbf{A}$};
\draw[->, thick, red] (O) -- (B) node[midway, above] {$\mathbf{B}$};

% Add axis labels
\draw[->] (O) -- (5, 0, 0) node[right] {X};
\draw[->] (O) -- (0, 5, 0) node[above] {Y};
\draw[->] (O) -- (0, 0, 5) node[above] {Z};

\end{tikzpicture}

\newpage

\hypertarget{naive-bayes}{%
\chapter{Naive Bayes}\label{naive-bayes}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $P(y | x_1, \ldots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i | y)}{P(x_1, \ldots, x_n)}$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-6-cosine_similarity.jpeg}
    \caption*{\Large BAYES somthing Whimsical - not yet}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}
\begin{a_def_eq}{Naive Bayes}{def7} 

\begin{align}\label{naivebayes}
P(y | x_1, \ldots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i | y)}{P(x_1, \ldots, x_n)}
\end{align}

\begin{description}[align=left, labelwidth=2.5cm, labelsep=0em, leftmargin=2.5cm] 
    \item[$x_i$] The \textbf{feature} \( x_i \), representing the \(i\)-th feature of the data point (such as a specific word frequency in text classification, or a specific measurement in other domains).
    \vspace{0.5\baselineskip}
    \item[$y$] The \textbf{class label} \( y \), which represents the class or category that the data point belongs to.
    \vspace{0.5\baselineskip}
    \item[$P(y)$] The \textbf{prior probability} of class \( y \), which represents the initial belief about the probability of class \( y \) occurring in the dataset.
    \vspace{0.5\baselineskip}
    \item[$P(x_i | y)$] The \textbf{likelihood} of feature \( x_i \) given class \( y \), which represents the probability of observing the feature \( x_i \) if the data point belongs to class \( y \).
    \vspace{0.5\baselineskip}
    \item[$P(x_1, \ldots, x_n)$] The \textbf{evidence} or \textbf{normalizing constant}, which ensures that the total probability sums to 1. It is typically computed as \( P(x_1, \ldots, x_n) = \sum_y P(y) \prod_{i=1}^{n} P(x_i | y) \), but for classification, it serves as a normalization term.
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: Naive Bayes is a probabilistic classifier based
on Bayes' theorem with strong independence assumptions.

\textbf{Description}: It assumes that the presence of a particular
feature in a class is unrelated to the presence of any other feature,
which simplifies computation.

\textbf{Importance in ML}: Naive Bayes is used for classification tasks,
especially in text classification and spam detection. Its simplicity and
efficiency make it a popular choice for high-dimensional datasets.

\vspace*{\fill}

\newpage

\hypertarget{naive-bayes---overview}{%
\section{Naive Baye's - overview}\label{naive-bayes---overview}}

Naive Bayes is a fundamental classification technique in machine
learning that utilizes Bayes' theorem to make predictions based on
feature independence. It is particularly valued for its simplicity and
efficiency, making it suitable for large datasets and real-time
applications. Overview of Naive Bayes

Naive Bayes classifiers are a family of linear probabilistic classifiers
that assume the features are \textbf{conditionally independent} given
the target class. This assumption simplifies the computation of
probabilities, allowing for quick and efficient classification. Despite
its ``naive'' assumption, this method has proven effective in various
applications, especially in natural language processing (NLP) tasks like
spam detection and sentiment analysis.

\textbf{Key Features:}:

Probabilistic Model: Naive Bayes uses conditional probability to predict
class membership based on the likelihood of features occurring within
each class.

Independence Assumption: It assumes that the presence of a particular
feature does not affect the presence of any other feature, which
simplifies calculations significantly.

Fast and Scalable: The algorithm is highly scalable, requiring only a
small amount of training data to estimate parameters effectively, making
it suitable for large datasets134. Applications

\textbf{Naive Bayes is widely used in various domains:}

Text Classification: Such as spam filtering and sentiment analysis.
Medical Diagnosis: Classifying diseases based on symptoms.
Recommendation Systems: Suggesting products based on user preferences.

In order to discuss the Naive Bayes' equation and its application we
first introduce the Bayes' equation.

\newpage

\hypertarget{introduction-to-bayesian-theory}{%
\section{Introduction to Bayesian
Theory}\label{introduction-to-bayesian-theory}}

\hypertarget{introduction-the-traditional-formulation-of-bayes-theory}{%
\subsection{Introduction the traditional formulation of Bayes'
theory}\label{introduction-the-traditional-formulation-of-bayes-theory}}

Bayes' Theorem is a powerful mathematical tool used in probability
theory to calculate conditional probabilities. It is named after
Reverend Thomas Bayes and articulates a fundamental relationship between
the conditional probabilities of two events.

The basic concept revolves around updating our initial beliefs or
estimates (prior probabilities) in light of new evidence or information
(likelihood) to arrive at a revised belief (posterior probability). This
process is particularly useful in various fields such as statistics,
machine learning, and data analysis, where making predictions based on
incomplete information is common.

The implication is simple in the sense we want to understand how ``the
occurrence of B'' affects ``the occurrence of A''

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    % Nodes for A and B
    \node (B) at (0,0) {$\underset{\text{We know this!}}{B}$};
    \node (A) at (4,0) {$\underset{\text{What is the impact?}}{A}$};

    % Draw the arrow from B to A
    \draw[->,thick] (B) to[bend left] (A);

    % Add the question mark above the arrow
    \node at (2,1.1) {implies?};
    
\end{tikzpicture}
\label{fig:B_to_A}
\end{figure}

\newpage

\hypertarget{bayes-formula}{%
\subsection{Bayes Formula}\label{bayes-formula}}

\begin{a_def}{Traditional Bayes'}{def7a} 

\begin{align}
\operatorname{P}(A\! \mid\! B)=\frac{\operatorname{P}(B\! \mid\! A) \cdot \operatorname{P}(A)}{\operatorname{P}(B)}
\end{align}

% \tcblower

\begin{description}[align=left, labelwidth=2.5cm, labelsep=0em, leftmargin=2.5cm] 
    \item[$A$] The \textbf{event of interest}, which represents the hypothesis or condition for which the probability is being computed.
    \vspace{0.5\baselineskip}
    \item[$B$] The \textbf{evidence or observed event}, which represents the known condition or observation influencing the computation of \( \operatorname{P}(A \mid B) \).
    \vspace{0.5\baselineskip}
    \item[$\operatorname{P}(A)$] The \textbf{prior probability} of event \( A \), representing the initial belief about the likelihood of \( A \) occurring, before considering the evidence \( B \).
    \vspace{0.5\baselineskip}
    \item[$\operatorname{P}(B)$] The \textbf{marginal probability} of \( B \), representing the total likelihood of observing \( B \) across all possible scenarios.
    \vspace{0.5\baselineskip}
    \item[$\operatorname{P}(B \mid A)$] The \textbf{likelihood} of observing evidence \( B \) given that \( A \) is true. It quantifies how probable the evidence is under the hypothesis.
    \vspace{0.5\baselineskip}
    \item[$\operatorname{P}(A \mid B)$] The \textbf{posterior probability} of \( A \) given \( B \), representing the updated belief about the likelihood of \( A \) after considering the evidence \( B \).
\end{description}

\end{a_def}

\hfill\break

\newpage

\hypertarget{explanation-of-the-player-in-bayes-formula}{%
\subsection{Explanation of the player in Bayes'
formula}\label{explanation-of-the-player-in-bayes-formula}}

\textbf{Prior Probability \(\operatorname{P}(A)\):}\\
This is your initial belief about the probability of an event before any
additional information is considered. For instance, if you are trying to
determine the probability that a randomly chosen person is left-handed
\((A)\), your prior might be the overall percentage of left-handed
people in the population.

\textbf{Likelihood \(\operatorname{P}(B\! \mid\! A)\):}\\
This reflects how probable the evidence \((B)\) is, assuming the
hypothesis \((A)\) is true. For example, if a diagnostic test is known
to detect a disease \(95 \%\) of the time when the disease is present,
then \(\operatorname{P}(B\! \mid\! A)=0.95\).

\textbf{Marginal Probability \(\operatorname{P}(B)\):}\\
This is the total probability of observing the evidence ( \(B\) ) under
all possible circumstances. It's often calculated as the sum of
probabilities for all possible values of
\(A: \operatorname{P}(B)=\operatorname{P}(B\! \mid\! A) \cdot \operatorname{P}(A)+\operatorname{P}(B\! \mid \sim\! A) \cdot \operatorname{P}(\sim\! A)\),
where \(\neg A\) represents the complement of \(A\).

\textbf{Posterior Probability \(\operatorname{P}(A\! \mid\! B)\):}\\
This is what you're solving for. It's your updated belief about the
probability of \(A\) after taking into account the new evidence \(B\).

\hypertarget{rational-considerations-of-the-meaning-of-the-bayes-formula}{%
\subsection{Rational Considerations of the meaning of the Bayes'
formula}\label{rational-considerations-of-the-meaning-of-the-bayes-formula}}

Taking a closer look at Bayes' formula:

\[
\overset{\text{Poterior}}{P(A \! \mid \! B)} 
= 
\overset{\text{Likelihood}}{P(B \! \mid \! A)} 
\cdot 
\frac{
    \overset{\text{Prior}}{P(A)}
}{
    \underset{\text{Evidence}}{P(B)}
}
\]

The key to tying this all together is to understand the following key
expression:

\[
\text {Posterior} = \text {Likelihood} \cdot \frac{\text { Prior }}{\text { Evidence }}
\]

\newpage

\hypertarget{fundamental-bayes-probability-space}{%
\section{Fundamental Bayes' Probability
Space}\label{fundamental-bayes-probability-space}}

We begin by showing the probability space via a Venn diagram
visualization.

\hypertarget{probability-space-venn-diagram-for-traditional-bayesian}{%
\subsection{Probability Space Venn Diagram for traditional
Bayesian}\label{probability-space-venn-diagram-for-traditional-bayesian}}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1.5]
    % Shade the field of the box pale yellow
    \fill[yellow!5] (-2,-2) rectangle (3,2);
    \draw (-2,-2) rectangle (3,2);

    % Shade the A circle light pink and B Circle light blue
    \fill[pink!50] (0,0) circle (1);
    \fill[blue!15] (1,0) circle (1);

    % Draw the circles for A and B
    \draw (0,0) circle (1);
    \draw (1,0) circle (1);

    \node at (-0.5,0) {$A$};
    \node at (1.5,0) {$B$};
    
    % Shade the intersection light purple
    \begin{scope}
        \clip (0,0) circle (1);
        \fill[purple!30] (1,0) circle (1);
    \end{scope}

    % Labels
    \node at (0.5,1.5) {$A \cap B$};
    \node at (-0.9,-1.5) {$\sim\!A \, \cap \sim\!B$};

    % Draw the arrow pointing to A intersect B
    % \draw[-{Latex[length=2mm]}] (0.5,1.25) -- (0.5,0.1);
    
    % Add the Omega symbol below the right lower side of the main box
    \node at (3.3,-1.80) {$\Omega$};
    
    
\end{tikzpicture}
\caption{Venn diagram representing the intersection and complements of sets A and B.}
\label{fig:venn-diagram}
\end{figure}

The Venn diagram depicts a Bayes' probability space, with sets \(A\) and
\(B\) as distinct events within the universal event space \(\Omega\).
The area \(A \cap B\) illustrates the joint occurrence of events \(A\)
and \(B\), while \(\sim\! A \, \cap \sim\! B\) denotes the region where
neither event takes place. This visualization aids in understanding
Bayesian inference, particularly how evidence \(B\) updates the
probability of event \(A\) according to Bayes' Theorem.

\textbf{Identities derived by inspection:} \[\begin{aligned} 
\operatorname{P}(A) &=\operatorname{P}(A \cap B)+\operatorname{P}(A\, \cap \sim B) \\ 
\operatorname{P}(\sim A)&=\operatorname{P}(\sim A \cap B)+\operatorname{P}(\sim A\, \cap \sim B) \\ 
\operatorname{P}(B)&=\operatorname{P}(A \cap B)+\operatorname{P}(\sim A \cap B) \\ 
\operatorname{P}(\sim B)&=\operatorname{P}(A\, \cap \sim B)+\operatorname{P}(\sim A\, \cap \sim B)
\end{aligned}\]

\textbf{Fundamental Axioms:} \[\begin{aligned}
\{\operatorname{P}(A), \operatorname{P}(B)\} &\in[0,1] \triangleq [0 , \Omega] \\ 
\operatorname{P}(A \cap B)&=\operatorname{P}(B \cap A)\\
\operatorname{P}(\overline{A \cap B}) &= \Omega + \operatorname{P}(A \cap B) - \operatorname{P}(A) - \operatorname{P}(B)
\end{aligned}\]

\newpage

\newpage

\hypertarget{kolmogorov-axioms-of-probability}{%
\section{Kolmogorov Axioms of
Probability}\label{kolmogorov-axioms-of-probability}}

The Kolmogorov axioms \{non-negativity, normalization, and additivity\}
provide a way to derive a general expression for conditional
probability, forming the basis of Bayes' theorem. We want a way to
efficiently calculate Bayesian probability updates. This is a derviation
grounded in the axiomatic structure of probability.

\hypertarget{the-kolmogorov-axioms}{%
\subsection{The Kolmogorov Axioms}\label{the-kolmogorov-axioms}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-negativity}: \(P(E) \geq 0\) for any event \(E\).
\item
  \textbf{Normalization}: \(P(\Omega) = 1\), where \(\Omega\) is the
  sample space.
\item
  \textbf{Additivity}: If \(A\) and \(B\) are disjoint events
  (\(A \cap B = \emptyset\)), then: \[
  P(A \cup B) = P(A) + P(B).
  \]
\end{enumerate}

\hypertarget{definition-of-conditional-probability}{%
\subsection{Definition of Conditional
Probability}\label{definition-of-conditional-probability}}

Conditional probability is defined as: \[
P(A \mid B) = \frac{P(A \cap B)}{P(B)} \quad \text{if } P(B) > 0.
\]

This definition can be derived systematically using the axioms of
probability and set theory.

\hypertarget{axiomatic-derivation-of-pa-mid-b}{%
\subsection{\texorpdfstring{Axiomatic Derivation of
\(P(A \mid B)\)}{Axiomatic Derivation of P(A \textbackslash mid B)}}\label{axiomatic-derivation-of-pa-mid-b}}

\textbf{Step 1: Define the restricted sample space}\\
When conditioning on event \(B\), we are effectively restricting the
sample space to \(B\). In this restricted sample space: - The total
probability of \(B\) becomes 1, i.e., \(P_B(\Omega) = 1\), where \(P_B\)
denotes the new probability measure conditioned on \(B\).

\textbf{Step 2: Normalize probabilities within \(B\)}\\
The probability of any event \(A\), given \(B\), should reflect the
proportion of \(A\) within \(B\). This leads to the proportionality: \[
P(A \mid B) \propto P(A \cap B).
\] To normalize this proportion, divide by the total probability of
\(B\): \[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}.
\]

\textbf{Step 3: Justify using Kolmogorov's axioms}\\
This definition satisfies the axioms of probability:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-negativity}: Since \(P(A \cap B) \geq 0\) and
  \(P(B) > 0\), \(P(A \mid B) \geq 0\).
\item
  \textbf{Normalization}: The total probability in the restricted space
  sums to 1: \[
  P(B \mid B) = \frac{P(B \cap B)}{P(B)} = \frac{P(B)}{P(B)} = 1.
  \]
\item
  \textbf{Additivity}: For disjoint events \(A_1, A_2\), where
  \(A_1 \cap A_2 = \emptyset\): \[
  P((A_1 \cup A_2) \mid B) = \frac{P((A_1 \cup A_2) \cap B)}{P(B)}.
  \] By the additivity axiom: \[
  P((A_1 \cup A_2) \cap B) = P((A_1 \cap B)) + P((A_2 \cap B)),
  \] so: \[
  P((A_1 \cup A_2) \mid B) = \frac{P(A_1 \cap B)}{P(B)} + \frac{P(A_2 \cap B)}{P(B)} = P(A_1 \mid B) + P(A_2 \mid B).
  \]
\end{enumerate}

\hypertarget{axiomatic-derivation-of-pb-mid-a}{%
\subsection{\texorpdfstring{Axiomatic Derivation of
\(P(B \mid A)\)}{Axiomatic Derivation of P(B \textbackslash mid A)}}\label{axiomatic-derivation-of-pb-mid-a}}

By symmetry of intersection (\(A \cap B = B \cap A\)), the same
reasoning applies: \[
P(B \mid A) = \frac{P(B \cap A)}{P(A)} \quad \text{if } P(A) > 0.
\]

\hypertarget{summary-of-the-axiomatic-derivation}{%
\subsection{Summary of the Axiomatic
Derivation}\label{summary-of-the-axiomatic-derivation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start from the definition of probability within a restricted sample
  space (conditioning).
\item
  Use normalization within the restricted space (\(B\)) to derive
  \(P(A \mid B)\).
\item
  Verify that the result satisfies Kolmogorov's axioms (non-negativity,
  normalization, additivity).
\item
  Symmetry of intersection ensures the analogous formula for
  \(P(B \mid A)\).
\end{enumerate}

Thus: \[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad P(B \mid A) = \frac{P(A \cap B)}{P(A)}.
\] These results naturally follow from the probability axioms and set
theory.

\newpage

\hypertarget{the-role-of-commutativity-in-bayesian-probability-calculations}{%
\subsection{The Role of Commutativity in Bayesian Probability
Calculations}\label{the-role-of-commutativity-in-bayesian-probability-calculations}}

The salient question arising at this juncture is:

\textbf{How do we relate the Venn diagram in
Figure:\(\,\)\ref{fig:venn-diagram} to Bayes' theorem?}

\textbf{And, how do we calculate using theorem?}

In set theory:

\[
A \cap B=B \cap A, \quad \text { and } \quad A \cup B=B \cup A .
\]

These properties reflect the invariance of the resulting set under the
order of operation. Similarly, in a metric space, where probability is
conceptualized as a measure \(P\), the measure of the intersection
\((A \cap B)\) is invariant under the order of \(A\) and \(B\). This
invariance is the cornerstone that allows Bayes' relation to hold up
under all valid probability distributions. This property allows us to
express the joint probability of two events \(A\) and \(B\) in multiple
equivalent ways:

\[
P(A \cap B)=P(B \cap A) .
\]

This symmetry is not arbitrary; it arises directly from the axioms of
set theory. Specifically, the intersection operation in sets is
commutative, meaning the order of the operands does not affect the
result. In a probabilistic context, this translates to the joint
occurrence of \(A\) and \(B\) being independent of which event is
referenced first.

From a metric perspective, probability can be thought of as a measure
defined on a *\textbf{sigma-algebra of subsets}. The intersection of two
subsets corresponds to the overlap of their measurable areas. The
measure of this overlap does not depend on the order in which the
subsets are combined.

This commutativity directly enables the derivation of Bayes' theorem.
The theorem hinges on the equivalence of two different factorizations of
the joint probability \(P(A \cap B)\) :

\[
P(A \cap B)=P(A) P(B \mid A)=P(B) P(A \mid B)
\]

Without the commutative property, such equivalence could not be
guaranteed. This would undermine the consistency of conditional
probability calculations and, by extension, the entire framework of
Bayesian analysis.

\hypertarget{computational-implications-for-bayesian-analysis}{%
\subsection{Computational Implications for Bayesian
Analysis}\label{computational-implications-for-bayesian-analysis}}

The commutative property simplifies Bayesian calculations by providing a
consistent framework to calculate conditional probabilities.
Specifically, we can use:

\[
P(A \mid B)=\frac{P(A \cap B)}{P(B)}, \quad \text { and } \quad P(B \mid A)=\frac{P(A \cap B)}{P(A)} .
\]

These forms are essential in computational Bayesian analysis because
they allow direct computation of conditional probabilities using readily
available components: the joint probability and the marginal
probabilities.

By relying on commutativity, we avoid ambiguity and reduce the
complexity of Bayesian inference to manageable and consistent
operations. This makes Bayesian analysis not only theoretically sound
but also practically implementable in diverse applications such as
machine learning, statistical inference, and decision-making under
uncertainty. The commutativity of intersection streamlines Bayesian
probability calculations, reducing them to the ratio of the events'
intersection to the total probability of \(A\) or \(B\). This pivotal
concept underpins Bayesian analysis. \[
P(B) P(A \mid B)=\operatorname{P}(A \cap B)=P(B \cap A)=P(A) P(B \mid A)
\] This consideration streamlines the computation of Bayesian
probabilities. By conceptualizing it as the ratio of the intersection of
events \(A\) and \(B\) to the total probability of \(A\) or \(B\), one
can readily calculate the conditional probabilities inherent in Bayes'
theorem.

Thus, we arrive at essential expressions for Bayesian inference
computation: \[
\operatorname{P}(A\! \mid\! B)=\frac{\operatorname{P}(A \cap B)}{\operatorname{P}(B)}
\] \[
\operatorname{P}(B\! \mid\! A)=\frac{\operatorname{P}(A \cap B)}{\operatorname{P}(A)}
\]

\newpage

\hypertarget{probability-space-graph-over-omega}{%
\section{\texorpdfstring{Probability Space Graph over
\(\Omega\)}{Probability Space Graph over \textbackslash Omega}}\label{probability-space-graph-over-omega}}

Interestingly Enough - these relations yield a probability symmetry
originating from \(\Omega\).

Interestingly enough, the relationships visualized in this graph yield a
probability symmetry originating from \(\Omega\). This graph is
meaningful because it provides a structured way to represent the
relationships between various conditional probabilities in the
probability space \(\Omega\). By mapping these relationships, we gain
insights into the fundamental symmetries and connections that define
conditional probability.

\begin{figure}[htbp]
\centering
\resizebox{0.9\textwidth}{!}
{
\begin{tikzpicture}[scale=1.1] 
    [
    new set=my nodes, 
    new set=my nodes2,
    every node/.style = {font=\footnotesize}, % Global style for nodes
    ] 

{
% define nodes
\node[draw, align=center] (Omega_Left) at (0,0) {\Large $\Omega$ \\ \footnotesize Prob \\ Space}; 
% \node[draw] (Omega_Left)    at    (0,0)     {$\Omega$};
\node[draw] (A)               at    (3,2)     {$P(A)$};
\node[draw] (notA)            at    (3,-2)    {$P(\sim\! A)$};
\node[draw] (AcapB)           at    (7,3)     {$P(A \cap B)$};
\node[draw] (AcapnotB)        at    (7,1)     {$P(A \cap \sim\! B)$};
\node[draw] (notAcapB)        at    (7,-1)    {$P(\sim\! A \cap B)$};
\node[draw] (notAcapnotB)     at    (7,-3)    {$P(\sim\! A \cap \sim\! B)$};

\node[draw] (BcapA)           at    (10,3)      {$P(B \cap A)$};
\node[draw] (notBcapA)        at    (10,-1)     {$P(\sim\! B \cap A)$};
\node[draw] (BcapnotA)        at    (10,1)    {$P(B \cap \sim\! A )$};
\node[draw] (notBcapnotA)     at    (10,-3)     {$P(\sim\! B \cap \sim\! A)$};

\node[draw] (B)               at    (14,2)      {$P(B)$};
\node[draw] (notB)            at    (14,-2)     {$P(\sim\! B)$};

\node[draw, align=center] (Omega_Right) at (17,0) {\Large $\Omega$ \\ \footnotesize Prob \\ Space}; 
% \node[draw] (Omega_Right)     at  (17,0)      {$\Omega$};


% make some help grid
% \draw[help lines] (-1,4) grid (14,-4);

% draw lines left Omega_Left to P(A) and P(~A)
\draw (Omega_Left) -- (A)       node [midway,above,sloped]      {\scriptsize $P(A\! \mid\! A)$};
\draw (Omega_Left) -- (notA)    node [midway,below,sloped]      {\scriptsize $P(\sim\! A\! \mid \sim\! A)$};

% draw lines second layer top from P(A)
\draw (A) -- (AcapB)            node [midway,above,sloped]      {\scriptsize $P(B\! \mid\! A)$};
\draw (A) -- (AcapnotB)         node [midway,below,sloped]      {\scriptsize $P(\sim\! B\! \mid\! A)$};

% draw lines second layer bot from P(~A)
\draw (notA) -- (notAcapB)      node [midway,above,sloped]      {\scriptsize $P(B\! \mid \sim\! A)$};
\draw (notA) -- (notAcapnotB)   node [midway,below,sloped]      {\scriptsize $P(\sim\! B\! \mid \sim\! A)$};

% draw commutative connection
\draw [dashed] (AcapB) -- (BcapA);
\draw [dashed] (AcapnotB) -- (notBcapA);
\draw [dashed] (notAcapB) -- (BcapnotA);        
\draw [dashed] (notAcapnotB) -- (notBcapnotA);

% draw lines second layer top from P(B)
\draw (B) -- (BcapA)            node [midway,above,sloped]      {\scriptsize $P(A\! \mid\! B)$};
\draw (B) -- (BcapnotA)         node [midway,below,sloped]      {\scriptsize $P(\sim\! A\! \mid\! B)$};

% draw lines second layer top from P(~B)
\draw (notB) -- (notBcapA)      node [midway,above,sloped]      {\scriptsize $P(A\! \mid \sim\! B)$};
\draw (notB) -- (notBcapnotA)   node [midway,below,sloped]      {\scriptsize $P(\sim\! A\! \mid \sim\! B)$};

% draw lines left Omega_Left to P(A) and P(~A)
\draw (Omega_Right) -- (B)      node [midway,above,sloped]      {\scriptsize $P(B\! \mid\! B)$};
\draw (Omega_Right) -- (notB)   node [midway,below,sloped]      {\scriptsize $P(\sim\! B\! \mid \sim\! B)$};


}
\end{tikzpicture}
}
\caption{Probability Graph $\Omega$ bridged by commutative intersection $P(A \cap B) = P(B \cap A)$}.
\label{fig:ProbGraph1-diagram}
\end{figure}

\hypertarget{why-is-this-graph-meaningful}{%
\subsection{Why is this graph
meaningful?}\label{why-is-this-graph-meaningful}}

This type of visualization allows us to:

\begin{itemize}
\tightlist
\item
  Understand how conditional probabilities link different events.
\item
  Highlight the inherent symmetry in relationships such as
  \(P(A \cap B) = P(B \cap A)\), emphasizing commutative properties.
\item
  Clarify how joint probabilities, complements, and conditional
  probabilities interact within the space \(\Omega\).
\end{itemize}

Each \textbf{edge} in the graph represents a \textbf{conditional
probability} that connects adjacent nodes. For example:

\begin{itemize}
\tightlist
\item
  An edge from \(\Omega\) to \(P(A)\) trivalially represents
  \(P(A \mid A)\), the probability of \(A\) given \(A\).
\item
  Similarly, edges like \(P(B \mid A)\) connect events \(P(A)\) and
  \(P(A \cap B)\), reinforcing the dependency between them.
\end{itemize}

\newpage

Through graph re-ordering we can better highlite inherent symmetries
around \(\Omega\). We can then better understand how these conditional
probabilities form a consistent framework that governs the relationships
between different events.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale = 1.05] 
    [
    every node/.style = {font=\footnotesize}
    ] 

{
% define nodes
% \node[draw] (Omega_Center)    at  (0,0)       {\Large $\Omega$};
\node[draw, align=center] (Omega_Center) at (0,0) {\Large $\Omega$ \\ \footnotesize Prob Space};
\node[draw] (A)             at  (-4,4)          {$P(A)$};
\node[draw] (notA)          at  (4,-4)        {$P(\sim\! A)$};
\node[draw] (AcapB)         at  (0,4)           {$P(A \cap B)$};
\node[draw] (AcapnotB)      at  (-4,0)      {$P(A \cap \sim\! B)$};
\node[draw] (notAcapB)      at  (4,0)       {$P(\sim\! A \cap B)$};
\node[draw] (notAcapnotB)   at  (0,-4)    {$P(\sim\! A \cap \sim\! B)$};

% \node[draw] (BcapA)           at  (8,3)       {$P(B \cap A)$};
% \node[draw] (notBcapA)        at  (8,-1)  {$P(\sim\! B \cap A)$};
% \node[draw] (BcapnotA)        at  (8,1)       {$P(B \cap \sim\! A )$};
% \node[draw] (notBcapnotA)         at  (8,-3)  {$P(\sim\! B \cap \sim\! A)$};

\node[draw] (B)             at  (4,4)   {$P(B)$};
\node[draw] (notB)          at  (-4,-4)     {$P(\sim\! B)$};


% make some help grid
% \draw[help lines] (-6,-6) grid (6,6);
\draw (Omega_Center) -- (A)         node [midway,above,sloped]      {\scriptsize $P(A\! \mid\! A)$};
\draw (Omega_Center) -- (notA)  node [midway,above,sloped]      {\scriptsize $P(\sim\! A\! \mid  \sim\! A)$};
\draw (Omega_Center) -- (B)         node [midway,above,sloped]      {\scriptsize $P(B\! \mid\! B)$};
\draw (Omega_Center) -- (notB)  node [midway,above,sloped]      {\scriptsize $P(\sim\! B\! \mid \sim\! B)$};

\draw  (A) -- (AcapB)           node [midway,above,sloped]        {\scriptsize $P(B\! \mid\! A)$};
\draw  (AcapnotB) -- (A)            node [midway,above,sloped]      {\scriptsize $P(\sim\! B\! \mid\! A)$};
\draw  (B) -- (AcapB)           node [midway,above,sloped]        {\scriptsize $P(A\! \mid\! B)$};
\draw  (B) -- (notAcapB)            node [midway,above,sloped]          {\scriptsize $P(\sim\! A\! \mid\! B)$};
\draw  (notB) -- (AcapnotB)         node [midway,above,sloped]      {\scriptsize $P(A\! \mid \sim\! B)$};
\draw  (notB) -- (notAcapnotB)      node [midway,above,sloped]  {\scriptsize $P(\sim\! A\! \mid \sim\! B)$};
\draw  (notA) -- (notAcapnotB)      node [midway,above,sloped]  {\scriptsize $P(\sim\! B\! \mid \sim\! A)$};
\draw  (notAcapB) -- (notA)     node [midway,above,sloped]          {\scriptsize $P(B\! \mid \sim\! A)$};

}
\end{tikzpicture}
\caption{Probability Graph displaying inherent symmetry around $\Omega$}
\label{fig:ProbGraph2-diagram}
\end{figure}

Probability theory provides a robust framework for understanding and
quantifying uncertainty. At the heart of this framework lies the concept
of a probability space, represented here as \(\Omega\). The mappings
within this space, as visualized in the following graphs, reveal deep
insights into how probabilities interact, connect, and relate to each
other.

This visualization captures the relationships between various events and
their intersections, along with the conditional probabilities that
bridge these relationships. Each node in the graph represents a specific
probability or set of probabilities, while the edges linking them denote
conditional probabilities, which express the likelihood of one event
given the occurrence of another. By structuring these connections
graphically, we can achieve a more intuitive grasp of the mathematical
underpinnings of probability theory.

\textbf{Why This Graph is Meaningful ?}

Symmetry and Commutative Properties:

The graph highlights a fundamental property of probability: the
commutative nature of intersections. For instance,
\(P(A \cap B) = P(B \cap A)\). This symmetry is clearly visualized
through the commutative links, represented as dashed lines.

Conditional Dependencies:

Each edge encodes a conditional probability, such as \(P(A \mid B)\) or
\(P(B \mid A)\). These conditional probabilities provide critical
insights into how one event influences another. The graph captures this
influence in a structured and visually accessible way.

Partitioning of Probability Space:

The diagram divides \(\Omega\) into mutually exclusive and exhaustive
components, such as \(P(A \cap B)\), \(P(A \cap \sim B)\),
\(P(\sim A \cap B)\), and \(P(\sim A \cap \sim B)\). This partitioning
reveals how probabilities distribute across the space and helps in
understanding joint and marginal probabilities.

Inherent Symmetry:

The visualization emphasizes the inherent symmetry in probability
relationships. For example, the mappings around \(\Omega\) are visually
balanced, reinforcing the idea that \(\Omega\) serves as the universal
set encompassing all events.

Practical Interpretations:

Such mappings are particularly useful in Bayesian inference, where
updating beliefs based on observed data requires careful computation of
conditional probabilities. This graph provides a foundation for such
calculations by making the relationships explicit.

Insights Gained from the Visualization

Connectivity:

The direct edges between nodes illustrate how probabilities propagate
through the system. For example, \(P(A)\) connects to \(P(A \cap B)\)
via the conditional probability \(P(B \mid A)\). This connectivity
mirrors real-world decision-making processes where one piece of
information influences another.

Comparative Understanding:

By juxtaposing related probabilities, such as \(P(A \cap B)\) and
\(P(B \cap A)\), the graph enables a side-by-side comparison that
reinforces mathematical equivalence.

Structure of Relationships:

The graph imposes a structure on the probability space, making abstract
relationships more tangible. This structured view is invaluable for
teaching, analysis, and practical applications such as decision analysis
and machine learning.

How to Use This Graph

Study Conditional Probabilities:

Trace the edges to understand the relationship between nodes. For
example, \(P(A \cap B)\) is linked to \(P(A)\) through \(P(B \mid A)\),
showing the dependence of the joint probability on the marginal and
conditional probabilities.

Analyze Bayesian Inference:

Use the graph to identify how evidence updates beliefs. For instance,
given \(P(B \mid A)\) and \(P(A)\), one can compute \(P(A \cap B)\),
which forms the basis of Bayesian updates.

Visualize Complex Spaces:

For advanced topics, such as joint distributions or higher-dimensional
spaces, extend the graphing technique to include additional events and
their intersections.

In summary, this probability space graph provides an essential tool for
exploring and understanding the relationships between events in a
probabilistic framework. By visualizing \(\Omega\) and its components,
we gain not only a mathematical appreciation but also a conceptual
understanding of the intricate dance of probabilities.

\newpage

\hypertarget{maringal-and-bayes-probability-space-sums}{%
\section{Maringal and Bayes' probability space
sums}\label{maringal-and-bayes-probability-space-sums}}

In Bayesian analysis, the interrelation of event probabilities and their
complements constructs a full probability model. The following table
clarifies this by mapping the joint probabilities of events \(A\) and
\(b\) along with their complements in the probability space \(\Omega\).
It exemplifies the key tenet that all outcome probabilities sum to
unity, aligning with \(\Omega\). The table shows the network for
calculating marginal probabilities and grasping Bayesian inference's
core, which will be demonstrated in subsequent examples. Note: While we
can normally employ \(\Omega = 1\), we can also commonly allow some
value like \(\Omega = 750\) and then the numbers sum to this value.

\begin{figure}[htbp]
\centering

\begin{tabular}{|c|c c c c c c c|}

\hline
               & &                       & &                 & &                &   \\
               
& & $\mathbf{\{A\}}$  & & $\mathbf{\{\sim\! A \}}$  & & \textbf{Marginal}       &   \\

               & &                       & &                 & &                &   \\
\hline
               & &                       & &                 & &                &   \\

$\mathbf{\{B\}}$  & & P($A \cap B$)      & + & P($\sim\! A \cap B$) & = & P($B$)  &    \\

               & &                       & &                 & &                &   \\
               & & +                     & &   +             & &       +        &   \\
               & &                       & &                 & &                &   \\
$\mathbf{\{\sim\! B \}}$ & & P($A\, \cap \sim\! B$)  & + & P($\sim\! A\, \cap \sim\! B$) & = & P($\sim\! B$)  & \\
               & &                       & &                 & &                &   \\
               & & \text{ }\begin{rotate}{90}=\end{rotate}     &   & \text{ }\begin{rotate}{90}=\end{rotate}   &   &  \text{ }\begin{rotate}{90}=\end{rotate}    & \\
               & &                       & &                 & &                &   \\
\textbf{Marginal} & & P($A$)             & + & P($\sim\! A$) & = & $\Omega$       &   \\
               & &                       & &                 & &                &   \\
\hline

\end{tabular}
\caption{Marginals and sums network of Traditional Bayes' Space}
\label{fig:m_bayes_space}

\end{figure}

\newpage

\newpage

\hypertarget{bayes-by-example}{%
\section{bayes' by example}\label{bayes-by-example}}

\hypertarget{classical-bayes-word-problem-example-red-ace-example}{%
\subsection{Classical Bayes' word problem example (``Red Ace
Example'')}\label{classical-bayes-word-problem-example-red-ace-example}}

Imagine you have a deck of cards, and you want to know the probability
that a card is an ace given that it's a red card. Here, the event A is
``the card is an ace,'' and B is ``the card is red.''

\begin{itemize}
\tightlist
\item
  The prior probability \(\operatorname{P}(A)\) is \(4 / 52\) since
  there are 4 aces in a 52 -card deck.
\item
  The likelihood \(\operatorname{P}(B \mid A)\) is \(1 / 2\) because
  half of the aces are red.
\item
  The marginal probability \(\operatorname{P}(B)\) is \(26 / 52\) since
  there are 26 red cards in the deck.
\end{itemize}

Plugging these values into Bayes' Theorem:

So, the probability that a card is an ace given that it's red is
\(1 / 13\).

Bayes' Theorem thus provides a robust framework for updating beliefs and
making informed decisions based on new data. It's a cornerstone of
statistical inference and finds applications in various fields, from
spam filtering in emails to disease diagnosis in medicine

\hypertarget{considering-the-classic-example-and-why-we-can-do-better}{%
\subsection{Considering the ``Classic Example'' and why we can do
better}\label{considering-the-classic-example-and-why-we-can-do-better}}

To candidly address the challenges inherent in understanding probability
spaces and the diverse applications of Bayes' Theorem, it is essential
to delve beyond the surface of typical examples. By doing so, we aim to
unravel the deeper intricacies of Bayes' formula. Our objective here is
not just to reiterate the theorem but to derive a series of identities
and explore the concept of probability metric spaces.

By fostering a profound comprehension from foundational principles and
underlying identities, we intend to cultivate a structured approach to
employing Bayes' Theorem in various scenarios. This is particularly
vital in contexts where the traditional formulation of Bayes' may not be
the most appropriate or insightful application. Through this
exploration, we aspire to formulate more nuanced and, ostensibly, more
compelling scenarios that demonstrate the versatility and power of
Bayes' Theorem.

In this journey, we will dissect the theorem, examining each component
and its role in shaping the overall framework. We'll explore how prior
beliefs are updated with new evidence, and how this process is
influenced by the underlying probability structures. By doing so, we aim
to provide readers with a toolkit for thinking critically about
probability and making more informed decisions in the face of
uncertainty.

Ultimately, this exploration is not just about understanding a
mathematical formula; it's about embracing a new way of thinking. It's
about shifting our perspective from seeing Bayes' Theorem as a mere tool
to appreciating it as a fundamental principle that underpins a vast
array of applications---from scientific research to everyday
decision-making.

\newpage

\hypertarget{a-visual-understanding-of-bayes}{%
\subsection{A visual understanding of
Bayes}\label{a-visual-understanding-of-bayes}}

\hypertarget{deriving-the-rules-of-probability}{%
\subsection{Deriving the Rules of
Probability}\label{deriving-the-rules-of-probability}}

To derive the rules of probability, consider the general case involving
two random variables \((X)\) and \((Y)\).

\hypertarget{example-context}{%
\subsection{Example Context}\label{example-context}}

In a medical example:

\begin{itemize}
\tightlist
\item
  \((X)\) could represent the \textbf{presence or absence of cancer}
\item
  \((Y)\) could represent the \textbf{outcome of a diagnostic test}
\end{itemize}

These variables vary between individuals in generally unknown ways, so
they are referred to as \textbf{random variables} (or \textbf{stochastic
variables}).

\hypertarget{variable-definitions}{%
\subsection{Variable Definitions}\label{variable-definitions}}

We assume:

\begin{itemize}
\tightlist
\item
  \((X)\) can take values \((x_i)\), where \(i = 1, \ldots, L\)
\item
  \((Y)\) can take values \((y_j)\), where \(j = 1, \ldots, M\)
\end{itemize}

\hypertarget{observational-setup}{%
\subsection{Observational Setup}\label{observational-setup}}

Suppose there are \(N\) total trials in which we sample both \((X)\) and
\((Y)\). Define:

\begin{itemize}
\tightlist
\item
  \((n_{ij})\): the \textbf{number of trials} where \((X = x_i)\) and
  \((Y = y_j)\)
\item
  \((c_i)\): the \textbf{number of trials} where \((X = x_i)\)
  (irrespective of \((Y)\))
\item
  \((r_j)\): the \textbf{number of trials} where \((Y = y_j)\)
  (irrespective of \((X)\))
\end{itemize}

\hypertarget{joint-probability}{%
\subsection{Joint Probability}\label{joint-probability}}

The \textbf{joint probability} \(p(X = x_i, Y = y_j)\) is the
probability that:

\begin{itemize}
\tightlist
\item
  \((X)\) takes the value \((x_i)\),
\item
  and \((Y)\) takes the value \((y_j)\).
\end{itemize}

It is calculated as the fraction of trials \((n_{ij})\), where
\((X = x_i)\) and \((Y = y_j)\), divided by the total number of trials
\((N)\): \[
p(X = x_i, Y = y_j) = \frac{n_{ij}}{N}.
\]

This represents the proportion of observations falling into the
corresponding cell \((i, j)\) in a contingency table.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    % Draw the grid
    \draw[red, thick] (0,0) grid (6,4);

    % Add an overbrace as a node for c_i
    % \node at (1.5, 4.5) {$\overbrace{n_{i1} + n_{i2} + \cdots + n_{iM}}^{c_i}$};

    % Add another overbrace as a node for c_i
    \node at (3.5, 4.3) {$\overbrace{\hspace{2.7em}}^{c_i}$};
 
    % Add a vertical brace for r_j
    \node[rotate=-90] at (6.3, 2.5) {$\overbrace{\hspace{2.7em}}^{r_i}$};

    % Add labels for x_i and y_j
    \node[anchor=north] at (3,-0.2) {$x_i$};
    \node[anchor=east] at (-0.15,2) {$y_j$};

    % Add the n_ij label in the center of the grid
    \node at (3.5,2.4) {$n_{ij}$};

\end{tikzpicture} 
\caption{Red Grid for Bayes Calc (edit later)}
\label{fig:red_bayes_grid}
\end{figure}

\hypertarget{sum-and-product-rules-of-probability}{%
\section{Sum and Product Rules of
Probability}\label{sum-and-product-rules-of-probability}}

To derive the \textbf{sum and product rules of probability}, consider
the following:

\begin{itemize}
\tightlist
\item
  We have two \textbf{random variables}:
\end{itemize}

-- \(X\), which takes values \(\{x_i\}\) where \(i = 1, \ldots, L\) --
\(Y\), which takes values \(\{y_j\}\) where \(j = 1, \ldots, M\)

\begin{itemize}
\tightlist
\item
  For this illustration:

  \begin{itemize}
  \tightlist
  \item
    \(L = 5\)
  \item
    \(M = 3\)
  \end{itemize}
\end{itemize}

\hypertarget{definitions}{%
\subsection{Definitions:}\label{definitions}}

\begin{itemize}
\tightlist
\item
  \textbf{Total instances}: Let \(N\) denote the total number of
  instances of these variables.
\item
  \textbf{Joint instances}: The number of instances where \(X = x_i\)
  and \(Y = y_j\) is \(n_{ij}\). This corresponds to the \textbf{number
  of instances in the \((i, j)\) cell of the array}.
\item
  \textbf{Column totals}:

  \begin{itemize}
  \tightlist
  \item
    The total number of instances in column \(i\), corresponding to
    \(X = x_i\), is \(c_i\).
  \end{itemize}
\item
  \textbf{Row totals}:

  \begin{itemize}
  \tightlist
  \item
    The total number of instances in row \(j\), corresponding to
    \(Y = y_j\), is \(r_j\).
  \end{itemize}
\end{itemize}

\hypertarget{summary-of-relationships}{%
\subsection{Summary of Relationships:}\label{summary-of-relationships}}

\[
\sum_{j=1}^{M} n_{ij} = c_i \quad \text{(Sum over rows for each column $i$)} 
\]

\[
\sum_{i=1}^{L} n_{ij} = r_j \quad \text{(Sum over columns for each row $j$)}
\]

\hypertarget{observations}{%
\subsection{Observations:}\label{observations}}

\begin{itemize}
\tightlist
\item
  The joint frequency \(n_{ij}\) determines how many instances satisfy
  \(X = x_i\) and \(Y = y_j\).
\item
  Marginal counts \(c_i\) and \(r_j\) are computed as the \textbf{sum of
  joint frequencies} over their respective rows or columns.
\end{itemize}

\newpage

\newpage

\hypertarget{connectinge-bayes-to-naive-bayes}{%
\section{Connectinge Bayes' to Naive
Bayes'}\label{connectinge-bayes-to-naive-bayes}}

In the context of extending your original Bayesian framework to Naive
Bayes, the key modification comes from how the events \(A\) and \(B\)
are treated when there is an assumption of conditional independence
between features in a classification context.

In Naive Bayes, the assumption is that the features (or events) are
conditionally independent given the class. This assumption drastically
simplifies the computation of the posterior probability, making it
computationally efficient for tasks like classification. Here's how the
core concepts from traditional Bayesian inference are adapted in Naive
Bayes:

Naive Bayes: Conditional Independence Assumption In traditional Bayesian
analysis, you might compute the probability of a set of features given a
class using a general conditional probability like:

\[
P(A \mid B)=\frac{P(A \cap B)}{P(B)}
\]

Where \(A\) and \(B\) represent some events. However, in the context of
Naive Bayes, if \(A_1, A_2, \ldots, A_n\) are features and \(B\) is the
class, the assumption of conditional independence leads to:

\[
P\left(B \mid A_1, A_2, \ldots, A_n\right)=\frac{P(B) \prod_{i=1}^n P\left(A_i \mid B\right)}{P\left(A_1, A_2, \ldots, A_n\right)}
\]

This assumption simplifies the likelihood term
\(P\left(A_1, A_2, \ldots, A_n \mid B\right)\) as the product of
individual conditional probabilities:

\[
P\left(A_1, A_2, \ldots, A_n \mid B\right)=\prod_{i=1}^n P\left(A_i \mid B\right)
\]

This is what makes Naive Bayes ``naive'' - it assumes that each feature
(or piece of evidence) contributes independently to the probability of
the class.

\newpage

\hypertarget{raw-bayes-text-here-first}{%
\section{Raw Bayes Text Here First}\label{raw-bayes-text-here-first}}

\newpage

\newpage

\hypertarget{motivation-for-laplace-smoothing}{%
\section{Motivation for Laplace
Smoothing}\label{motivation-for-laplace-smoothing}}

In probabilistic models like Naive Bayes, the posterior probability of a
class is calculated as a product of conditional probabilities. If any
one of these conditional probabilities is zero, the entire product
becomes zero:

\[
P(C \mid D) \propto P(C) \prod_{i=1}^n P(x_i \mid C)
\]

Here: - \(C\) is the class, - \(D = \{x_1, x_2, \dots, x_n\}\) is the
set of observed features.

If \(P(x_k \mid C) = 0\) for any \(k\), the entire product becomes zero:

\[
P(C \mid D) = 0 \quad \text{if any} \quad P(x_k \mid C) = 0
\]

This occurs when a feature \(x_k\) is absent in the training data for a
given class \(C\), leading to zero probability. This is problematic as
it eliminates the class from consideration even when other features
strongly support it.

\textbf{Solution: Laplace Smoothing}

To address this issue, we add a small constant \(\alpha\) (typically
\(\alpha = 1\)) to the counts, ensuring no probability is ever zero.
This technique is called \textbf{Laplace smoothing}.

\hypertarget{formal-definition-of-laplace-smoothing}{%
\section{Formal Definition of Laplace
Smoothing}\label{formal-definition-of-laplace-smoothing}}

\textbf{Without Smoothing}

The conditional probability \(P(x \mid C)\) is defined as:

\[
P(x \mid C) = \frac{\text{count}(x, C)}{\sum_{x' \in V} \text{count}(x', C)}
\]

Where: - \(\text{count}(x, C)\): Frequency of feature \(x\) in class
\(C\), - \(V\): The set of all possible features, -
\(\sum_{x' \in V} \text{count}(x', C)\): Total frequency of features in
class \(C\).

If \(\text{count}(x, C) = 0\), then \(P(x \mid C) = 0\), resulting in
\(P(C \mid D) = 0\).

\textbf{With Laplace Smoothing}

Laplace smoothing adjusts the formula to:

\[
P(x \mid C) = \frac{\text{count}(x, C) + \alpha}{\sum_{x' \in V} \text{count}(x', C) + \alpha |V|}
\]

Where: - \(\alpha\): Smoothing parameter (typically \(\alpha = 1\)), -
\(|V|\): The number of unique features.

\hypertarget{naive-bayes-formula-with-laplace-smoothing}{%
\section{Naive Bayes Formula with Laplace
Smoothing}\label{naive-bayes-formula-with-laplace-smoothing}}

With Laplace smoothing, the posterior probability is computed as:

\[
P(C \mid D) \propto P(C) \prod_{i=1}^n \frac{\text{count}(x_i, C) + \alpha}{\sum_{x' \in V} \text{count}(x', C) + \alpha |V|}
\]

\textbf{Advantages of Laplace Smoothing}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensures \(P(x \mid C) > 0\) for all features \(x\), avoiding zero
  probabilities.
\item
  Handles previously unseen features gracefully during prediction.
\end{enumerate}

** Implementation in R**

Below is an example of Laplace smoothing implementation in R:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example of Laplace smoothing implementation}

\CommentTok{\# Feature counts in two classes}
\NormalTok{feature\_counts\_class1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{feature\_counts\_class2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# Total feature counts in each class}
\NormalTok{total\_class1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(feature\_counts\_class1)}
\NormalTok{total\_class2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(feature\_counts\_class2)}

\CommentTok{\# Vocabulary size (number of unique features)}
\NormalTok{vocab\_size }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(feature\_counts\_class1)}

\CommentTok{\# Smoothing parameter (alpha)}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \DecValTok{1}

\CommentTok{\# Calculate smoothed probabilities for a feature in each class}
\NormalTok{smoothed\_prob\_class1 }\OtherTok{\textless{}{-}}\NormalTok{ (feature\_counts\_class1 }\SpecialCharTok{+}\NormalTok{ alpha) }\SpecialCharTok{/}\NormalTok{ (total\_class1 }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ vocab\_size)}
\NormalTok{smoothed\_prob\_class2 }\OtherTok{\textless{}{-}}\NormalTok{ (feature\_counts\_class2 }\SpecialCharTok{+}\NormalTok{ alpha) }\SpecialCharTok{/}\NormalTok{ (total\_class2 }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ vocab\_size)}

\CommentTok{\# Print the smoothed probabilities}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Smoothed probabilities for class 1:"}\NormalTok{, smoothed\_prob\_class1, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Smoothed probabilities for class 2:"}\NormalTok{, smoothed\_prob\_class2, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{example-of-zero-probability-problem-with-nb}{%
\section{Example of Zero Probability Problem (with
NB)}\label{example-of-zero-probability-problem-with-nb}}

For SPAM Email assessment

In the Naive Bayes algorithm, the conditional probability of a word
given a class is calculated as:

\[
P(\text{word} | \text{class}) = \frac{\text{count}(\text{word in class})}{\text{total words in class}}
\]

If a word does not appear in a class in the training data,
\(\text{count}(\text{word in class}) = 0\), resulting in
\(P(\text{word} | \text{class}) = 0\). This leads to a zero probability
for the entire class since probabilities are multiplied in Naive Bayes.

\hypertarget{solution-laplace-smoothing}{%
\section{Solution: Laplace Smoothing}\label{solution-laplace-smoothing}}

To address this issue, we use Laplace smoothing, which ensures no
probability is ever zero. The formula becomes:

\[
P(\text{word} | \text{class}) = \frac{\text{count}(\text{word in class}) + 1}{\text{total words in class} + V}
\]

Where: - \(V\) is the vocabulary size (the total number of unique words
in the dataset). - \(+1\) ensures all probabilities are greater than
zero.

\hypertarget{adjusted-naive-bayes-formula}{%
\subsection{Adjusted Naive Bayes
Formula}\label{adjusted-naive-bayes-formula}}

After applying Laplace smoothing, the Naive Bayes formula is:

\[
P(\text{class} | \text{document}) \propto P(\text{class}) \prod_{\text{word} \in \text{document}} \frac{\text{count}(\text{word in class}) + 1}{\text{total words in class} + V}
\]

\hypertarget{implementation-in-r}{%
\section{Implementation in R}\label{implementation-in-r}}

Here's how you can implement Laplace smoothing in R:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Vocabulary}
\NormalTok{vocabulary }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Lunch"}\NormalTok{, }\StringTok{"Money"}\NormalTok{, }\StringTok{"Dear"}\NormalTok{, }\StringTok{"Friend"}\NormalTok{)}

\CommentTok{\# Word counts in two classes: Normal (N) and Spam (S)}
\NormalTok{word\_counts\_N }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{Lunch =} \DecValTok{2}\NormalTok{, }\AttributeTok{Money =} \DecValTok{1}\NormalTok{, }\AttributeTok{Dear =} \DecValTok{5}\NormalTok{, }\AttributeTok{Friend =} \DecValTok{3}\NormalTok{)}
\NormalTok{word\_counts\_S }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{Lunch =} \DecValTok{0}\NormalTok{, }\AttributeTok{Money =} \DecValTok{7}\NormalTok{, }\AttributeTok{Dear =} \DecValTok{2}\NormalTok{, }\AttributeTok{Friend =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Total words in each class}
\NormalTok{total\_words\_N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(word\_counts\_N)}
\NormalTok{total\_words\_S }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(word\_counts\_S)}

\CommentTok{\# Vocabulary size}
\NormalTok{V }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(vocabulary)}

\CommentTok{\# Laplace Smoothed Probabilities for word "Lunch"}
\NormalTok{P\_Lunch\_given\_N }\OtherTok{\textless{}{-}}\NormalTok{ (word\_counts\_N[}\StringTok{"Lunch"}\NormalTok{] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (total\_words\_N }\SpecialCharTok{+}\NormalTok{ V)}
\NormalTok{P\_Lunch\_given\_S }\OtherTok{\textless{}{-}}\NormalTok{ (word\_counts\_S[}\StringTok{"Lunch"}\NormalTok{] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (total\_words\_S }\SpecialCharTok{+}\NormalTok{ V)}

\CommentTok{\# Output}
\FunctionTok{cat}\NormalTok{(}\StringTok{"P(Lunch | N) ="}\NormalTok{, P\_Lunch\_given\_N, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"P(Lunch | S) ="}\NormalTok{, P\_Lunch\_given\_S, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\newpage

\hypertarget{naive-bayes-commutative-property}{%
\section{Naive Bayes Commutative
Property}\label{naive-bayes-commutative-property}}

Naive Bayes makes a critical assumption: \textbf{features are
conditionally independent given the class}. This implies that the order
of features does not affect the computed probabilities. Mathematically:

\[
P(C \mid x_1, x_2, \ldots, x_n) \propto P(C) \prod_{i=1}^n P(x_i \mid C)
\]

Since multiplication is commutative, rearranging the features
\(x_1, x_2, \ldots, x_n\) does not change the result:

\[
P(C \mid x_1, x_2, \ldots, x_n) = P(C \mid x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(n)})
\]

where \(\sigma\) is any permutation of the feature indices. For example:

\[
P(C \mid x_1, x_2) = P(C \mid x_2, x_1)
\]

\textbf{Conditional Independence Assumption}

The root cause of this behavior is the \textbf{conditional independence
assumption} in Naive Bayes:

\[
P(x_1, x_2, \ldots, x_n \mid C) = \prod_{i=1}^n P(x_i \mid C)
\]

As a result: 1. The sequence \(x_1, x_2\) produces the same result as
\(x_2, x_1\). 2. Any sequential or contextual dependency between
features is ignored.

For instance:

\[
P(x_1, x_2 \mid C) = P(x_2, x_1 \mid C)
\]

This makes Naive Bayes invariant to the order of features.

** Implications of the Commutative Property**

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Naivety of Naive Bayes
\end{enumerate}

By ignoring feature order, Naive Bayes becomes naive in applications
where sequential dependencies matter, such as: - Natural language
processing (e.g., ``Friend Dear'' vs.~``Dear Friend''), - Time-series
analysis (e.g., stock price sequences).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Loss of Information
\end{enumerate}

For datasets where the order of features carries important information,
the independence assumption results in a loss of discriminatory power.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Trade-off for Simplicity
\end{enumerate}

The simplicity of the model allows for fast computation and scalability
but at the cost of ignoring meaningful relationships between features.

\textbf{Addressing the Limitation}

Alternative approaches that account for sequential dependencies include:
1. \textbf{N-gram Models}: Capture dependencies between adjacent
features in text. 2. \textbf{Hidden Markov Models (HMMs)}: Incorporate
sequential information. 3. \textbf{Neural Networks}: Modern
architectures (e.g., RNNs, Transformers) explicitly model feature order.

Here is a simple example of Naive Bayes ignoring feature order:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example data}
\NormalTok{class\_probs }\OtherTok{\textless{}{-}} \FloatTok{0.67}  \CommentTok{\# P(C)}
\NormalTok{feature\_probs }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{Dear =} \FloatTok{0.43}\NormalTok{, }\AttributeTok{Friend =} \FloatTok{0.29}\NormalTok{)  }\CommentTok{\# P(x | C)}

\CommentTok{\# Compute scores for "Dear Friend" and "Friend Dear"}
\NormalTok{score\_dear\_friend }\OtherTok{\textless{}{-}}\NormalTok{ class\_probs }\SpecialCharTok{*}\NormalTok{ feature\_probs[}\StringTok{"Dear"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ feature\_probs[}\StringTok{"Friend"}\NormalTok{]}
\NormalTok{score\_friend\_dear }\OtherTok{\textless{}{-}}\NormalTok{ class\_probs }\SpecialCharTok{*}\NormalTok{ feature\_probs[}\StringTok{"Friend"}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ feature\_probs[}\StringTok{"Dear"}\NormalTok{]}

\CommentTok{\# Output}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Score for \textquotesingle{}Dear Friend\textquotesingle{}:"}\NormalTok{, score\_dear\_friend, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Score for \textquotesingle{}Friend Dear\textquotesingle{}:"}\NormalTok{, score\_friend\_dear, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\newpage

\newpage

\hypertarget{modifications-in-venn-diagram-representation}{%
\section{Modifications in Venn Diagram
Representation}\label{modifications-in-venn-diagram-representation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Traditional Bayesian Space: The Venn diagram you provided earlier
  represents events \(A\) and \(B\) within the universal space
  \(\Omega\). Each event may overlap or be disjoint, and their
  interactions are captured through their joint, marginal, and
  conditional probabilities.
\item
  Naive Bayes Representation: The key modification is that, in the Naive
  Bayes model, we assume that each feature \(A_1, A_2, \ldots, A_n\) is
  conditionally independent given the class \(B\). This simplification
  means the probability space is represented differently, potentially
  with separate Venn diagrams for each feature given \(B\).
\end{enumerate}

In the case of Naive Bayes, you can think of the events
\(A_1, A_2, \ldots, A_n\) as having their own Venn diagrams, where each
feature \(A_i\) contributes independently to the classification
decision. The overlap between these features and \(B\) is still shown in
the diagram, but the assumption of conditional independence implies that
these overlaps do not interact as they would in a more general Bayesian
framework.

Modified Probability Space for Naive Bayes For Naive Bayes, we are
interested in the posterior probability of the class \(B\) given the
features \(A_1, A_2, \ldots, A_n\) :

\[
P\left(B \mid A_1, A_2, \ldots, A_n\right)=\frac{P(B) \prod_{i=1}^n P\left(A_i \mid B\right)}{P\left(A_1, A_2, \ldots, A_n\right)}
\]

This equation breaks down into: 1. Prior: \(P(B)\) - the prior
probability of class \(B\). 2. Likelihood:
\(\prod_{i=1}^n P\left(A_i \mid B\right)\) - the product of the
conditional probabilities for each feature given the class. 3.
Normalization: \(P\left(A_1, A_2, \ldots, A_n\right)\) - the likelihood
of the observed features across all classes, used for normalization.

\hypertarget{modifications-to-your-original-bayesian-framework}{%
\section{Modifications to Your Original Bayesian
Framework}\label{modifications-to-your-original-bayesian-framework}}

The Venn diagram for Naive Bayes will primarily have distinct regions
where each feature \(A_1, A_2, \ldots, A_n\) is considered conditionally
independent of each other, but dependent on the class \(B\). These
dependencies are shown as separate slices or parts of the overall space,
but they do not overlap in the way traditional Bayesian probability
might depict interactions between events. 1. The Venn diagram of the
original Bayesian framework might have complex intersections between
events \(A\) and \(B\), as they can influence each other. 2. In the
Naive Bayes model, the overlap between events is reduced due to the
assumption of conditional independence. Each feature is considered
independently given the class, simplifying the model.

Thus, while the structure of the probability space in both models
follows a similar form, the key difference is how the conditional
independence assumption in Naive Bayes simplifies the joint probability
computation. Instead of modeling the full interaction between features,
Naive Bayes reduces it to the product of individual feature
probabilities conditioned on the class.

\hypertarget{naive-bayes-as-filter}{%
\section{Naive Bayes' as Filter}\label{naive-bayes-as-filter}}

Yes, Naive Bayes can be thought of as a filter in certain contexts,
especially in classification tasks. The idea behind this is that Naive
Bayes filters out irrelevant features (or at least gives them less
weight) due to its assumption of conditional independence.

Key Aspects of Naive Bayes as a Filter:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Feature Relevance: Naive Bayes classifies by calculating the
  probability of each class given the features. However, because of the
  conditional independence assumption, it treats each feature
  independently and does not model interactions between features. As a
  result, irrelevant features that do not help discriminate between
  classes will not have a significant impact on the classification. In
  this sense, Naive Bayes inherently ``filters out'' noisy features that
  don't provide discriminative power.
\item
  Probabilistic Thresholding: Naive Bayes computes the posterior
  probability of each class using Bayes' theorem, and the class with the
  highest probability is chosen. Features that are more relevant to the
  class have a stronger influence on the posterior probability. Features
  that are less relevant have a smaller influence because their
  conditional probability given the class is likely to be uniform or
  less informative. This allows Naive Bayes to ``filter'' out noisy
  features by giving them a lower weight in the final decision.
\item
  Handling Redundant Features: Because Naive Bayes treats each feature
  as independent, it does not take into account any redundant or
  correlated features (which may be a source of noise in other
  classification algorithms). In cases where features are highly
  correlated, Naive Bayes essentially treats them separately, and the
  model does not suffer as much from overfitting due to feature
  redundancy. This simplification can act as a form of regularization,
  which also helps in ``filtering out'' the unnecessary complexity that
  might arise from correlated features.
\end{enumerate}

\newpage

Naive Bayes in Practice - Text Classification: In tasks like spam
detection, Naive Bayes acts as a filter by giving higher probabilities
to features (e.g., certain words) that are more indicative of spam or
ham (non-spam). The algorithm does not consider the relationship between
words but simply filters through each word's likelihood in the context
of spam and ham categories. - Feature Selection: In situations where you
have a large number of features, Naive Bayes naturally reduces the
influence of irrelevant features. While it's not a feature selection
method per se (like some others that explicitly remove unimportant
features), its simplification (treating features independently)
effectively reduces the impact of less important features.

Limitations of Naive Bayes as a Filter: While Naive Bayes does perform a
form of feature weighting implicitly, it's important to note that it's
not a filtering algorithm in the strict sense, like some other models
(e.g., Decision Trees, Random Forests, or feature selection algorithms)
that explicitly discard features. Naive Bayes doesn't eliminate features
or explicitly ignore them; instead, it simply assigns lower
probabilities to features that don't significantly contribute to class
distinctions.

In Summary: Naive Bayes can act as a filter in that it implicitly
``filters out'' irrelevant or less relevant features by not considering
the dependencies between them and by giving less weight to features that
don't distinguish well between classes. However, it doesn't perform
explicit feature selection or removal like other models that focus on
filtering features based on their importance.

\newpage

\hypertarget{example-to-classical-bayes-vs.-naive-bayes}{%
\section{Example to classical Bayes' vs.~Naive
Bayes'}\label{example-to-classical-bayes-vs.-naive-bayes}}

In the scenario presented in the image, the problem involves a medical
test for a condition (illness), with the concern being a false positive
(i.e., when the test indicates illness, but the person is actually
healthy). Let's break down how Naive Bayes could be used to address this
problem, step by step.

Key Components of the Scenario:

Events:

\begin{itemize}
\tightlist
\item
  A: The person has the illness (cancer).
\item
  \(\sim\) A: The person is illness-free.
\item
  \(B\) : The test result is positive.
\item
  \(\sim B\) : The test result is negative.
\end{itemize}

Given Data:

\begin{itemize}
\tightlist
\item
  \(P(A \mid B)\) : The probability that the person has the illness
  given a positive test result.
\item
  \(P(\sim A \mid B)\) : The probability that the person is illness-free
  given a positive test result.
\item
  The contingency table is used to provide the necessary data for
  calculating these probabilities.
\end{itemize}

From the image, we have the following values from the contingency table:

\begin{tabular}{|l|l|l|}
\hline Test Result & Illness $(A)$ & Iliness-Free $(\sim A)$ \\
\hline Positive & 8 & 103 \\
\hline Negative & 2 & 895 \\
\hline
\end{tabular}

So:

\begin{itemize}
\tightlist
\item
  Total number of tests \(=8+2+103+895=1008\).
\item
  Number of true positives (illness and positive test, \(A \cap B\) )
  \(=8\).
\item
  Number of false positives (illness-free and positive test,
  \(\sim A \cap B\) ) \(=103\).
\item
  Number of true negatives (illness-free and negative test,
  \(\sim A \cap \sim B\) ) \(=895\).
\item
  Number of false negatives (iliness and negative test,
  \(A \cap \sim B\) ) \(=2\).
\end{itemize}

\newpage

\hypertarget{using-bayes-and-the-tie-to-physics}{%
\section{USing Bayes and the tie to
physics}\label{using-bayes-and-the-tie-to-physics}}

\hypertarget{physics-analogies-and-bayesian-reasoning-models}{%
\subsection{Physics Analogies and Bayesian Reasoning
Models}\label{physics-analogies-and-bayesian-reasoning-models}}

A key concept in information theory, which yields its implied
relationship to physics, is the average value (or expectation) of the
information content of events (symbols arriving at the end of a
communication line):

\[
H(x)=-\sum_x P(x) \log _2 P(x)
\]

This symbol \(H(x)\) is often called information entropy, drawing an
analogy to physics (specifically thermodynamics), where entropy is a
measure of disorder, particularly the number of specific ways that a set
of particles may be arranged in space. As Feynman (2000) illustrated, we
can understand this by considering a hypothetical situation \(A\): a
volume containing a single molecule of gas with \(X\) equally likely
locations. In this uniform distribution across \(X\) locations, the
information needed to describe any possible location is given by:

\[
H_A(x)=-\sum_x P(x) \log _2 P(x)=-x\left(\frac{1}{x}\right) \log _2\left(\frac{1}{x}\right)=-\log _2\left(\frac{1}{x}\right)
\]

Thus, any message indicating the particle's location has this
information content. While this scenario assumes uniform distribution,
one can generalize this approach to compute the average information
content across any distribution.

Now, consider performing work to compress the gas, halving the original
volume (situation \(B\)). Here, half the terms in the sum are zeroed
out, the remaining probabilities double, and the bits required to convey
the particle's location are given by:

\[
H_B(x)=-\log _2\left(\frac{1}{2 x}\right)=-\log _2\left(\frac{1}{x}\right)-1=H_A(x)-1
\]

In this way, the volume compression effectively reduces the information
entropy (expected information content) of messages indicating the
particle's location by one bit.

\hypertarget{extending-the-analogy-to-biologics-lot-to-lot-variance}{%
\subsection{Extending the Analogy to Biologics Lot-to-Lot
Variance}\label{extending-the-analogy-to-biologics-lot-to-lot-variance}}

In biologics, particularly in the production of drug substances, there
is an analogous concept where we can think of ``information'' as the
variability or uncertainty in quality attributes across different
production lots. Consider the production of several biologics lots,
where we measure attributes such as purity, potency, and concentration.
Each lot can be thought of as a unique ``event'' with an associated
probability distribution, akin to particle locations in thermodynamics.

For small set sizes, such as when \(n = 3\) to \(5\), we face higher
uncertainty and variance in these quality attributes across lots.
Information entropy, in this case, represents the uncertainty in
predicting the quality attributes for future lots based on initial
measurements. The entropy \(H(x)\) of these quality attributes might
indicate how ``disordered'' or variable our lot-to-lot outcomes are
under initial conditions.

Imagine now performing a series of adjustments after each production
run. By incorporating learnings from each new lot (e.g., 1 through 5),
we might make adjustments to the process parameters to reduce
variability. This is analogous to ``compressing'' the space in which our
attributes vary, akin to reducing the gas volume. For example, if we
find that a specific pH adjustment improves consistency, we may decrease
the uncertainty (information entropy) about the quality attributes of
subsequent lots. The expected entropy of future lots \(6\), \(7\),
\(8\), etc., becomes:

\[
H_{\text{new}}(x) = H_{\text{initial}}(x) - \Delta H
\]

where \(\Delta H\) represents the reduction in entropy due to process
optimization and learnings from previous lots.

Thus, the iterative nature of Bayesian reasoning in this context
reflects how biologics manufacturing often relies on sequential data
collection and process optimization. Just as reducing a gas's volume
constrains the possible locations for particles, process adjustments
reduce the variability in quality attributes, thereby lowering the
information entropy and enhancing the predictability of future lot
outcomes.

In practice, this iterative approach of learning and adjustment can
reduce the ``noise'' in biologic production and improve consistency,
making the analogy between thermodynamic entropy and biologics
lot-to-lot variance a powerful conceptual tool.

\newpage

\hypertarget{using-naive-bayes}{%
\section{Using Naive Bayes}\label{using-naive-bayes}}

Naive Bayes assumes conditional independence between features (in this
case, the presence of illness and the test result). However, since we
are just using the test result as evidence for illness or not, the Naive
Bayes classifier simplifies to using Bayes' theorem directly.

The posterior probability of having the illness given a positive test
result is given by Bayes' theorem:

\[
P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}
\]

Where: - \(P(A \mid B)\) : Probability of having the illness given a
positive test result. - \(P(B \mid A)\) : Probability of testing
positive given that the person has the illness (True Positive Rate). -
\(\quad P(A)\) : Prior probability of having the illness (based on the
population's illness rate). - \(P(B)\) : Probability of testing
positive, which is the total probability of a positive test (True
Positives + False Positives).

Step-by-Step Calculation: 1. Prior Probability \(P(A)\) : This is the
probability of having the illness in the population:

\[
P(A)=\frac{\text { Number of illness cases }}{\text { Total number of tests }}=\frac{8+2}{1008}=\frac{10}{1008} \approx 0.0099
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Likelihood \(P(B \mid A)\) : This is the probability of testing
  positive given the person has the illness:
\end{enumerate}

\[
P(B \mid A)=\frac{\text { True Positives }}{\text { Total illness cases }}=\frac{8}{10}=0.8
\]

\newpage

\[
P(B \mid A)=\frac{\text { True Positives }}{\text { Total illness cases }}=\frac{8}{10}=0.8
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Evidence \(P(B)\) : This is the total probability of a positive test
  result:
\end{enumerate}

\[
P(B)=\frac{\text { True Positives + False Positives }}{\text { Total number of tests }}=\frac{8+103}{1008}=\frac{111}{1008} \approx 0.1102
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Posterior Probability \(P(A \mid B)\) : Now, applying Bayes' theorem:
\end{enumerate}

\[
P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}=\frac{(0.8)(0.0099)}{0.1102} \approx 0.0717
\]

Thus, the probability that a person has the illness given a positive
test result is approximately 7.17\%.

Conclusion: Even though the test is positive, the low prior probability
of the illness ( \(0.99 \%\) ) and the high false positive rate ( 103
out of 895 illness-free people test positive) result in a relatively low
posterior probability for the person actually having the illness. This
demonstrates how Naive Bayes uses the data and Bayes' theorem to provide
a more accurate estimate of the illness probability, given the test
results.

\newpage

\newpage

\hypertarget{after-area}{%
\section{after area}\label{after-area}}

\hypertarget{bayes-start-building}{%
\section{Bayes start building}\label{bayes-start-building}}

\begin{tikzpicture}
    \draw[thick,->] (0,0) -- (1,1);
    \draw[thick,->] (0,0) -- (1,-1);
\end{tikzpicture}

\newpage

\hypertarget{maximum-likelihood}{%
\chapter{Maximum Likelihood}\label{maximum-likelihood}}

\huge

\[
\color{dblu}{\hat{\theta} = \underset{\theta}{ \mathop{\mathrm{arg\,max}}} \prod_{i=1}^n P(x_i\! \mid\! \theta)}
\] \normalsize

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-8-MLE.jpeg}
    \caption*{\Large Parametric based Maximum}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Maximum Likelihood Estimation}{def8} 

\begin{align}
\mathop{\hat{\theta} = \mathrm{arg\,max}}_{\theta} \prod_{i=1}^n P(x_i\! \mid\! \theta)
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$x_i$] The \textbf{observed data point} or the \( i \)-th element of the dataset, which the likelihood function evaluates based on the parameter \( \theta \). More specifically belongs to a set of data namely $x_i \in \{ x_1, x_2, \dots, x_n \} = X$.
    \vspace{0.5\baselineskip}
    \item[$\theta$] The \textbf{parameter(s)} to be estimated using MLE. These can represent different characteristics of the distribution (e.g., mean, variance). For the illustration here we are only considering one value for $\theta$. More generally belongs to a set of data namely $\theta_i \in \{ \theta_1, \theta_2, \dots, \theta_m \} = \Theta$.
    \vspace{0.5\baselineskip}
    \item[$\hat{\theta}$] The \textbf{MLE estimate} of the parameter $\theta$, representing the value that maximizes the likelihood function.
    \vspace{0.5\baselineskip}
    \item[$P(x_i | \theta)$] The \textbf{likelihood function} evaluated for the data point $ x_i$ given the parameter $\theta$. It represents the probability of observing $x_i$ under the assumed parameter.
    % weird spacing correction ... \vspace{-0.3\baselineskip}
    \vspace{-0.3\baselineskip}
    \item[$\prod$] The \textbf{product operator}, which combines the likelihoods of all observed data points $x_i \in \{ x_1, x_2, \dots, x_n \} = X$ to compute the overall likelihood of the dataset.
    \vspace{0.5\baselineskip}
    \item[$\underset{\theta}{\mathop{\mathrm{arg\,max}}}$] The \textbf{argument of the maximum}, which says to return the value of \( \theta \) that maximizes the likelihood function.
    \vspace{0.5\baselineskip}
\end{description}

\end{a_def_eq}

\textbf{Introduction}: MLE is a method used to estimate the parameters
of a statistical model.

\textbf{Description}: It finds the parameter values \(\Theta\) that
maximize the likelihood of making the observations given the model
\(P(x_i\! \mid\! \theta)\), and a specific data set \(X\).

\textbf{Importance in ML}: MLE is used to train many machine learning
models, including logistic regression and Gaussian mixture models. It
provides a way to fit models to data and make statistical inferences.

\vspace*{\fill}

\newpage

\hypertarget{what-is-behind-the-equation-6}{%
\section{What is Behind the
Equation}\label{what-is-behind-the-equation-6}}

The Maximum Likelihood Estimation equation, while compact in its
representation, embodies several fundamental statistical concepts and
operational components that work together to find optimal parameter
estimates. Let's break down its key elements and underlying principles:

\textbf{Core Components of the MLE}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Probability Model Structure
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The equation centers on \(P(x_i|\theta)\), which represents our
  assumed probabilistic model.
\item
  This model defines how we believe our data is probabilistically
  represented.
\item
  The vertical bar ``\(\mid\)'' indicates conditional probability:
  ``probability of data \(x_i\) given parameter \(\theta\)''.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Independence Assumption
\end{enumerate}

\begin{itemize}
\tightlist
\item
  We assume, that the product \(\prod\) implies that each probability
  \(P(x_i\! \mid\! \theta)\) for a given \(x_1\) observation is
  independent.
\item
  This allows the benefit to multiply individual probabilities or each
  of the n observations:
  \(P(X|\theta) = P(x_1|\theta) \times P(x_2|\theta) \times ... \times P(x_n|\theta)\).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Optimization Framework
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The operator \(\arg\max_{\theta}\) indicates we're searching for the
  best parameter value.
\item
  This search spans the entire parameter space for \(\theta\).
\item
  We seek the global maximum of the likelihood function with repsect to
  \(\theta\).
\end{itemize}

\textbf{Mathematical Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Monotonic Transformation
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Often converted to log-likelihood for computational efficiency
  leveraging the identity property of logs via
  \(\log(\prod P(x_i|\theta)) = \sum \log P(x_i|\theta)\).
\item
  This transforms products into sums while preserving the maximum value
  at some \(\theta\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Derivatives and Critical Points
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Finding maximum often involves setting derivatives to zero
  \(\frac{\partial}{\partial \theta} \log L(\theta) = 0\).
\item
  Second derivatives confirm maximum vs.~minimum via the Hessian.
\end{itemize}

\textbf{Statistical Implications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample Size Effects
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Larger n typically leads to more accurate estimates.
\item
  Product terms count grows with each additional observation.
\item
  Asymptotic properties become more relevant with large n.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Parameter Space
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(\theta\) can be single-valued or multi-dimensional
\item
  Parameter constraints affect the optimization
\item
  Solution must lie within valid parameter space
\end{itemize}

\textbf{Practical Considerations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computational Aspects
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Numerical underflow can occur with large products
\item
  Log transformation helps prevent computational issues
\item
  Optimization algorithms may need constraints
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Model Selection Impact
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Choice of probability model affects likelihood structure
\item
  Different models lead to different estimation procedures
\item
  Model assumptions must be validated T his equation thus represents a
  sophisticated interplay between probability theory, optimization
  methods, and statistical inference, providing a systematic framework
  for parameter estimation in statistical modeling and machine learning
  applications.
\end{itemize}

\hypertarget{historical-context-3}{%
\section{Historical Context}\label{historical-context-3}}

The method of Maximum Likelihood Estimation was first formally
introduced by \textbf{Ronald A. Fisher} in his 1922 paper \emph{``On the
Mathematical Foundations of Theoretical Statistics''}. Fisher's
groundbreaking work laid the foundation for modern statistical theory,
and MLE quickly became one of the most widely adopted estimation
methods. However, the ideas behind MLE can be traced back even further
to \textbf{Carl Friedrich Gauss} and \textbf{Pierre-Simon Laplace}, who
used related concepts in the context of least squares fitting and
probability theory.

The modern form of MLE was developed by \textbf{Ronald A. Fisher} in his
groundbreaking 1921 paper, \emph{``On the Mathematical Foundations of
Theoretical Statistics''}. Fisher introduced MLE as a method to estimate
parameters of a statistical model by maximizing the likelihood of
observing the data given those parameters. Fisher's work formalized MLE
and established its theoretical underpinnings, including the idea that
MLE estimators are asymptotically unbiased, efficient, and consistent
under regular conditions. Fisher's development of MLE was revolutionary
because it provided a clear, consistent framework for estimating model
parameters that was both flexible and powerful. Over the years, MLE has
found widespread use in many statistical applications, ranging from
simple regression models to complex models in machine learning and
bioinformatics.

\textbf{Impact and Adoption}

\begin{itemize}
\tightlist
\item
  Fisher's ideas were revolutionary because they provided a unified
  framework for parameter estimation that was both robust and flexible,
  applicable to many different types of statistical models.
\item
  MLE quickly became the cornerstone of statistical inference, replacing
  older methods like method of moments or least squares in many
  applications.
\end{itemize}

\textbf{Further Developments}

\begin{itemize}
\tightlist
\item
  In the decades following Fisher's work, \textbf{Bayesian methods} and
  \textbf{generalized likelihood approaches} expanded on MLE, but it
  remained a dominant method in frequentist statistics.
\item
  Modern developments have extended MLE to complex models and
  high-dimensional data, incorporating computational techniques like the
  \textbf{expectation-maximization (EM) algorithm} for handling missing
  data and latent variables.
\end{itemize}

MLE was first formally developed by Ronald Fisher in the 1920s, building
on earlier work by Gauss and Laplace. It became a foundational tool in
statistics due to its general applicability and strong theoretical
properties, such as consistency and efficiency. Over the years, it has
remained a key method in both classical and modern statistical modeling.

\newpage

\hypertarget{mle-generral-introduction}{%
\section{MLE Generral Introduction}\label{mle-generral-introduction}}

Maximum Likelihood Estimation (MLE) is one of the most widely used and
fundamental methods in statistical inference. It provides a systematic
approach for estimating the parameters of a probabilistic model based on
observed data. At its core, MLE seeks to find the parameter values that
make the observed data most likely, given a specific model. This method
plays a pivotal role in various fields, including regression analysis,
bioinformatics, machine learning, and econometrics, where it is
frequently used to \textbf{fit models to data} and make predictions.

\textbf{What is Maximum Likelihood Estimation?}

In simple terms, \textbf{Maximum Likelihood Estimation} aims to
determine the parameter values of a statistical model that would make
the observed data as probable as possible. Suppose we have a statistical
model parameterized by a set of parameters \(\theta\). Given some
observed data \(X = (x_1, x_2, \dots, x_n)\), the likelihood function
\(\mathcal{L}(\theta)\) expresses the probability of observing this data
as a function of the model parameters \(\theta\). The goal of MLE is to
find the parameter values \(\hat{\theta}\) that maximize this likelihood
function:

\[
\hat{\theta} = \arg \max_{\theta} \mathcal{L}(\theta\! \mid\! X)
\]

This can be thought of as searching for the set of parameters under
which the model would most likely generate the data we have observed.
The value of \(\hat{\theta}\) that achieves this maximization is called
the \textbf{Maximum Likelihood Estimate}.

\textbf{The Likelihood Function}

The likelihood function is central to MLE. For a given set of parameters
\(\theta\), it calculates the probability of observing the data set
\(X\). For independent and identically distributed (i.i.d.) random
variables, the likelihood is the product of the individual probabilities
(or probability density functions). For example, if the data
\(X = (X_1, X_2, \dots, X_n)\) follows a probability distribution
\(f(X_i | \theta)\), the likelihood function is:

\[
\mathcal{L}(\theta | X) = \prod_{i=1}^{n} f(X_i | \theta)
\]

In practice, it's often easier to work with the \textbf{log-likelihood
function}, which is the natural logarithm of the likelihood function.
The logarithmic transformation turns products into sums, simplifying the
maximization process:

\[
\ell(\theta | X) = \log \mathcal{L}(\theta | X) = \sum_{i=1}^{n} \log f(X_i | \theta)
\]

Maximizing the log-likelihood function is mathematically equivalent to
maximizing the likelihood itself, but it is computationally simpler and
more stable.

\hypertarget{likelihood-function}{%
\subsection{Likelihood Function}\label{likelihood-function}}

Given an assumed probability distribution (e.g., normal distribution,
exponential distribution, etc.), the likelihood function represents the
probability of observing the given data under that distribution. For
instance, if \(X_1, X_2, \dots, X_n\) are observed data points from a
distribution with parameter \(\theta\), the likelihood function
\(\mathcal{L}(\theta)\) is expressed as:

\[
\mathcal{L}(\theta | X) = P(\{x_1, x_2, \dots, x_n \}\! \mid\! \theta)
\]

Where \(P\) denotes the probability (or probability density) of
observing the data given the parameters.

The Left-Hand Side: \(\theta \mid X\) : The notation \(\theta \mid X\)
on the left-hand side indicates the conditional dependence of the
parameters on the observed data. We are interested in the distribution
of the parameters \(\theta\) given the data \(X\), and the likelihood
function \(L(\theta \mid X)\) is designed to express this. It's clear
that the likelihood is defined in terms of parameters, with the data
acting as the conditioning factor, further emphasizing that the model is
being adjusted to fit the data.

Likelihood Function Structure: The likelihood function
\(L(\theta \mid X)\) represents the probability of observing the data
\(X=\left(x_1, x_2, \ldots, x_n\right)\), given a particular set of
parameters \(\theta\). Notice the conditioning notation
\(P\left(x_i \mid \theta\right)\) on the right-hand side, where \(x_i\)
represents the observed data and \(\theta\) represents the unknown
parameters that govern the model. The likelihood function
\(L(\theta \mid X)\) then depends on the parameters \(\theta\) that we
wish to estimate.

The Concept of `Fitting the Model': This equation reinforces the idea of
``fitting models to data'' because we are searching for the parameter
values \(\hat{\theta}\) that maximize the likelihood of the data under
the model. We do not alter the observed data \(x_i\); rather, we adjust
the model's parameters \(\theta\) so that the model explains the data as
best as possible. Maximizing the likelihood function allows us to choose
the parameters that make the data as probable as possible under the
assumed model.

\hypertarget{the-log-likelihood}{%
\subsection{The Log-Likelihood}\label{the-log-likelihood}}

Log-Likelihood for Convenience: The log-likelihood function, which is
the logarithm of the likelihood, turns the product of probabilities into
a sum of log-probabilities. This transformation simplifies the
maximization process and stabilizes numerical calculations, especially
when dealing with small probabilities that could otherwise lead to
computational underflow. The loglikelihood still retains the same
relationship: the model's parameters \(\theta\) are adjusted, not the
data \(x_i\).

The log-likelihood function is typically easier to work with. By taking
the natural logarithm of the likelihood function, we convert the product
of probabilities into a sum, which is computationally more stable:

\[
\ell(\theta | X) = \log \left( \prod_{i=1}^{n} f(X_i | \theta) \right) = \sum_{i=1}^{n} \log f(X_i | \theta)
\]

This transformation simplifies differentiation and allows us to apply
optimization techniques more efficiently.

\textbf{Useful identities}

We remind the reader of some useful log identities that are useful in
the manipulation of these sourts of expressions:

\[
\begin{aligned}
\log(a \times b) &= \log(a) + \log(b)\\
\log \left(p^{a}\right)&=a \log (p) \\
\log \left(p^{a+b}\right)&=a \log (p)+b \log (p) \\
\log \left(\prod_{i=1}^{n} p^{x_{i}}\right)&=\sum_{i=1}^{n} x_{i} \log (p)\\ 
\prod_{i=1}^{n} p^{x_{i}}&=p^{\sum x_{i}} \\
\end{aligned}
\]

\hypertarget{maximum-likelihood-estimation}{%
\subsection{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation}}

The maximum likelihood estimator \(\hat{\theta}\) is the value of
\(\theta\) that maximizes the log-likelihood function. Mathematically,
this is equivalent to solving the following optimization problem:

\[
\hat{\theta} = \underset{\theta}{ \mathop{\mathrm{arg\,max}}} \ell(\theta | X)
\]

In cases where the likelihood function is differentiable, we can use
calculus to find the maximum. Specifically, we take the derivative of
the log-likelihood with respect to \(\theta\) and set it equal to zero:

\[
\frac{\partial}{\partial \theta} \ell(\theta | X) = 0
\]

This yields a set of equations that can be solved to find the optimal
parameter estimates.

\hypertarget{connection-to-other-estimation-methods}{%
\section{Connection to Other Estimation
Methods}\label{connection-to-other-estimation-methods}}

Maximum Likelihood Estimation is closely related to other methods of
statistical estimation, including \textbf{least squares estimation}. In
fact, for many common models, MLE is equivalent to the least squares
method. For example, in the case of linear regression where the errors
are assumed to follow a normal distribution with constant variance, the
MLE for the regression parameters turns out to be the same as the least
squares solution.

From a \textbf{Bayesian} perspective, MLE can be seen as a special case
of \textbf{maximum a posteriori (MAP) estimation} with a uniform prior.
In frequentist inference, MLE is a specific instance of an
\textbf{extremum estimator}, where the objective function is the
likelihood function.

\hypertarget{theoretical-properties-of-mle}{%
\section{Theoretical Properties of
MLE}\label{theoretical-properties-of-mle}}

One of the reasons MLE is so widely used is that it has desirable
\textbf{asymptotic properties} under certain regularity conditions:

\begin{itemize}
\item
  \textbf{Consistency}: As the sample size grows, the MLE
  \(\hat{\theta}\) converges to the true parameter value \(\theta\). In
  other words, MLE produces increasingly accurate estimates as more data
  becomes available.
\item
  \textbf{Asymptotic Normality}: Under regular conditions, the
  distribution of the MLE becomes approximately normal as the sample
  size increases. This means that the estimator becomes approximately
  unbiased and its variance shrinks as the sample size grows.
\item
  \textbf{Efficiency}: MLE is often asymptotically efficient, meaning it
  achieves the lowest possible variance among unbiased estimators. In
  fact, it attains the \textbf{Cramer-Rao lower bound}, which sets a
  lower bound on the variance of an unbiased estimator.
\end{itemize}

However, MLE is not without its challenges. In some cases, the
likelihood function may not have a closed-form solution, requiring
numerical optimization methods such as \textbf{gradient descent} or
\textbf{expectation-maximization} (EM). Additionally, MLE estimators can
be \textbf{sensitive to model misspecification}, and in small sample
sizes, they may exhibit \textbf{bias} or \textbf{overfitting}.

Maximum Likelihood Estimation is a powerful and widely used statistical
method for parameter estimation. By maximizing the likelihood function,
we can obtain parameter estimates that are consistent, efficient, and
asymptotically normal. This method is applicable in a wide range of
statistical models and provides a solid foundation for both frequentist
and Bayesian approaches to statistical inference.

In the following sections, we will delve deeper into MLE, explore
examples of its application, and discuss how to implement MLE in various
contexts. We will also examine the limitations of MLE and compare it to
alternative estimation methods.

\newpage

\hypertarget{the-role-of-the-fisher-information}{%
\section{The Role of the Fisher
Information}\label{the-role-of-the-fisher-information}}

The \textbf{Fisher Information} is a measure of the amount of
information that an observable random variable carries about an unknown
parameter. It plays a significant role in MLE, especially in assessing
the variance of the estimator.

\hypertarget{fisher-information}{%
\subsection{Fisher Information}\label{fisher-information}}

\[
I(\theta) = -\mathbb{E} \left[ \frac{\partial^2}{\partial \theta^2} \log \mathcal{L}(\theta) \right]
\]

The Fisher Information is used to compute the \textbf{Cramer-Rao lower
bound}, which sets a lower limit on the variance of an unbiased
estimator.

\hypertarget{the-role-of-the-cramer-rao-lower-bound}{%
\section{The Role of the Cramer-Rao Lower
Bound}\label{the-role-of-the-cramer-rao-lower-bound}}

The \textbf{Cramer-Rao lower bound (CRLB)} plays an important role in
statistical estimation theory, particularly in determining the
efficiency of an estimator. It provides a theoretical limit on the
variance of any unbiased estimator of a parameter, thereby offering a
benchmark against which the performance of different estimators can be
evaluated.

\textbf{Definition of the Cramer-Rao Lower Bound}

The \textbf{Cramer-Rao lower bound} establishes that, for any unbiased
estimator \(\hat{\theta}\) of a parameter \(\theta\), the variance of
\(\hat{\theta}\) cannot be smaller than the inverse of the
\textbf{Fisher Information} \(I(\theta)\):

\[
\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
\]

Where: - \(I(\theta)\) is the \textbf{Fisher Information} of the
parameter \(\theta\), which quantifies the amount of information the
observed data provides about the parameter.

\textbf{Intuition Behind the Cramer-Rao Lower Bound}

The CRLB gives an indication of how well a parameter can be estimated
based on the available data: - The \textbf{Fisher Information}
\(I(\theta)\) essentially measures the sensitivity of the likelihood
function to changes in the parameter \(\theta\). A high value of
\(I(\theta)\) implies that the likelihood function changes steeply with
respect to \(\theta\), indicating that the parameter is easier to
estimate. - Conversely, a lower value of \(I(\theta)\) means the
likelihood is relatively flat, suggesting that the parameter is harder
to estimate accurately.

The \textbf{CRLB} provides the lowest possible variance for an unbiased
estimator, which means that no other unbiased estimator can achieve a
variance lower than this bound. Thus, it serves as a benchmark for
evaluating the efficiency of an estimator.

\textbf{Efficiency of Estimators}

An estimator that \textbf{attains the CRLB} is called an
\textbf{efficient estimator}. Such an estimator is said to be optimal
because it achieves the minimum possible variance for unbiased
estimators, making it the best possible estimator in terms of accuracy
among the class of unbiased estimators.

For example, in the case of estimating the mean of a normal distribution
with known variance, the sample mean is an efficient estimator because
its variance matches the CRLB.

\hypertarget{role-in-statistical-inference}{%
\section{4. Role in Statistical
Inference}\label{role-in-statistical-inference}}

\begin{itemize}
\item
  \textbf{Theoretical Benchmark}: The CRLB is often used as a
  \textbf{theoretical benchmark} to determine how well an estimator is
  performing. If an estimator has a variance close to the CRLB, it is
  considered efficient and optimal. If the variance is far from the
  CRLB, there is potential room for improvement.
\item
  \textbf{Unbiased Estimators}: The CRLB is only applicable to
  \textbf{unbiased estimators}. An estimator is unbiased if its expected
  value equals the true parameter value, meaning that it correctly
  estimates the parameter ``on average''. The CRLB gives us the lower
  variance bound for such unbiased estimators, but it does not provide
  information about biased estimators.
\item
  \textbf{Asymptotic Efficiency}: Many maximum likelihood estimators
  (MLEs) are known to be \textbf{asymptotically efficient}, meaning that
  as the sample size grows to infinity, the variance of the MLE
  approaches the CRLB. This property is one of the reasons why MLE is
  such a popular and powerful method for parameter estimation.
\end{itemize}

\hypertarget{practical-implications}{%
\section{5. Practical Implications}\label{practical-implications}}

\begin{itemize}
\item
  \textbf{Model Evaluation}: In practical applications, the CRLB helps
  in evaluating the effectiveness of different models and estimation
  techniques. By comparing the variance of an estimator to the CRLB,
  statisticians can determine whether their chosen estimator is nearly
  optimal or whether alternative methods might yield more accurate
  estimates.
\item
  \textbf{Trade-off Between Bias and Variance}: Although the CRLB is
  useful for unbiased estimators, many real-world problems involve
  estimators that are biased but have a lower overall \textbf{mean
  squared error (MSE)}. The \textbf{bias-variance trade-off} often leads
  to the choice of biased estimators that offer lower MSE compared to
  strictly unbiased ones.
\end{itemize}

\hypertarget{examples}{%
\section{Examples}\label{examples}}

\begin{itemize}
\item
  \textbf{Normal Distribution Example}: For a normal distribution with
  mean \(\mu\) and variance \(\sigma^2\), if \(\sigma^2\) is known, the
  CRLB for estimating \(\mu\) is:

  \[
  \text{Var}(\hat{\mu}) \geq \frac{\sigma^2}{n}
  \]

  where \(n\) is the sample size. In this case, the sample mean
  \(\hat{\mu}\) is an efficient estimator, as it achieves this bound.
\item
  \textbf{Poisson Distribution Example}: For a Poisson-distributed
  random variable with parameter \(\lambda\), the CRLB for estimating
  \(\lambda\) is:

  \[
  \text{Var}(\hat{\lambda}) \geq \frac{\lambda}{n}
  \]

  where \(n\) is the number of observations. Here, the sample mean of
  the Poisson-distributed data is an efficient estimator for
  \(\lambda\).
\end{itemize}

The \textbf{Cramer-Rao lower bound} plays a critical role in statistical
estimation by providing a \textbf{lower limit on the variance} of any
unbiased estimator of a parameter. It serves as a benchmark for
determining the \textbf{efficiency} of an estimator, with efficient
estimators achieving the CRLB and thus having the minimum possible
variance among unbiased estimators. The CRLB also highlights the
relationship between Fisher Information and the precision of parameter
estimates, with larger Fisher Information indicating a tighter bound and
more accurate estimation.

Understanding the CRLB helps in assessing estimator quality, choosing
appropriate estimation techniques, and determining whether an estimator
is optimal in the sense of having the smallest possible variance. It is
a fundamental concept in statistical theory that aids in ensuring the
precision and reliability of parameter estimates in practice.

\newpage

\hypertarget{estimating-parameters-with-mle}{%
\section{Estimating Parameters with
MLE}\label{estimating-parameters-with-mle}}

In this section, we will demonstrate how to apply MLE to estimate
parameters for a simple model. We'll use both a theoretical example and
a data-driven case study.

\hypertarget{example-estimating-the-mean-of-a-normal-distribution}{%
\subsection{Example: Estimating the Mean of a Normal
Distribution}\label{example-estimating-the-mean-of-a-normal-distribution}}

Given a dataset of independent and identically distributed (i.i.d.)
observations \(X_1, X_2, \dots, X_n\) from a normal distribution with
unknown mean \(\mu\) and variance \(\sigma^2\), the MLE for \(\mu\) is
the sample mean.

\hypertarget{mle-for-the-mean-of-a-normal-distribution}{%
\subsubsection{MLE for the Mean of a Normal
Distribution}\label{mle-for-the-mean-of-a-normal-distribution}}

\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} X_i
\] This estimator is obtained by maximizing the log-likelihood for the
normal distribution.

\newpage

\hypertarget{properties-of-mle}{%
\section{Properties of MLE}\label{properties-of-mle}}

MLE estimators have desirable properties under certain regularity
conditions: - \textbf{Consistency}: As the sample size increases, the
MLE converges to the true parameter value. - \textbf{Asymptotic
Normality}: The distribution of the MLE approaches a normal distribution
as the sample size grows large. - \textbf{Efficiency}: MLE is often
asymptotically efficient, meaning it achieves the lowest possible
variance among unbiased estimators.

\hypertarget{challenges-and-limitations-of-mle}{%
\section{Challenges and Limitations of
MLE}\label{challenges-and-limitations-of-mle}}

While MLE is a powerful method, it comes with its own set of challenges:
- \textbf{Complexity of Maximization}: In some cases, the likelihood
function may not have a closed-form solution, requiring numerical
optimization. - \textbf{Non-Identifiability}: The parameters may not be
identifiable if the likelihood function does not contain enough
information to estimate the parameters uniquely. - \textbf{Overfitting}:
MLE can overfit the data if the model is too complex relative to the
available data.

\hypertarget{gpt-dump-conclusion}{%
\section{GPT Dump Conclusion}\label{gpt-dump-conclusion}}

In this chapter, we introduced the principles of Maximum Likelihood
Estimation, a widely used method for parameter estimation in statistical
models. We discussed the likelihood function, the MLE process, its
properties, and common challenges associated with MLE. In the next
chapter, we will explore Bayesian Estimation and compare it with MLE.

\newpage

\hypertarget{claude-dump}{%
\section{Claude Dump}\label{claude-dump}}

\hypertarget{maximum-likelihood-estimation-1}{%
\subsection{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation-1}}

The Maximum Likelihood Estimation (MLE) stands as one of the most
fundamental and powerful methods in statistical inference and machine
learning. This chapter explores how MLE provides a systematic approach
to parameter estimation by finding values that maximize the likelihood
of observing the given data.

\hypertarget{historical-context-and-development}{%
\section{Historical Context and
Development}\label{historical-context-and-development}}

The concept of maximum likelihood was first developed by R.A. Fisher in
the 1920s, though similar ideas can be traced back to Carl Friedrich
Gauss and Pierre-Simon Laplace. Fisher's revolutionary paper ``On the
Mathematical Foundations of Theoretical Statistics'' (1922) formally
introduced MLE, establishing it as a cornerstone of modern statistical
theory.

\hypertarget{theoretical-foundation}{%
\section{Theoretical Foundation}\label{theoretical-foundation}}

\hypertarget{the-likelihood-function}{%
\subsection{The Likelihood Function}\label{the-likelihood-function}}

The likelihood function represents the probability of observing the data
given a set of parameters. For a dataset \(X = \{x_1, ..., x_n\}\) and
parameters \(\theta\), the likelihood function is:

\[L(\theta|X) = P(X|\theta) = \prod_{i=1}^n P(x_i|\theta)\]

\hypertarget{log-likelihood}{%
\subsection{Log-Likelihood}\label{log-likelihood}}

We often work with the log-likelihood for computational convenience:

\[\ell(\theta|X) = \log L(\theta|X) = \sum_{i=1}^n \log P(x_i|\theta)\]

\hypertarget{the-maximum-likelihood-principle}{%
\section{The Maximum Likelihood
Principle}\label{the-maximum-likelihood-principle}}

\begin{a_def_eq}{Maximum Likelihood Estimator}{mle-principle}
The Maximum Likelihood Estimator $\hat{\theta}_{MLE}$ is defined as:

$$\underset{\theta}{\mathop{\mathrm{arg\,max}}} \prod_{i=1}^n P(x_i | \theta)$$

$$\underset{\theta}{\mathop{\mathrm{arg\,max}}}\; L(\theta|X) = \underset{\theta}{\mathop{\mathrm{arg\,max}}} \sum_{i=1}^n \log P(x_i|\theta)$$
\end{a_def_eq}

\hypertarget{properties-of-mle-1}{%
\section{Properties of MLE}\label{properties-of-mle-1}}

\hypertarget{consistency}{%
\subsection{Consistency}\label{consistency}}

Under regular conditions, MLE estimates converge to the true parameter
values as sample size increases.

\hypertarget{asymptotic-normality}{%
\subsection{Asymptotic Normality}\label{asymptotic-normality}}

For large samples, the MLE is approximately normally distributed around
the true parameter value.

\hypertarget{efficiency}{%
\subsection{Efficiency}\label{efficiency}}

MLE achieves the Cramér-Rao lower bound asymptotically, making it an
efficient estimator.

\hypertarget{applications-in-machine-learning-3}{%
\section{Applications in Machine
Learning}\label{applications-in-machine-learning-3}}

\hypertarget{gaussian-distribution-example}{%
\subsection{Gaussian Distribution
Example}\label{gaussian-distribution-example}}

\begin{a_exmp}{MLE for Gaussian Distribution}{gaussian-mle}
For a normal distribution $N(\mu, \sigma^2)$, the MLEs are:

$$\hat{\mu}_{MLE} = \frac{1}{n}\sum_{i=1}^n x_i$$
$$\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu})^2$$
\end{a_exmp}

\hypertarget{linear-regression-connection}{%
\subsection{Linear Regression
Connection}\label{linear-regression-connection}}

\begin{a_lemma}{MLE in Linear Regression}{linear-reg-mle}
The MLE for linear regression parameters under Gaussian noise assumptions coincides with the least squares solution.
\end{a_lemma}

When analytical solutions aren't available, numerical methods like
gradient descent or Newton-Raphson are used:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mle\_optimization(data, initial\_params):}
    \KeywordTok{def}\NormalTok{ negative\_log\_likelihood(params):}
        \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(log\_probability(data, params))}
    
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimize.minimize(negative\_log\_likelihood, }
\NormalTok{                             initial\_params,}
\NormalTok{                             method}\OperatorTok{=}\StringTok{\textquotesingle{}BFGS\textquotesingle{}}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ result.x}
\end{Highlighting}
\end{Shaded}

\normalsize

\newpage

\hypertarget{example---npdf}{%
\section{Example - Npdf}\label{example---npdf}}

This section demonstrates the application of Maximum Likelihood
Estimation (MLE) to the Normal Probability Density Function (Npdf), one
of the most widely used distributions in statistics. The goal is to
estimate the parameters of the normal distribution-the mean ( \(\mu\) )
and variance \(\left(\sigma^2\right)\)-directly from observed data. By
applying MLE, we derive parameter estimates that maximize the likelihood
of the observed data under the assumption that it follows a normal
distribution.

The normal distribution, defined by its bell-shaped curve, is
characterized by two parameters: the mean, which determines the
distribution's center, and the variance, which controls its spread.
Using MLE, we systematically derive these parameters by constructing the
likelihood function for the normal distribution and then simplifying it
using logarithmic transformations. This process highlights the power of
MLE in handling complex probabilistic models, especially through the use
of log-likelihoods for easier computation.

This example serves as a concrete illustration of how MLE works in
practice, focusing on the normal distribution as a specific case. By
working through the derivation step by step, we demonstrate how MLE
transforms the abstract concept of likelihood maximization into a
practical tool for estimating key statistical parameters. The final
results provide closed-form solutions for the mean and variance,
showcasing the elegance and utility of MLE in parameter estimation for
real-world data.

Identities

Recall we can expand the product by using use some simple ``nifty'\,'
identities for the log function and sum of exponents:

\[
\begin{aligned}
\prod_{i=1}^{n} p^{x_{i}}&=p^{\sum x_{i}} \\
\log \left(\prod_{i=1}^{n} p^{x_{i}}\right)&=\sum_{i=1}^{n} x_{i} \log (p)\\ 
\end{aligned}
\]

\[
\begin{aligned}
\log(a \times b) &= \log(a) + \log(b)\\
\prod_{i=1}^{n} p^{x_{i}}&=p^{\sum x_{i}} \\
\log \left(p^{a}\right)&=a \log (p) \\
\log \left(p^{a+b}\right)&=a \log (p)+b \log (p) \\
\log \left(\prod_{i=1}^{n} p^{x_{i}}\right)&=\sum_{i=1}^{n} x_{i} \log (p)\\ 
\log (e) &= 1
\end{aligned}
\]

Taking the log of the likelihood function (for ease of \(1^{st}\) order
derivative minimization):

\begin{equation}
\log ( \prod_{i=1}^{n} f_{X}(x ; \mu, \sigma))
\end{equation}

\[
\log ( \prod_{i=1}^{n} f_{X}(x ; \mu, \sigma))
\]

\begin{align}
=\log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\left(x_{1}-\mu\right)^{2} / 2 \sigma^{2}} \times \ldots \times \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\left(x_{n}-\mu\right)^{2} / 2 \sigma^{2}}\right)
\end{align}

Now note specifically for \(x_{1}\): \begin{align}
\log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} \times e^{-\left(x_{1}-\mu\right)^{2} / 2 \sigma^{2}}\right)
\end{align}

\begin{align}
=\overbrace{\log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right)+ \log \left(e^{-\left(x_{1}-\mu\right)^{2} / 2 \sigma^{2}}\right)}^{\log(a \times b) = \log(a) + \log(b}
\end{align}

\begin{align}
=\log \left[\left(2 \pi \sigma^{2}\right)^{-1 / 2}\right] - \overbrace{ \frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}} \log (e)}^{\log \left(p^{a}\right) =a \log (p)}
\end{align}

\[
 = -\frac{1}{2} \log \left(2 \pi \sigma^{2}\right) -\frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}} = -\frac{1}{2} \log (2 \pi)-\frac{1}{2} \log \left(\sigma^{2}\right)-\frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}}
\] \[
= -\frac{1}{2} \log (2 \pi)-\log (\sigma)-\frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}}
\]

Which gives us a easier to handle MLE objective function for
differentiation:

By summing over all n samples for \(x_{i}\):

\[
\log \left[L\left(\mu, \sigma \mid x_{1}, \ldots, x_{n}\right)\right]
\]

\[
\begin{gathered}
=-\frac{1}{2} \log (2 \pi)-\log (\sigma)-\frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}}
-\ldots-\frac{1}{2} \log (2 \pi)-\log (\sigma)-\frac{\left(x_{n}-\mu\right)^{2}}{2 \sigma^{2}}
\end{gathered}
\]

\[
=-\frac{n}{2} \log (2 \pi)-n \log (\sigma)-\frac{\left(x_{1}-\mu\right)^{2}}{2 \sigma^{2}}-\ldots-\frac{\left(x_{n}-\mu\right)^{2}}{2 \sigma^{2}}
\]

Both the mean and the variance are UNKNOWN - and we want to estimate
them on the basis of these observations ! We want to determine the
proper \(\mu\), and \$ v\$ based on the data.

Note that the term \(x\) is a `vector'

\$\text {We seek to minimize this expression: } \$

\[
-\frac{n}{2} \log (2 \pi)- n \log \sigma -\sum_{i=1}^{n} \frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}
\]

\(\text {We minimize w.r.t. } \mu \text {, }\)

to obtain the estimate of the mean:

\[
\frac{\partial}{\partial \mu} \log \left[L\left(\mu, \sigma \mid x_{1}, \ldots, x_{n}\right)\right] = 0
\]

\[
\frac{\partial \left( \frac{n}{2} \log (2 \pi) + {n} \log \sigma+\sum_{i=1}^{n} \frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}\right)}{\partial \mu}= 0
\] \[
=0-0+\frac{\left(x_{1}-\mu\right)}{\sigma^{2}}+\ldots+\frac{\left(x_{n}-\mu\right)}{\sigma^{2}}
\]

\[
=\frac{1}{\sigma^{2}}\left[\left(x_{1}+\ldots+x_{n}\right)-n \mu\right]=0
\]

\[
\frac{1}{\sigma^{2}} \sum_{i=1}^{n} \left(x_{i}-\mu\right)=0 ~~~~ \Rightarrow ~~~~ \sum_{i=1}^{n}  x_{i}=n \mu ~~~~ \Rightarrow ~~~~ \widehat{\mu}=\frac{x_{1}+\cdots+x_{n}}{n}
\]

\(\text {We minimize w.r.t. } \sigma \text {, }\) to obtain the estimate
of the variance:

\[
\frac{\partial}{\partial \sigma} \log \left[L\left(\mu, \sigma \mid x_{1}, \ldots, x_{n}\right)\right] = 0
\]

\[
\frac{\partial \left( \frac{n}{2} \log (2 \pi) + {n} \log \sigma+\sum_{i=1}^{n} \frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma}\right)}{\partial \sigma}= 0
\]

\[
=0-\frac{n}{\sigma} -\sum_{i=1}^{n}\frac{\left(x_{i}-\mu\right)^{2}}{2}(-2) \sigma^{-3} = 0
\]

\[
\frac{n}{\sigma}+\sum_{i=1}^{n}\frac{\left(x_{1}-\mu\right)^{2}}{\sigma^{3}} = 0 
\]

\[
\begin{aligned}
&0=-n+\frac{1}{\sigma^{2}}\left[\left(x_{1}-\mu\right)^{2}+\ldots+\left(x_{n}-\mu\right)^{2}\right] \\
&n=\frac{1}{\sigma^{2}}\left[\left(x_{1}-\mu\right)^{2}+\ldots+\left(x_{n}-\mu\right)^{2}\right] \\
&n \sigma^{2}=\left(x_{1}-\mu\right)^{2}+\ldots+\left(x_{n}-\mu\right)^{2}
\end{aligned}
\]

\[
\sigma^{2} \cdot n -\sum_{i=1}^{n} \left(x_{i}-\mu\right)^{2}=0 ~~~ \Rightarrow ~~~ \widehat{\sigma}=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-\widehat{\mu}\right)^{2}}
\]

\[
\widehat{\sigma}=\sqrt{\frac{\left(x_{1}-\mu\right)^{2}+\ldots+\left(x_{n}-\mu\right)^{2}}{n}}
\]

Thus maximum of the MLE \(L\left(\Theta \mid X \right)\), or in this
case: \(\log \left(L\left(\Theta \mid X \right)\right)\), for

\[
\quad f_{X}(x ; \mu, \sigma)= \frac{1}{\sqrt{2 \pi \sigma}} \exp \left\{-\frac{\left(x-\mu\right)^{2}}{2 \sigma}\right\}
\] results in the estimate for the mean \(\mu\) and the standard
deviation \(\sigma\).\textbackslash{}

\[
\Rightarrow ~~~~ \widehat{\mu}=\frac{x_{1}+\cdots+x_{n}}{n}
\]

\[
\Rightarrow ~~~~ \widehat{\sigma}=\sqrt{\frac{\left(x_{1}-\mu\right)^{2}+\ldots+\left(x_{n}-\mu\right)^{2}}{n}}
\]

\newpage

\hypertarget{example---wpdf}{%
\section{Example - Wpdf}\label{example---wpdf}}

In this section, we apply the Maximum Likelihood Estimation (MLE) method
to the Weibull Probability Density Function (Wpdf), a widely used
distribution in reliability analysis and survival studies. Unlike the
normal distribution, the Weibull distribution is a non-symmetric,
two-parameter model characterized by its shape parameter \(k\) (or
\(\gamma\) ) and scale parameter \(\lambda\) (or \(\theta\) ). Despite
these differences, the general approach to MLE derivation remains the
same, demonstrating the versatility of MLE as a parameter estimation
technique.

The Weibull distribution differs from the normal distribution in that it
can model a variety of behaviors depending on the shape parameter \(k\),
making it especially useful for modeling failure times and lifespans.
The two parameters \(k\) and \(\lambda\) determine the shape and scale
of the distribution, respectively, and must be estimated from observed
data. Using MLE, we construct the likelihood function for the Weibull
distribution based on the assumption that the data are independent and
identically distributed (i.i.d.).

While the MLE derivation for the Weibull distribution follows the same
principles as the normal distribution, the asymmetry and mathematical
complexity of the Weibull model make the process more challenging.
Specifically, the lack of a closed-form analytical solution for the
shape parameter \(\hat{\gamma}\) requires us to rely on numerical
methods or iterative techniques for estimation. Once \(\hat{\gamma}\) is
estimated, the scale parameter \(\hat{\theta}\) can be determined more
directly.

This section provides a step-by-step derivation of the MLE equations for
the Weibull distribution, highlighting the mathematical tools required
to solve for the parameters. By the end, we will have a framework for
estimating the shape and scale parameters of the Weibull distribution,
demonstrating how the MLE approach extends seamlessly beyond symmetric
distributions like the normal to more complex, asymmetric models.

We start by assuming Weibull pdf that is \textbf{Independent Identically
Distributed} which is usually designated as i.i.d. (or perhaps Markov
depending on how you define or scope the analysis):

\[
X = \left\{x_{1}, \ldots, x_{n}\right\}, \quad W{_{pdf}}(k, \lambda)
\]

\[
f(x ; \lambda, k)= \begin{cases}\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1} e^{-(x / \lambda)^{k}}, & x \geq 0 \\ 0, & x<0\end{cases}
\]

\[
L\left(\Theta \mid X \right)= L\left(x_{1}, \cdots x_{n} ; \gamma, \theta\right)=\prod_{i=1}^{n}(\gamma / \theta) x_{i}^{\gamma-1} \exp \left(-x_{i}^{\gamma} / \theta\right)
\]

Consider the maximum of
\(\log \left(L\left(\Theta \mid X \right)\right)\):\textbackslash{}

\[
\frac{\partial (\log L\left(\Theta \mid X \right))}{\partial \gamma}=0 
\quad and \quad
\frac{\partial (\log L\left(\Theta \mid X \right))}{\partial \theta}=0 
\quad and \quad 
\]

Solving the first order differentials:

\[
\frac{\partial \ln L}{\partial \gamma}=\frac{n}{\gamma}+\sum_{1}^{n} \ln x_{i}-\frac{1}{\theta} \sum_{1}^{n} x_{i}^{\gamma} \ln x_{i}=0
\]

\[
\frac{\partial \ln L}{\partial \theta}=-\frac{n}{\theta}+\frac{1}{\theta^{2}} \sum_{1}^{n} x_{i}^{\gamma} \quad=0
\]

On eliminating \(\theta\) between these two equations and simplifying,
we have \[
\left[\frac{\sum_{1}^{n} x_{i}^{\gamma} \ln x_{i}}{\sum_{1}^{n} x_{i}^{\gamma}}-\frac{1}{\gamma}\right]=\frac{1}{n} \sum_{1}^{n} \ln x_{i},
\]

\noindent We have to now solve the MLE for the best estimate of
\(\hat{\gamma}\). There is no analytic closed solution here for
\(\hat{\gamma}\), unfortunately. However, this can be solved by
iteration, or trial and error. Once two values \(\gamma_{1}\) and
\(\gamma_{2}\) have been found in an interval such that
\(\gamma_{1}<\gamma<\gamma_{2}\), end with a linear interpolation for
\(\hat{\gamma}\).

\textbf{Trial and Error Approach}

Step-by-Step Algorithm for Trial-and-Error Search to Estimate
\(\hat{\gamma}\) Since there is no closed-form solution for the shape
parameter \(\hat{\gamma}\) in the Weibull distribution MLE, we can use a
trial-and-error search algorithm to estimate it. This involves
iteratively testing values of \(\gamma\) until we find the one that
satisfies the MLE equation with sufficient accuracy. Below is an
algorithmic approach to perform this search, step by step:

Step 1: Define the MLE Residual Function

Rearrange the MLE equation for \(\gamma\) into a residual function that
must evaluate to zero:

\[
\operatorname{Residual}(\gamma)=\left[\frac{\sum_{i=1}^n x_i^\gamma \ln x_i}{\sum_{i=1}^n x_i^\gamma}-\frac{1}{\gamma}\right]-\frac{1}{n} \sum_{i=1}^n \ln x_i
\]

The goal is to find the \(\gamma\) that makes
\(\operatorname{Residual}(\gamma) \approx 0\).

Step 2: Set an Initial Search Range

\begin{itemize}
\tightlist
\item
  Choose an initial search interval \(\left[\gamma_1, \gamma_2\right]\)
  based on prior knowledge of the Weibull distribution or the data. For
  example:
\item
  Start with \(\gamma_1=0.5\) (a reasonable lower bound for the shape
  parameter).
\item
  Choose \(\gamma_2=5.0\) (a reasonable upper bound for most practical
  applications).
\end{itemize}

Step 3: Evaluate the Residual Function at the Bounds

\begin{itemize}
\tightlist
\item
  Compute Residual \(\left(\gamma_1\right)\) and
  \(\operatorname{Residual}\left(\gamma_2\right)\) :
\item
  If
  \(\operatorname{Residual}\left(\gamma_1\right) \cdot \operatorname{Residual}\left(\gamma_2\right)>0\),
  the root may not lie in this interval. Adjust the bounds and repeat.
\item
  If
  \(\operatorname{Residual}\left(\gamma_1\right) \cdot \operatorname{Residual}\left(\gamma_2\right)<0\),
  a root exists in the interval \(\left[\gamma_1, \gamma_2\right]\), and
  you can proceed.
\end{itemize}

Step 4: Use Bisection or Iterative Refinement

If a root exists in the interval \(\left[\gamma_1, \gamma_2\right]\),
refine the estimate of \(\gamma\) iteratively:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the Midpoint:
\end{enumerate}

\[
\gamma_{\text {mid }}=\frac{\gamma_1+\gamma_2}{2} .
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Evaluate the Residual at the Midpoint: Residual
  \(\left(\gamma_{\text {mid }}\right)\).
\item
  Update the Interval Based on the Sign:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  If Residual
  \(\left(\gamma_{\text {mid }}\right) \cdot \operatorname{Residual}\left(\gamma_1\right)<0\),
  the root lies in \(\left[\gamma_1, \gamma_{\text {mid }}\right]\). Set
  \(\gamma_2=\) \(\gamma_{\text {mid }}\).
\item
  If
  \(\operatorname{Residual}\left(\gamma_{\text {mid }}\right) \cdot \operatorname{Residual}\left(\gamma_2\right)<0\),
  the root lies in \(\left[\gamma_{\text {mid }}, \gamma_2\right]\). Set
  \(\gamma_1=\) \(\gamma_{\text {mid }}\).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Repeat steps \(1-3\) until the interval
  \(\left[\gamma_1, \gamma_2\right]\) is sufficiently small (i.e.,
  \(\left|\gamma_2-\gamma_1\right|<\) tolerance).
\end{enumerate}

Step 5: Perform Linear Interpolation (Optional)

Once the interval \(\left[\gamma_1, \gamma_2\right]\) is sufficiently
small, you can use linear interpolation to estimate \(\hat{\gamma}\)
more precisely:

\[
\hat{\gamma}=\gamma_1-\frac{\operatorname{Residual}\left(\gamma_1\right)}{\operatorname{Residual}\left(\gamma_2\right)-\operatorname{Residual}\left(\gamma_1\right)} \cdot\left(\gamma_2-\gamma_1\right)
\]

Step 6: Verify the Solution

\begin{itemize}
\tightlist
\item
  Evaluate Residual \((\hat{\gamma})\) to ensure it is sufficiently
  close to zero (e.g.,
  \(|\operatorname{Residual}(\hat{\gamma})|<10^{-6}\) ).
\item
  If the residual is not small enough, adjust the initial bounds or
  tolerance and repeat the process.
\end{itemize}

Step 7: Estimate the Scale Parameter \(\hat{\theta}\)

Once \(\hat{\gamma}\) is determined, compute the scale parameter
\(\hat{\theta}\) using:

\[
\hat{\theta}=\frac{1}{n} \sum_{i=1}^n x_i^{\hat{\gamma}} .
\]

Notes

\begin{itemize}
\tightlist
\item
  Bisection Method: The method converges reliably but may be slower than
  other rootfinding techniques like Newton-Raphson.
\item
  Newton-Raphson: While faster, it requires the derivative of the
  residual function and a good initial guess, so it may not always be
  practical for this problem.
\item
  Tolerance: Choose a small tolerance, such as \(10^{-6}\), to ensure
  accurate estimates.
\end{itemize}

This algorithm provides a robust framework for estimating
\(\hat{\gamma}\) through trial and error, leveraging numerical methods
to handle the absence of a closed-form solution.

With \(\hat{\gamma}\) thus determined, \(\theta\) is estimated as

\[
\hat{\theta}=\sum_{1}^{n} x_{i}^{\hat{\gamma}} / n
\] \noindent Interestingly enough - this simply looks like the average
of the \(x_{i}\) at some power \(\hat{\gamma}\).

\newpage

\hypertarget{example---bernoulli-distribution}{%
\section{Example - Bernoulli
Distribution}\label{example---bernoulli-distribution}}

Coin tosses are best modeled with a Bernoulli distribution. The common
formulation used for the Frequentist approach is via the Probability
Mass Function (PMF) formulation. A very simple case of this formulation
is as follows;

\[
f(x ; p)=p^{x}(1-p)^{1-x}.
\]

Where, \(x\) is a `random variable' that represents an observation of a
coin toss (here we assume \(x=1\) for \textit{Heads} and \(x=0\) for
\textit{Tails}). Here \(p\) is a parameter referred to as the
`probability' of \textit{Heads}. We will refer to the set of all
possible `parameters' as \(\Theta\) onward. This function represents how
probable each value of x is according to the particular distribution law
we have chosen. Note the simple properties of this PMF:

\begin{align}
x=1 \; & \rightarrow \; f(1; p) = p\\
x=0 \; & \rightarrow \; f(0; p) = 1-p 
\end{align}

What is important to understand here is that \(x\) is a discrete outcome
of the toss, either \(1\) or \(0\), or specifically
\(x \in \left\{1, 0\right\}\) as a set of only two possible outcomes.
Carefully noting this is NOT the analog range
\(0 \leq x \leq 1\).\textbackslash{}

We can determine or `regress' so to speak the parameter \(p\) from the
data. In order to do this - lets consider a Frequentist approach
employing a Maximum Likelihood Estimation (MLE) procedure. This method
takes approach of maximizing likelihood of \(m\) potential parameters
\(\left\{p_{1}, \ldots, p_{m}\right\}\) given the data set
\(\left\{x_{1}, \ldots, x_{n}\right\}\) of size \(n\). \(\mathcal{L}\)
represents the likelihood:

\begin{align}
\mathcal{L}(\Theta ; D) & =f\left(x_{1}, \ldots, x_{n} \mid \Theta\right)\\
D & =\left\{x_{1}, \ldots, x_{n}\right\}\\
\Theta & =\left\{p_{1}, \ldots, p_{m}\right\}
\end{align}

Be careful to note that there are \(n\) data points, and a model with
\(m\) parameters. \textbackslash{}

But substitution using the definition of the PME the expression now
becomes:

\[
\mathcal{L}(\Theta ; D)=\prod_{i=1}^{n} f\left(x_{i} ; p\right)=\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}}=p^{\sum x_{i}}(1-p)^{1-\sum x_{i}}
\]

Of note here is that we are leveraging the Markov Chain principle - that
the sequential coin flips are NOT auto-correlated!

\[
\prod_{i=1}^{n} f\left(x_{i} ; p\right)=\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}}
\]

This is a multiplicative sum, or multiplying the of each event to each
subsequent event. This is important to note and understand. Markov means
here that you can just chain the events together in a probability
multiplicative sum.

And this can be evaluated for a local maximum by using the first
derivative:

\[
\frac{\partial (\mathcal{L}(\Theta ; D))}{\partial \Theta}=0
\]

Ok - this is the really hard to solve - no?

\[
\frac{\partial (\mathcal{L}(\Theta ; D))}{\partial \Theta}=
\frac{\partial (\prod_{i=1}^{n} f\left(x_{i} ; p\right))}{\partial p}=
\frac{\partial (\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}})}{\partial p}=0
\]

How do you approach this? \ldots{} ok - lets go do a quick trick here!

Take a side step\ldots{} what do we know about first derivatives, and
the \(\text{log}\) of first derivatives ?

Take a generic polynomial first derivative to determine it's maximum
point:

\[
\frac{\partial\left(x^{2}+5\right)}{\partial x}=0
\]

The maximum is at:

\[
2 x=0
\]\\

Now take the log(f(x)), and look at ti's first derivative to determine
the maximum:

\[
\frac{\partial \log \left(x^{2}+5\right)}{\partial x}=0
\]

The maximum is at:

\[
\frac{2 x}{x^{2}+5}=0
\]

with this solution \$x=0 \$, being the same.

Interesting - we can use this!!

Now, strictly speaking, you have to be careful that the function is
indeed defined for \(f(x;p)\) and \(\log (f(x;p))\) at the maximum of
interest otherwise things get a bit weird. But in our case it works.
Why? Basically because \(p \in [0,1]\), and the assumption is the PMF is
smooth and not discontinuous in this range of \(p\). Carefully noting
this IS the analog range \(0 \leq p \leq 1\).

So, with this in mind - we can leverage this idea in form of the MLE
first derivative expression:

\[
\frac{\partial \log (\mathcal{L}(\Theta ; D))}{\partial \Theta}=0
\]

Working with:

\[
\mathcal{L}(\Theta ; D)=p^{\sum x_{i}}(1-p)^{1-\sum x_{i}}
\]

We can expand this to the following:

\[
\log (\mathcal{L}(\Theta ; D))
=\sum x_{i} \log (p)+\left(n-\sum x_{i}\right) \log (1-p)
\]

using some simple identities for the log function and sum of exponents:
\[
\begin{aligned}
\prod_{i=1}^{n} p^{x_{i}}&=p^{\sum x_{i}} \\
\log \left(p^{a}\right)&=a \log (p) \\
\log \left(p^{a+b}\right)&=a \log (p)+b \log (p) \\
\log \left(\prod_{i=1}^{n} p^{x_{i}}\right)&=\sum_{i=1}^{n} x_{i} \log (p) 
\end{aligned}
\]

Now we can more easily handle this first order derivative of the MLE
expression \[
\frac{\partial (\mathcal{L}(\Theta ; D))}{\partial \Theta}=0
\]

From above: \[
\frac{\partial (\mathcal{L}(\Theta ; D))}{\partial \Theta}=
\frac{\partial (p^{\sum x_{i}}(1-p)^{1-\sum x_{i}})}{\partial p}=0
\]

This becomes, by application of the log:

\[
\frac{\partial \log (\mathcal{L}(\Theta ; D))}{\partial p}=\frac{\partial\left(\sum x_{i} \log (p)+\left(n-\sum x_{i}\right) \log (1-p)\right)}{\partial p}=
\frac{\partial \sum x_{i} \log (p)}{\partial p}+\frac{\left.\partial\left(n-\sum x_{i}\right) \log (1-p)\right)}{\partial p}
\]

Note that there is not dependency of \(p\) on \(x_{i}\) or \(n\).
Meaning - that the model parameter \(p\), does not depend on the
individual \(x_{i}\) since we are looking for the maximum of the set of
\(x_{i}\)s (or \(D =\left\{x_{1}, \ldots, x_{n}\right\}\) the whole
set). This is not so easy to see - think about it carefully. Thus;

\[
\frac{\left.\partial\left(n-\sum x_{i}\right) \log (1-p)\right)}{\partial p} = \left(n-\sum x_{i}\right) \frac{\partial \log (1-p))}{\partial p}
\]

Which is now easier to deal with and solve.

\[
\frac{\partial \log (\mathcal{L}(\Theta ; D))}{\partial p}=
\sum x_{i} \frac{\partial \log (p)}{\partial p}+\left(n-\sum x_{i}\right) \frac{\partial \log (1-p))}{\partial p}=
\frac{\sum x_{i}}{p}-\frac{n-\sum x_{i}}{1-p}
\]

But wait how did you do that?

Note the following Chain Rule differential identity:

\[
\frac{d(f(g(p)))}{d p}=\frac{d f}{d g} \frac{d g}{d p} = \frac{-1}{1-p}
\]

With the idea here that \(f(p) = \log (g(p))\) and \(g(p) = (1-p)\).

So lets solve for the maximum: \[
\frac{\partial \log (\mathcal{L}(\Theta ; D))}{\partial p}=\frac{\sum x_{i}}{p}-\frac{n-\sum x_{i}}{1-p}=0
\]

Multiplying everything by \(p(1-p)\) and expanding parenthesis we get:

\[
\sum x_{i}(1-p)-\left(n-\sum x_{i}\right) p=0
\]

\[
\sum x_{i}-p \sum x_{i}-n p+p \sum x_{i}=0
\]

Canceling out the terms and rearranging:

\[
\sum x_{i}-n p=0
\]

\[
p=\frac{\sum x_{i}}{n}
\]

So, here is the derivation of our intuitive formula. The Bernoulli
distribution and its MLE estimate of the mean.

But what does this say? \ldots{} it is simple really \ldots{}

Take a coin flip it \(n\) times, and add up the results with either
\(x\) is either \(1\) or \(0\), and average this value over the total
number of flips \(n\). What do you get? The probability of the coin p
being heads or tails, which, if it is a fair coin, should be
\(p = 0.5\).

Crazy right?

The illustration is to show concepts, not solve something really
important.

\newpage

\newpage

\hypertarget{mle-section}{%
\section{MLE Section}\label{mle-section}}

\newpage

\hypertarget{mle-section-1}{%
\section{MLE Section}\label{mle-section-1}}

\newpage

\hypertarget{determining-mle-a-generalized-approach}{%
\section{Determining MLE: A Generalized
Approach}\label{determining-mle-a-generalized-approach}}

Maximum Likelihood Estimation (MLE) is one of the most widely used
methods in statistical estimation, offering a flexible and efficient way
to estimate parameters of a statistical model based on observed data. At
its core, MLE aims to find the set of parameters that maximize the
likelihood of the observed data under a given model. In the context of
this section, we will explore the general methodology for computing the
MLE, without assuming any specific form for the underlying probability
density function (PDF). Instead, this flowchart serves as a broad,
multidimensional illustration of the steps required to derive the MLE,
providing a systematic approach to parameter estimation applicable to a
wide range of statistical models.

\hypertarget{general-overview-of-the-mle-process}{%
\subsection{General Overview of the MLE
Process}\label{general-overview-of-the-mle-process}}

The flowchart presented here details the sequential steps involved in
the computation of the MLE for a given dataset. It is a generalized
workflow that does not assume a particular distribution or
parameterization for the data. Instead, it provides a robust framework
for identifying the parameters of an unknown model, ensuring that the
resulting MLE is well-defined and mathematically sound. By focusing on
the methodology of parameter estimation, the flowchart offers a clear
visualization of the steps involved in determining the maximum
likelihood estimates (MLE) that best describe the observed data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialization and Likelihood Calculation: The first step in this
  process is the specification of the likelihood function \(L(\Theta)\),
  where \(\Theta=\left[\theta_1, \theta_2, \ldots, \theta_m\right]\) is
  the vector of parameters to be estimated, and
  \(X=\left[x_1, x_2, \ldots, x_n\right]\) represents the observed data.
  For a given model, the likelihood function \(L(\Theta)\) is defined as
  the probability of observing the data given a particular set of
  parameter values. In this generalized framework, the likelihood
  function is assumed to take the form
  \(L(\Theta)=\prod_{i=1}^n P\left(x_i \mid \Theta\right)\), where
  \(P\left(x_i \mid \Theta\right)\) represents the conditional
  probability of the data point \(x_i\) given the parameter vector
  \(\Theta\).
\item
  Log-Likelihood and Gradient Calculation: Since the likelihood function
  is often complicated and involves large products of probabilities, it
  is common practice to work with the loglikelihood function,
  \(\ell(\Theta)=\ln L(\Theta)\). Taking the logarithm of the likelihood
  simplifies the expression and transforms products into sums, making it
  more tractable for optimization. The log-likelihood is given by
  \(\ell(\Theta)=\sum_{i=1}^n \ln P\left(x_i \mid \Theta\right)\). The
  next step involves computing the gradient of the log-likelihood with
  respect to the parameters, denoted as
  \(\nabla_{\Theta} \ell(\Theta)\). This gradient represents the
  first-order partial derivatives of the log-likelihood function with
  respect to each parameter \(\theta_k\), and it provides the direction
  in which the log-likelihood increases most rapidly.
\item
  Solving for the Local Maxima: The goal of MLE is to maximize the
  log-likelihood function. To find the parameter values that maximize
  \(\ell(\Theta)\), we set the gradient equal to zero, i.e.,
  \(\nabla_{\Theta} \ell(\Theta)=0\). This condition ensures that the
  log-likelihood function has a local maximum at the estimated parameter
  values. Solving this system of equations typically requires numerical
  methods, especially in high-dimensional settings, where an analytical
  solution may not be feasible.
\item
  Hessian Matrix and Curvature Evaluation: After finding the critical
  points where the gradient is zero, we next examine the nature of these
  critical points. This is accomplished by evaluating the second-order
  partial derivatives of the log-likelihood function, which form the
  Hessian matrix \(H(\Theta)\). The Hessian matrix is defined as
  \(H(\Theta)=\frac{\partial^2 \ell(\Theta)}{\partial \Theta^2}\), and
  it encapsulates information about the curvature of the log-likelihood
  surface. A negative-definite Hessian matrix indicates that the
  critical point corresponds to a local maximum. The negative
  definiteness of the Hessian can be verified by examining its
  eigenvalues. If all eigenvalues of the Hessian are negative, the
  critical point is indeed a maximum, and the corresponding parameter
  values are the MLE.
\item
  Eigenvalue Check and Final Validation: The final step in the process
  is to verify whether the Hessian matrix is negative-definite, which
  confirms that the solution corresponds to a local maximum. This can be
  done by checking if all the eigenvalues of the Hessian matrix are
  negative. In the flowchart, this is represented as checking the
  condition \(\mathbf{v}^{\top} H(\Theta) \mathbf{v}<0\) for all
  non-zero vectors \(\mathbf{v}\), or equivalently by checking whether
  the eigenvalues of \(H\) are negative (i.e.,
  \(H \mathbf{v}=\lambda \mathbf{v}\) with \(\lambda_i<0\) for all \(i\)
  ). If the Hessian is negative-definite, the local maximum is
  confirmed, and the MLE for the parameters is validated.
\item
  Iteration and Convergence: In some cases, especially in
  high-dimensional or complex problems, the optimization may require
  iterative steps to converge to the global maximum. The algorithm may
  re-seed the initial parameter estimates \(\Theta\) and attempt to find
  new local maxima until convergence is achieved. The algorithm halts
  once the MLE is determined, and the estimated parameters \(\Theta\)
  are printed as the final result.
\end{enumerate}

\begin{figure}[htbp]
\centering
\resizebox{0.95\textwidth}{!}
{
\begin{tikzpicture}
  [
        node distance = 8mm and 16mm,
        start chain = A going below,
    start chain = B going below,
        base/.style = 
            {
        draw, minimum width=32mm, minimum height=8mm,
        align=center, on chain=A
        },
        base2/.style = 
            {
        draw, minimum width=32mm, minimum height=8mm,
        align=center, on chain=B
        },
    startstop/.style = {base, rounded corners, fill=red!20},
    process/.style = {base, fill=yellow!30},
        io/.style = 
                {
                base, trapezium, trapezium left angle=70, 
                trapezium right angle=110, fill=blue!30
                },
        decision/.style = {base, diamond, fill=green!30},
        every edge quotes/.style = {auto=right},
        math_cmnt_head/.style={base2, rounded corners, fill=red!20},
        math_cmnt/.style={base2, fill=blue!20}
  ]

\node [startstop]           {MLE Evaluation};            % <-- A-1
\node [process]         {$P(X\!\! \mid\! \Theta)$};
\node [process]         {$L(\Theta)=\prod_{i=1}^n P\left(x_i\! \mid\! \Theta\right)$};
\node [process]         {$\ell(\Theta)=\ln L(\Theta)=\sum_{i=1}^n \ln P\left(x_i\! \mid\! \Theta\right)$};
\node [process]         {$\frac{\partial \ell(\Theta)}{\partial \theta_k}=\left[\nabla_{\Theta} \ell(\Theta)\right]_k$};
\node [process, yshift=-0.3cm]         {
                Solve for $\Theta$ such that:\\ \\$\nabla_{\Theta} \ell(\Theta)=0$ \\ \\
                via system  \{$\frac{\partial \ell(\Theta)}{\partial \theta_k}=0$\}; \\ \\
                finding values of: \\ 
                
                $\Theta =\left[\theta_1 \cdots \theta_k \cdots \theta_m\right]$
                };
\node [process]         {
                $H(\Theta)=\frac{\partial^2 \ell(\Theta)}{\partial \Theta^2} = 
                \left[\begin{array}{ccc}\frac{\partial^2 \ell(\Theta)}{\partial \theta_1^2} & 
                    \cdots & \frac{\partial^2 \ell(\Theta)}{\partial \theta_1 \partial \theta_k} \\ 
                \vdots & \ddots & \vdots \\ 
                \frac{\partial^2 \ell(\Theta)}{\partial \theta_k \partial \theta_1} &
                     \cdots & \frac{\partial^2 \ell(\Theta)}{\partial \theta_k^2}\end{array}\right]$
                };
\node [decision]         {H \\ negative \\ definite?};
\node [process]         {MLE Params $\Theta$ Defined and Validated};
\node [startstop]           {MLE Determined}; 
          % <-- A-6
%
\node [process,                             % <-- A-7
       left=of A-8]     {re-seed $\Theta$ \\ find new local maxima};

\node [math_cmnt_head, on chain=B, xshift=7cm] {Underlying key step \\ details and identities};

\node [math_cmnt, on chain=B, yshift=0.2cm ] {
  $ \begin{aligned}
    X&=\left[x_1, x_2, \ldots, x_n\right] \\
    \Theta &=\left[\theta_1, \theta_2, \ldots, \theta_m\right] \\
  \end{aligned}$
};

\node [math_cmnt, on chain = B, yshift=0.25cm] {
  $ \begin{aligned}
    \prod_{i=1}^{n} p^{x_{i}} &= p^{\sum x_{i}} \\
    \log \left(\prod_{i=1}^{n} p^{x_{i}}\right)&=\sum_{i=1}^{n} x_{i} \log (p)\\ 
  \end{aligned}$
};

\node [math_cmnt, on chain = B, yshift=-9.3cm] {
                $\mathbf{v}^{\top} H(\Theta) \mathbf{v}<0 \; $ for all $\; \mathbf{v} \neq 0$ \\ 
                -- or -- \\ 
                $H \mathbf{v}=\lambda \mathbf{v}$\\
                $\lambda_i<0$ for all $i=1,2, \ldots, k$\\ 
                (eigenvalues of $H$ are negative)
};


%%
\draw [arrows=-Stealth] 
    (A-1) edge["choose conditional pdf"]            (A-2)
    (A-2) edge["find likelihoood"]      (A-3)
    (A-3) edge["calculate log likelihoood"]     (A-4)
    (A-4) edge["calculate derivatives via gradient"]        (A-5)
    (A-5) edge node[pos=0.3, auto=right] {solve for local maxima} (A-6)
    (A-6) edge["calculate Hessian"]     (A-7)
    (A-7) edge["check if negative definite"]            (A-8)
    (A-8) edge["yes"]           (A-9)
    (A-8) edge["no"]            (A-11)
    (A-9) edge["parametric model found for pdf"]                        (A-10)
    (A-11) |- ($(A-5.south west)!0.5!([yshift=-3mm] A-6.north west)$) -| ([xshift=- 9mm] A-6.north);

\end{tikzpicture}
}
\caption{MLE Flow Chart}.
\label{fig:MLE_Flow-diagram}
\end{figure}

\newpage

This generalized flowchart for determining the MLE highlights the
flexibility and power of the maximum likelihood method. By not assuming
a specific form for the underlying PDF, it provides a versatile
framework that can be applied to a wide variety of statistical models.
Whether the data follows a normal distribution, a Poisson distribution,
or any other form, the steps outlined in this process can be adapted to
estimate the parameters of the model. The primary goal of this workflow
is to demonstrate the general steps involved in determining the MLE,
from likelihood calculation and gradient computation to the evaluation
of the Hessian matrix and verification of the local maximum.

Through these steps, one can confidently arrive at the parameter
estimates that best describe the data according to the principle of
maximum likelihood. The beauty of this approach lies in its generality,
making it an invaluable tool for statisticians, data scientists, and
anyone interested in understanding the underlying relationships in
complex datasets. By following the methodical procedure outlined here,
practitioners can ensure that the MLE provides a robust and accurate
representation of the data, laying the foundation for further analysis
and modeling.

\hypertarget{step-by-step-description-of-mle-worth-it}{%
\section{STEP By STEP Description of MLE (worth it
?)}\label{step-by-step-description-of-mle-worth-it}}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

\begin{center}
\colorbox{white}{\color{navyimpactblue} \huge $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon$}
\end{center}

\hfill\break

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-9-LR.jpeg}
    \caption*{\Large Line Fitting Straight Assumption}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Linear Regression}{def9} 

\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon
\end{align}


\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$y$] The \textbf{dependent variable}, representing the outcome or the target we aim to predict.
    \vspace{0.5\baselineskip}
    \item[$x_i$] The \textbf{independent variable(s)}, representing the features or predictors for the \( i \)-th observation.
    \vspace{0.5\baselineskip}
    \item[$\beta_0$] The \textbf{intercept}, representing the expected value of \( y \) when all predictors are zero.
    \vspace{0.5\baselineskip}
    \item[$\beta_i$] The \textbf{coefficient} for predictor \( x_i \), representing the change in the expected value of \( y \) for a one-unit change in \( x_i \), while holding all other variables constant.
    \vspace{0.5\baselineskip}
    \item[$\epsilon$] The \textbf{error term}, accounting for the variability in \( y \) that cannot be explained by the linear combination of predictors.
    \vspace{0.5\baselineskip}
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: Linear regression is a simple and widely used
method for predictive analysis.

\textbf{Description}: It models the relationship between a dependent
variable and one or more independent variables by fitting a linear
equation to observed data.

\textbf{Importance in ML}: Linear regression is fundamental for
understanding relationships between variables and is used in predictive
modeling to estimate outcomes.

\vspace*{\fill}

\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Linear regression is a fundamental statistical approach used to model
the relationship between a dependent variable and one or more
independent variables. The objective is to establish a linear
relationship between inputs (predictors) and an output variable that
allows for interpretation, prediction, and inference.

This chapter discusses Equation 9, which formulates the core concept of
linear regression. The method is commonly applied across various fields
such as finance, economics, medicine, and more.

\hypertarget{mathematical-formulation}{%
\section{Mathematical Formulation}\label{mathematical-formulation}}

The linear regression equation is represented by Equation 9. Here, the
model is built by finding the coefficients
\(\beta_0, \beta_1, \ldots, \beta_n\) that minimize the error term
\(\epsilon\).

The general approach for estimation in linear regression is based on
minimizing the residual sum of squares (RSS):

\[
RSS = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_n x_{in}))^2
\]

The goal is to determine values for the coefficients \(\beta_i\) that
minimize the \(RSS\), resulting in the best fit line through the
observed data.

\hypertarget{understanding-the-components}{%
\section{Understanding the
Components}\label{understanding-the-components}}

\begin{itemize}
\tightlist
\item
  \textbf{Dependent Variable (\(y\))}: The outcome that we are trying to
  predict.
\item
  \textbf{Independent Variables (\(x_i\))}: Predictors or features used
  in the model.
\item
  \textbf{Coefficients (\(\beta_i\))}: Parameters that describe the
  influence of the predictors on the outcome.
\item
  \textbf{Error Term (\(\epsilon\))}: Represents the portion of \(y\)
  that cannot be explained by the linear model.
\end{itemize}

\hypertarget{estimation-using-ordinary-least-squares-ols}{%
\section{Estimation using Ordinary Least Squares
(OLS)}\label{estimation-using-ordinary-least-squares-ols}}

Ordinary Least Squares (OLS) is the most common method used to estimate
the parameters in linear regression. The idea is to minimize the
difference between the observed values and the values predicted by the
model. This difference is captured by the residual sum of squares, which
is minimized to obtain the optimal values of the coefficients.

\hypertarget{implementation-in-r-1}{%
\section{Implementation in R}\label{implementation-in-r-1}}

Below, we demonstrate how to fit a linear regression model in R using a
simple mock dataset. The dataset consists of a response variable \(y\)
and two predictors \(x_1\) and \(x_2\).

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generating Mock Data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \DecValTok{3}\NormalTok{)}
\NormalTok{epsilon }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x1 }\SpecialCharTok{{-}} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{x2 }\SpecialCharTok{+}\NormalTok{ epsilon}

\CommentTok{\# Creating a data frame}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x1, x2)}

\CommentTok{\# Linear Regression Model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{data =}\NormalTok{ data)}

\CommentTok{\# Summary of the Model}
\FunctionTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x1 + x2, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8730 -0.6607 -0.1245  0.6214  2.0798 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  3.38862    0.43979   7.705 1.13e-11 ***
## x1           1.93341    0.05243  36.873  < 2e-16 ***
## x2          -0.49206    0.03300 -14.912  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9513 on 97 degrees of freedom
## Multiple R-squared:  0.9442, Adjusted R-squared:  0.943 
## F-statistic: 820.2 on 2 and 97 DF,  p-value: < 2.2e-16
\end{verbatim}

\normalsize

\newpage

\hypertarget{limitations-or-pitfalls-of-linear-regression}{%
\section{Limitations or Pitfalls of Linear
Regression}\label{limitations-or-pitfalls-of-linear-regression}}

While linear regression is a simple and often effective approach, it
comes with certain limitations and potential pitfalls. Understanding
these drawbacks is crucial when determining whether linear regression is
the best choice for a given dataset. Here are some of the key
limitations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Assumption of Linearity}: Linear regression assumes that there
  is a linear relationship between the independent and dependent
  variables. If the true relationship is non-linear, the linear model
  will not adequately capture the complexity of the data, leading to
  poor predictive performance.
\item
  \textbf{Outliers Can Have a Large Influence}: Linear regression is
  sensitive to outliers, which can disproportionately affect the fit of
  the model. A single extreme value in the data can significantly alter
  the coefficient estimates, leading to a biased or misleading
  interpretation of the model (more on this below).
\item
  \textbf{Multicollinearity}: When independent variables are highly
  correlated, the estimates of the regression coefficients can become
  unstable. This phenomenon, known as multicollinearity, makes it
  difficult to determine the independent effect of each predictor on the
  dependent variable.
\item
  \textbf{Non-Constant Variance (Homoscedasticity)}: Linear regression
  assumes that the variance of the errors is constant across all levels
  of the independent variables. If this assumption is violated
  (heteroscedasticity), it can lead to inefficient estimates and affect
  hypothesis testing.
\item
  \textbf{Assumption of Normality in Errors}: Linear regression assumes
  that the error terms are normally distributed. If the errors deviate
  significantly from normality, this can impact confidence intervals and
  the reliability of hypothesis tests.
\item
  \textbf{Overfitting in High Dimensions}: When there are many
  predictors relative to the number of observations, linear regression
  can easily overfit the data, capturing noise instead of the underlying
  trend. Regularization methods such as Lasso or Ridge regression may be
  more appropriate in such cases.
\end{enumerate}

\newpage

\hypertarget{impact-of-outliers-on-goodness-of-fit}{%
\section{Impact of Outliers on Goodness of
Fit}\label{impact-of-outliers-on-goodness-of-fit}}

Outliers are extreme values that do not follow the general trend of the
rest of the data. In linear regression, outliers can have a major
influence on the estimated model parameters, ultimately affecting the
goodness of fit. Let's discuss how outliers impact the model:

\begin{itemize}
\item
  \textbf{Effect on Coefficients}: Since linear regression aims to
  minimize the residual sum of squares (RSS), any point that lies far
  away from the regression line will have a disproportionately large
  impact on the resulting line. The presence of outliers can pull the
  regression line toward themselves, distorting the relationship between
  the dependent and independent variables.
\item
  \textbf{Leverage and Influence}: Outliers can be categorized into two
  types: those that have high leverage and those that have high
  influence. Leverage points are those that lie far from the mean of the
  predictors, and they can significantly affect the position of the
  regression line even if their response values align with the trend.
  Influential points are outliers that change the overall fit of the
  model, and removing these points can substantially alter the estimated
  coefficients.
\item
  \textbf{Example of Outlier Influence}: The effect of an outlier can be
  illustrated using a simple plot:
\end{itemize}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generating Mock Data with an Outlier}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{456}\NormalTok{)}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{epsilon }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x1 }\SpecialCharTok{+}\NormalTok{ epsilon}

\CommentTok{\# Adding an Outlier}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(x1, }\DecValTok{20}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(y, }\DecValTok{100}\NormalTok{)}

\CommentTok{\# Fitting a Linear Model}
\NormalTok{data\_with\_outlier }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x1)}
\NormalTok{model\_with\_outlier }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1, }\AttributeTok{data =}\NormalTok{ data\_with\_outlier)}

\CommentTok{\# Plotting}
\FunctionTok{plot}\NormalTok{(x1, y, }\AttributeTok{main =} \StringTok{"Effect of Outlier on Linear Regression"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"x1"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"y"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(model\_with\_outlier, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/unnamed-chunk-7-1} 

}

\caption{Default caption for all figures}\label{fig:unnamed-chunk-7}
\end{figure}
\normalsize

\newpage

In the plot above, the red regression line is influenced by the single
outlier, deviating significantly from what it would be without that
extreme point. In practice, techniques such as robust regression can be
employed to minimize the impact of such outliers.

\hypertarget{residual-sum-of-squares-rss-and-l2-norm}{%
\subsection{Residual Sum of Squares (RSS) and L2
Norm}\label{residual-sum-of-squares-rss-and-l2-norm}}

\hypertarget{residual-sum-of-squares-rss}{%
\subsubsection{Residual Sum of Squares
(RSS)}\label{residual-sum-of-squares-rss}}

The Residual Sum of Squares (RSS) is a key concept in linear regression,
representing the total deviation of the observed values from the values
predicted by the model. The objective of linear regression is to
minimize the RSS, which is mathematically defined as:

\[
R S S=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
\]

\hypertarget{the-l2-norm}{%
\subsubsection{The L2 Norm}\label{the-l2-norm}}

The RSS minimization problem is related to the concept of the L2 norm.
In the context of linear regression, the L2 norm represents the
Euclidean distance between the actual observed values and the predicted
values of the model. The L2 norm is defined as:

\[
\|\mathbf{r}\|_2=\left(\sum_{i=1}^n r_i^2\right)^{1 / 2}
\]

where \(r_i=y_i-\hat{y}_i\) are the residuals. The RSS is simply the
squared L 2 norm of the residual vector, meaning that the linear
regression objective can be understood as minimizing the L2 distance
between the predicted and actual values.

This connection to the L2 norm is one of the reasons why linear
regression is so sensitive to outliers. Because each residual is
squared, larger residuals have a disproportionate impact, which leads to
a model that might be unduly influenced by a few data points.

\hypertarget{comparison-with-l1-norm}{%
\subsubsection{Comparison with L1 Norm}\label{comparison-with-l1-norm}}

An alternative to the L2 norm is the L1 norm, which forms the basis for
Lasso Regression. The L1 norm is defined as:

\[
\|\mathbf{r}\|_1=\sum_{i=1}^n\left|r_i\right|
\]

Minimizing the L1 norm instead of the L2 norm results in a model that is
less sensitive to outliers. This is because the L1 norm does not square
the residuals, giving less weight to extreme errors. In fact, L1-based
models often produce sparse solutions, making them particularly useful
in high-dimensional settings where feature selection is important.

\hypertarget{regularization-and-the-l2-penalty}{%
\subsubsection{Regularization and the L2
Penalty}\label{regularization-and-the-l2-penalty}}

In practice, Ridge Regression (a form of regularized linear regression)
adds an L2 penalty term to the loss function to prevent overfitting:

\[
L(\beta)=R S S+\lambda \sum_{j=1}^n \beta_j^2
\]

The \(\lambda\) term controls the amount of regularization, shrinking
the coefficients toward zero and reducing model complexity. This can be
particularly useful when dealing with multicollinearity or
high-dimensional datasets.

Practical Implementation with Regularization

Below is an example in R where we use the glmnet package to fit a Ridge
regression model:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the glmnet package}
\FunctionTok{library}\NormalTok{(glmnet)}

\CommentTok{\# Generating Mock Data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{789}\NormalTok{)}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \DecValTok{3}\NormalTok{)}
\NormalTok{epsilon }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x1 }\SpecialCharTok{{-}} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{x2 }\SpecialCharTok{+}\NormalTok{ epsilon}

\CommentTok{\# Creating a matrix for predictors (required by glmnet)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(x1, x2))}
 
\CommentTok{\# Fitting a Ridge Regression Model}
\NormalTok{ridge\_model }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(X, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{) }\CommentTok{\# alpha = 0 for Ridge}

\CommentTok{\# Plotting the coefficient paths}
\FunctionTok{plot}\NormalTok{(ridge\_model, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Coefficient Paths for Ridge Regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{25MLEqs-V0.12_files/figure-latex/unnamed-chunk-8-1} 

}

\caption{Default caption for all figures}\label{fig:unnamed-chunk-8}
\end{figure}
\normalsize

The plot above shows how the coefficients shrink as the regularization
parameter \(\lambda\) increases, helping to control overfitting while
still capturing the main patterns in the data.

\hypertarget{equation-summary}{%
\section{Equation Summary}\label{equation-summary}}

In this extended discussion of linear regression, we highlighted some of
the key limitations of the method, discussed the significant impact that
outliers can have on the model, and introduced the concept of RSS and
its relation to the L2 norm. We also explored how these concepts are
extended in regularized linear regression, such as Ridge regression, to
address issues like overfitting and multicollinearity. Linear regression
remains a powerful and widely used tool, but it's important to be
mindful of the underlying assumptions and potential pitfalls. By
understanding the limitations and exploring regularization techniques,
we can build more robust and effective predictive models.

\newpage

\newpage

\hypertarget{ordinary-least-squares-ols}{%
\chapter{Ordinary Least Squares
(OLS)}\label{ordinary-least-squares-ols}}

\huge

\[
\color{dblu}{\hat{\beta} = (X^T X)^{-1} X^T y}
\] \normalsize

\begin{figure*}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{pictures/Whimsical-10-OLS.jpeg}
    \caption*{\Large Multivariate Linear Model Fitting}
  \end{center}
\end{figure*}

\newpage

\vspace*{\fill}

\begin{a_def_eq}{Maximum Likelihood Estimation}{def10} 

\begin{align} 
\hat{\beta} = (X^T X)^{-1} X^T y 
\end{align}

\begin{description}[align=left, labelwidth=2cm, labelsep=0em, leftmargin=2cm]
    \item[$\hat{\beta}$] The \textbf{estimated parameter vector}, representing the values that maximize the likelihood function in the context of linear regression.
    \vspace{0.5\baselineskip}
    \item[$X$] The \textbf{design matrix}, a matrix of independent variables (features) where each row corresponds to an observation and each column to a feature in the dataset.
    \vspace{0.5\baselineskip}
    \item[$y$] The \textbf{vector of observed values}, representing the dependent variable or the target values corresponding to the observations in the dataset.
    \vspace{0.5\baselineskip}
    \item[$X^T$] The \textbf{transpose of the design matrix}, which switches the rows and columns of the matrix $X$.
    \vspace{0.5\baselineskip}
    \item[$(X^T X)^{-1}$] The \textbf{inverse of the matrix product} of $X^T$ and $X$, a necessary step to solve for $\hat{\beta}$ using the normal equation.
    \vspace{0.5\baselineskip}
\end{description}

\end{a_def_eq}

\hfill\break

\textbf{Introduction}: OLS is a method for estimating the unknown
parameters in a linear regression model.

\textbf{Description}: It minimizes the sum of squared residuals between
the observed and predicted values.

\textbf{Importance in ML}: OLS is a fundamental technique for regression
analysis. It helps determine the best-fit line for the data, making it
useful for predictive modeling. \vspace*{\fill}

\newpage

\hypertarget{linear-model-intro}{%
\section{Linear Model Intro}\label{linear-model-intro}}

We have linear model - and assume the error, in this setting
\(\epsilon\), is normally distributed. This is a very important
consideration since it frames how the solution for the linear regression
problem is obtained!

This is a VECTOR equation of the overall data set: \[
Y=A+B X + [\varepsilon]
\] Here we denote \([\varepsilon]\) as a VECTOR of normal error for the
outcome vector Y, where

\[
\varepsilon \sim N\left(0, \sigma^{2}\right) \text{ is i.i.d, Markov}
\]

This is the linear model estimator for a sample over \(i\) scalar
equation (notice the hat in \(\hat{y}_{i}\)):

\[
\hat{y}_{i}=\alpha+\beta x_{i}
\]

\[
\varepsilon_{i} = \hat{y}_{i}-y_{i}
\]

We would like to determine \(\alpha\) and \(\beta\). So, we are trying
to minimize this equation, the residuals of error:

\[
\sum_{i=1}^{n}\varepsilon_{i}^{2} = \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2} = \sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2}
\]

Now we consider this from a MLE perspective, but first consider the
expected value of \(y_{i}\):

What is dist of \(y_{i}\)?, look at the estimated value of \(y_{i}\):

\[
\begin{aligned}
E\left(y_{i}\right)&=E\left(\alpha+\beta x_{i}+\varepsilon_{i}\right)\\
&=\alpha+\beta x_{i}+E\left(\varepsilon_{i}\right) \\
&=\alpha+\beta x_{i}
\end{aligned}
\]

Now also consider the variance of \(y_{i}\):

\[
\begin{aligned}
\operatorname{Var}\left(y_{i}\right) &=\operatorname{Var}\left(\alpha+\beta x_{i}+\varepsilon_{i}\right) \\
&=\operatorname{Var}\left(\varepsilon_{i}\right) \\
&=\sigma^{2}
\end{aligned}
\]

Note the pdf for \(y_{i}\) is all about the expected value and the
variance of the data. So we can say this in a specific way \ldots{} take
a look at this to better understand the definitions here.

\[
y_{i} \sim N\left(E\left(y_{i}\right), \operatorname{Var}\left(y_{i}\right)\right) \sim N\left(\alpha+\beta x_{i}, \sigma\right).
\]

Here is the ``burn your noodle idea here'' \ldots{} These \(y_{i}\) are
independent, but NOT identically distributed, since the `mean' or
expected value depends on \(x_{i}\), as you should inspect
\(E\left(y_{i}\right)\) carefully and see
\(y_{i} \sim N\left(\alpha+\beta x_{i}, \sigma\right)\) \ldots{} A
little tricky but important to realize.

Ok so lets write down the pdf for for \(y_{i}\);

\[
\Rightarrow f\left(y_{i} \mid \alpha, \beta, \sigma^{2}\right)=\frac{e^{-\left(y_{i}-\alpha-\beta x_{i}\right)^{2} / 2 \sigma^{2}}}{\sqrt{2 \pi \sigma^{2}}}
\]

This is a pdf for one single \(y_{i}\), recalling from above that this
has it's own pdf, and each \(y_{i}\) is not identically distributed. So,
we have to consider the likelihood.

The likelihood \ldots{}

\[
\Rightarrow L\left(\alpha,\beta, \sigma^{2} \mid y_{1 \ldots} y_{n}\right)=\prod_{i=1}^{n} f\left(y_{i} \mid \alpha_{1}, \beta, \sigma^{2}\right) = \prod_{i=1}^{n}\frac{e^{-\left(y_{i}-\alpha-\beta x_{i}\right)^{2} / 2 \sigma^{2}}}{\sqrt{2 \pi \sigma^{2}}}
\]

\[
L\left(\alpha,\beta, \sigma^{2} \mid y_{1 \ldots} y_{n}\right) =
\frac{\prod_{i=1}^{n} \exp \left(-\left(y_{i}-\alpha-\beta x_{i}\right)^{2} / 2 \sigma^{2}\right)}{\left(2 \pi \sigma^{2}\right)^{n / 2}}
\]

\[
= \frac{\exp ( \sum_{i=1}^{n} \frac{-\left(y_{i}-\alpha-\beta_{x i}\right)^{2}}{2 \sigma^{2}})}{\left(2 \pi \sigma^{2}\right)^{n / 2}}
\]

We designate the \(\log L\) with a small \(l\), and you get the log form
of the likelihood (please recall the tricks for manipulating products
and sums of logs):

\[
l\left(\alpha_{1} \beta_{1} \sigma^{2} \mid Y_{1}, \ldots, Y_{n}\right) \\
=\sum_{i=1}^{n}\left(y_{i}-\alpha-\beta x_{i}\right)^{2} / 2 \sigma^{2}
-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)
\]

So we are back to the MLE ideas: With the log-likelihood, we can
maximized with the first derivative. ``Nothing new to see here \ldots{}
move along folks'\,' lol. As such, we will determine the model
parameters \$\alpha \$ and \(\beta\) using the MLE approach from before
on our linear model log-likelihood expression.

So lets talk about what this looks like for \(\beta\)

\[
y_{i}=\alpha+\beta x_{i}+\varepsilon_{i} \quad \text{ and } \quad
\varepsilon_{i} \sim  N\left(0,\sigma^{2}\right) \quad \text{ i.i.d. }
\]

\[
\hat{\beta} \sim N\left(\beta, \frac{\sigma^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\right) \quad \text{note} \quad \bar{x}=\mu_{x}
\]

\[
\hat{\beta}=\frac{\sum_{i=1}^{n} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\]

\[
E(\hat{\beta})=E\left[\frac{\sum_{i=1}^{n} Y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\right]
\]

\[
=\frac{\sum_{i=1}^{n}E[y_{i}]\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\]

\[
=\frac{\sum_{i=1}^{n} \left(\alpha+\beta x_{i} \right)\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\]

\[
\text { let } \gamma=\alpha-\beta \bar{x}
\]

\[
=\frac{\sum_{i=1}^{n}\left(\gamma+\beta\left(x_{i}-\bar{x}\right)\right)\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}
\]

collecting terms and cancelling we arrive at:

\[
E(\hat{\beta})=\frac{\gamma \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)+\sum_{i=1}^{n} \beta\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} =\frac{\gamma \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}+\beta
\]

focus on the definition of the sums of x

\[
\begin{gathered}
\sum_{i=n}^{n}\left(x_{i}-\bar{x}\right) \\
=\left[\sum x_{i}\right]-n \bar{x} = 0 \\
\text{since ... }
n \bar{x}=\sum x_{i}
\end{gathered}
\]\textbackslash{}

So we get a collapse of the expected value (kind of a let down): \[
E(\hat{\beta})=\beta
\]

We can extend this to multiple dimensions in X

\[
P(Y\! \mid\! X)=N\left(f(X), \sigma^{2}\right)
\]

Assume \(f(X)\) is linear in \(X\)

\[
f(X)=\sum_{i=1}^{N} W_{i} X_{i}+W_{0}=W^T X
\]

What is this ? .. this is \(y = m x + b\) in multiple dimensions.

Where we denote by \(X\) an extended vector with an additional entry \[
X=\left[1, X_{1}, X_{2}, \ldots, X_{N}\right], W=\left[W_{0}, W_{1}, W_{2}, \ldots, W_{N}\right]
\]

Note in this case \(N\) is not the sample size. It is the number of
independent variables we are trying to model.

Maximizing the log likelihood on a training set
\(\left\{\left(X^{l}, Y^{l}\right) \mid l=1, \ldots, L\right\}\)

\[
W^{*}=\operatorname{argmax}_{W} \sum_{l=1}^{L} \log P\left(Y^{l} \mid X^{l} ; W\right)
\] Where

\[
P(Y \mid X)=N\left(f(X), \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{(Y-f(X))^{2}}{2 \sigma^{2}}\right\}
\]

Note in this case \(L\) \textbf{IS} the sample size. It is the number of
samples of data we are processing.

Maximizing the log likelihood on a training set
\(\left\{\left(X^{l}, Y^{l}\right) \mid l=1, \ldots, L\right\}\) \[
W^{*}=\operatorname{argmax}_{W} \sum_{l=1}^{L} \log P\left(Y^{l} \mid X^{l} ; W\right)
\]

Where

\[
P(Y \mid X)=N\left(f(X), \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{(Y-f(X))^{2}}{2 \sigma^{2}}\right\}
\]

Note in this case \(L\) \textbf{IS} the sample size. It is the number of
samples of data we are processing.

Perform the derivative calculation:

\[
\begin{aligned}
\frac{\partial \sum_{l}(y-f(x ; W))^{2}}{\partial w_{i}} &=\sum_{l} 2(y-f(x ; W)) \frac{\partial(y-f(x ; W))}{\partial w_{i}} \\
&=\sum_{l}-2(y-f(x ; W)) \frac{\partial f(x ; W)}{\partial w_{i}}
\end{aligned}
\]

Set the derivative to zero, \[
\sum_{l=1}^{L}\left(Y^{l}-W^{T} X^{l}\right) X^{l}=0
\]

Can be written as sn alternative matrix representation

\[
\mathbf{X Y}-\mathbf{X X}^{\mathrm{T}} \mathbf{W}=\mathbf{0}
\]

Where \(\mathrm{X}\) is data matrix, with each column being a training
example, \(\mathrm{Y}\) is the vector of target variables.

\[
\mathbf{X}=\left[X^{1}, X^{2}, \ldots, X^L\right], \mathbf{Y}=\left[Y^{1}, Y^{2}, \ldots, Y^L\right]^{T}
\]

Solving \(\mathbf{X Y}-\mathbf{X X}^{\mathrm{T}} \mathbf{W}=\mathbf{0}\)
\(W=\left(X X^{T}\right)^{-1} X Y\)

Where

\[
\boldsymbol{X}^{+}=\left(\boldsymbol{X} \boldsymbol{X}^{T}\right)^{-1} \boldsymbol{X}
\]

is called the Pseudo Inverse.

This leads us to the following final relation for MLE on Linear
Regression \ldots{} showing the linear regression using the MLE for the
optimization to find the proper fit parameters is the following elegant
equation.

\[
\arg \max _{\beta} \prod_{i=1}^{n} \operatorname{Normal}\left(y_{i} \mid \mu=(X \beta)_{i}, \sigma\right)=\left(X^{T} X\right)^{-1} X^{T} y
\]

We will dig into this further and also learn about MAP - which is the
Bayesian path for this math.

Random Finds - for exposition

\[
\text {Error} = \sum_{i=1}^{n} (\text{real.output} - \text{prediction}) ^{2}
\]

Suppose we have \(k\) predictor variables \(x_{1}, \ldots, x_{k}\) and a
dependent variable \(y\). We consider the linear relation:

\[
\xi_{i}\left(\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right)=\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}
\]

Often we do not explicitly write \(\xi\) as a function of the parameters
\(\beta_{0}, \ldots \beta_{k}\), and write instead:

\[
\xi_{i}=\beta_{0}+\beta_{1} x_{1 i}+\cdots+\beta_{k} x_{k i}
\]

The parameters \(\beta_{0}, \beta_{1}, \ldots, \beta_{k}\) are called
(regression) coefficients. In particular, \(\beta_{0}\) is called the
(regression) intercept and \(\beta_{1}, \ldots, \beta_{k}\) are
(regression) slope coefficients.

The term simple linear regression is often used to cover the special
case of \(k=1\). If there is more than one predictor, i.e., \(k>1\), the
term multiple linear regression is common.

Based on the predictions of a parameter vector
\(\left\langle\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right\rangle\), we
consider the residual sum of squares as a measure of prediction error:

\[
\operatorname{RSS}_{\left\langle\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right\rangle}=\sum_{i=1}^{k}\left[y_{i}-\xi_{i}\left(\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right)\right]^{2}
\]

We would like to find the best parameter values (denoted traditionally
by a hat on the parameter's variable: \(\hat{\beta}_{i}\) ) in the sense
of minimizing the residual sum of squares:

\[
\left\langle\hat{\beta}_{0}, \hat{\beta}_{1}, \ldots, \hat{\beta}_{k}\right\rangle=\arg \min _{\left\langle\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right\rangle} \operatorname{RSS}_{\left\langle\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right\rangle}
\]

The prediction corresponding to the best parameter values is denoted by
\(\hat{\xi} \in \mathbb{R}^{n}\) and called the best linear predictor:

\[
\hat{\xi}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{1 i}+\cdots+\hat{\beta}_{k} x_{k i}
\]

It is also possible, and often convenient, to state the linear
regression model in terms of matrix operations. Traditionally, we
consider a so-called predictor matrix \(X\) of size \(n \times(k+1)\),
where \(n\) is the number of observations in the data set and \(k\) is
the number of predictor variables. The predictor matrix includes the
values for all predictor variables and it also includes an ``intercept
column'' \(\left(X^{T}\right)_{0}\) for which \(X_{i 0}=1\) for all
\(1 \leq i \leq n\) so that the intercept \(\beta_{0}\) can be treated
on a par with the other regression coefficients.

Using the predictor matrix \(X\), the linear predictor vector \(\xi\)
is:

\[
\xi=X \beta
\]

This is the formulation of the Matrices \ldots{}

\[
X=\overbrace{
\left[\begin{array}{cccccc}
1 & x_{11} & x_{12} & x_{13} & \cdots & x_{1 k} \\
1 & x_{21} & x_{22} & x_{23} & \cdots & x_{2 k} \\
\vdots & & \ddots & & \vdots \\
1 & x_{n 1} & x_{n 2} & x_{n 3} & \cdots & x_{n k}
\end{array}\right]
}^{n \times k}
\]

\[
\hat{\beta}=
\overbrace{
\left[\hat{\beta}_{0} \hat{\beta}_{1} \cdots \hat{\beta}_{k}\right]^{T} 
}^{k \times 1}
\]

\[
\hat{y}=
\overbrace{\left[\begin{array}{c}
\hat{y}_{0} \\
\hat{y}_{1} \\
\vdots \\
\hat{y}_{k}
\end{array}\right]
}^{n \times 1}
\]

So let's show how this works out from the algebraic form of the
regression to the matrix form of the pseudo inverse \ldots{} a with a
little bit more detail.

Let \(X\) be the \(n \times(k+1)\) regression matrix for a linear
regression model with \(k\) predictor variables for a data set \(y\)
with \(n\) observations. The solution for OLS regression

\[
\hat{\beta}=\left\langle\hat{\beta}_{0}, \hat{\beta}_{1}, \ldots, \hat{\beta}_{k}\right\rangle=\arg \min _{\beta} \sum_{i=1}^{k}\left(y_{i}-(X \beta)_{i}\right)^{2}
\]

is given by:

\[
\hat{\beta}=\left(X^{T} X\right)^{-1} X^{T} y
\]

Proof. With \(n\) observations, the vector \(\xi\) of predicted values
for given coefficient vector \(\beta\) is:

\[
\xi=X \beta
\]

Where (recall) we have the `b' encoded in the X vector definition

\[
\xi=X \beta= X_{0 } \beta_{0} + X_{N} \beta_{N}
\]

and

More explicitly, this means that:

\[
\begin{aligned}
\xi_{1} &=\beta_{0}+\beta_{1} X_{11}+\beta_{2} X_{12}+\ldots+\beta_{k} X_{1 k} \\
\xi_{2} &=\beta_{0}+\beta_{1} X_{21}+\beta_{2} X_{22}+\ldots+\beta_{k} X_{2 k} \\
\ldots & \\
\xi_{n} &=\beta_{0}+\beta_{1} X_{n 1}+\beta_{2} X_{n 2}+\ldots+\beta_{k} X_{n k}
\end{aligned}
\]

The OLS estimator is obtained (like in the special case of simple linear
regression) by minimizing the residual sum of squares (RSS). The RSS for
the multiple linear regression model is

\[
Q=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\beta_{2} X_{i 2}-\ldots-\beta_{k} X_{i k}\right)^{2}
\]

To find the minimum of \(Q\) we calculate the first partial derivative
of \(Q\) for each \(\beta_{j}\) :

\[
\begin{aligned}
&\frac{\partial Q}{\partial \beta_{0}}=2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\beta_{2} X_{i 2}-\ldots-\beta_{k} X_{i k}\right)(-1) \\
&\frac{\partial Q}{\partial \beta_{1}}=2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\beta_{2} X_{i 2}-\ldots-\beta_{k} X_{i k}\right)\left(-X_{i 1}\right) \\
&\frac{\partial Q}{\partial \beta_{2}}=2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\beta_{2} X_{i 2}-\ldots-\beta_{k} X_{i k}\right)\left(-X_{i 2}\right) \\
&\frac{\partial Q}{\partial \beta_{k}}=2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\beta_{2} X_{i 2}-\ldots-\beta_{k} X_{i k}\right)\left(-X_{i k}\right)
\end{aligned}
\]

For the minimum \(\hat{\beta}\) the derivative of each equation must be
zero:

\[
\begin{aligned}
&\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} X_{i 1}-\hat{\beta}_{2} X_{i 2}-\ldots-\hat{\beta}_{k} X_{i k}\right)=0 \\
&\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} X_{i 1}-\hat{\beta}_{2} X_{i 2}-\ldots-\hat{\beta}_{k} X_{i k}\right) X_{i 1}=0 \\
&\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} X_{i 1}-\hat{\beta}_{2} X_{i 2}-\ldots-\hat{\beta}_{k} X_{i k}\right) X_{i 2}=0 \\
&\ldots \\
&\sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} X_{i 1}-\hat{\beta}_{2} X_{i 2}-\ldots-\hat{\beta}_{k} X_{i k}\right) X_{i k}=0
\end{aligned}
\]

Alternatively, we can use matrix notation and combine the above
equations into the following form:

\[
X^{T} y-X^{T} X \hat{\beta}=0
\]

Rearranging this, the following expression is known as normal equations:

\[
X^{T} X \hat{\beta}=X^{T} y
\]

Just for illustration, the system of normal equations in expanded matrix
notation is:

\[
\left[\begin{array}{cccc}
n & \sum_{i=1}^{n} X_{i 1} & \cdots & \sum_{i=1}^{n} X_{i k} \\
\sum_{i=1}^{n} X_{i 1} & \sum_{i=1}^{n} X_{i 1}^{2} & \ldots & \sum_{i=1}^{n} X_{i 1} X_{i k} \\
\ldots & \ldots & \ldots & \ldots \\
\sum_{i=1}^{n} X_{i k} & \sum_{i=1}^{n} X_{i k} X_{i 1} & \cdots & \sum_{i=1}^{n} X_{i k}^{2}
\end{array}\right]\left[\begin{array}{c}
\hat{\beta}_{0} \\
\hat{\beta}_{1} \\
\cdots \\
\hat{\beta}_{k}
\end{array}\right]=\left[\begin{array}{c}
\sum_{i=1}^{n} y_{i} \\
\sum_{i=1}^{n} X_{i 1} y_{i} \\
\ldots \\
\sum_{i=1}^{n} X_{i k} y_{i}
\end{array}\right]
\]

The estimator \(\hat{\beta}\) can be obtained by rearranging again:

\[
\hat{\beta}=\left(X^{T} X\right)^{-1} X^{T} y
\]

\newpage

\hypertarget{r-squared-r2-score}{%
\chapter{R-squared (R\^{}2) Score}\label{r-squared-r2-score}}

\[
R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\]

\textbf{Introduction}: R-squared is a statistical measure that
represents the proportion of variance in the dependent variable
explained by the independent variables.

\textbf{Description}: It is a goodness-of-fit measure that ranges from 0
to 1, with values closer to 1 indicating a better fit.

\textbf{Importance in ML}: R-squared is used to evaluate the performance
of regression models. It provides insights into how well the model
explains the variance in the target variable.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{mean-squared-error-mse}{%
\chapter{Mean Squared Error (MSE)}\label{mean-squared-error-mse}}

\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]

\textbf{Introduction}: Mean Squared Error is a common loss function used
to measure the average squared difference between predicted and actual
values.

\textbf{Description}: It calculates the squared difference for each
observation and then averages them to provide a metric of model
accuracy.

\textbf{Importance in ML}: MSE is used to evaluate the performance of
regression models. Lower MSE values indicate a better fit, making it
essential for identifying the optimal model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{mean-squared-error-with-l2-regularization-mse-l2-reg}{%
\chapter{Mean Squared Error with L2 Regularization (MSE + L2
Reg)}\label{mean-squared-error-with-l2-regularization-mse-l2-reg}}

\[
\text{MSE}_{\text{regularized}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p \beta_j^2
\]

\textbf{Introduction}: Regularized MSE adds a penalty to the loss
function to prevent overfitting by discouraging overly complex models.

\textbf{Description}: The regularization term \(\lambda \sum \beta_j^2\)
helps keep model parameters small, which leads to simpler models.

\textbf{Importance in ML}: L2 regularization is key in reducing
overfitting by controlling the complexity of the model, ensuring that
the model generalizes well to new data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{eigenvectors-and-eigenvalues}{%
\chapter{Eigenvectors and
Eigenvalues}\label{eigenvectors-and-eigenvalues}}

\[
A v = \lambda v
\]

\textbf{Introduction}: Eigenvectors and eigenvalues are fundamental
concepts in linear algebra used to understand the structure of matrices.

\textbf{Description}: They represent the directions along which linear
transformations act by only scaling the vectors, without changing their
direction.

\textbf{Importance in ML}: Eigenvectors and eigenvalues are used in
dimensionality reduction techniques like PCA (Principal Component
Analysis), which helps reduce the number of features while preserving
essential information.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{entropy}{%
\chapter{Entropy}\label{entropy}}

\[
\text{Entropy} = - \sum_{i} p_i \log_2(p_i)
\]

\textbf{Introduction}: Entropy is a measure of uncertainty or randomness
in a dataset.

\textbf{Description}: It quantifies the impurity in a dataset, making it
a key concept in information theory and decision trees.

\textbf{Importance in ML}: Entropy is used in decision tree algorithms
to decide the best split at each node by measuring the purity of a
dataset. Lower entropy indicates a more homogenous group of samples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{k-means-clustering}{%
\chapter{K-Means Clustering}\label{k-means-clustering}}

\[
\arg\min_S \sum_{k=1}^K \sum_{x \in S_k} \|x - \mu_k\|^2
\]

\textbf{Introduction}: K-Means is an unsupervised learning algorithm
used for clustering data into K distinct groups.

\textbf{Description}: It minimizes the sum of squared distances between
data points and the centroid of their assigned cluster.

\textbf{Importance in ML}: K-Means is a fundamental clustering technique
used in exploratory data analysis and segmentation tasks. It helps
discover patterns and relationships in unlabeled data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{kullback-leibler-kl-divergence}{%
\chapter{Kullback-Leibler (KL)
Divergence}\label{kullback-leibler-kl-divergence}}

\[
D_{KL}(P \| Q) = \sum_{x \in \chi} P(x) \log \frac{P(x)}{Q(x)}
\]

\textbf{Introduction}: KL Divergence is a measure of how one probability
distribution diverges from a second, reference probability distribution.

\textbf{Description}: It is commonly used to measure the difference
between two probability distributions.

\textbf{Importance in ML}: KL Divergence is used in machine learning for
loss functions in models like variational autoencoders. It quantifies
how well the learned distribution approximates the true data
distribution.

\newpage

\hypertarget{introduction-to-kullback-leibler-kl-divergence}{%
\section{Introduction to Kullback-Leibler (KL)
Divergence}\label{introduction-to-kullback-leibler-kl-divergence}}

The \textbf{Kullback-Leibler (KL) divergence}, also called relative
entropy, measures the discrepancy between two probability distributions,
\(P\) (the true distribution) and \(Q\) (the approximation). It
quantifies the inefficiency of using \(Q\) to encode data from \(P\).

For discrete distributions, the KL divergence is defined as:

\[
D_{KL}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
\]

For continuous distributions, it is expressed as:

\[
D_{KL}(P \| Q) = \int_{-\infty}^\infty p(x) \log \frac{p(x)}{q(x)} \, dx
\]

Key properties of KL divergence: - It is not symmetric:
\(D_{KL}(P \| Q) \neq D_{KL}(Q \| P)\). - It is non-negative:
\(D_{KL}(P \| Q) \geq 0\), with equality only when \(P = Q\).

\hypertarget{example-calculation}{%
\section{Example Calculation}\label{example-calculation}}

We compute \(D_{KL}(P \| Q)\) for two discrete distributions: -
\(P(x_1, x_2) = [x_1, 1-x_1]\) - \(Q(x_1, x_2) = [0.5, 0.5]\)

The KL divergence formula becomes:

\[
D_{KL}(P \| Q) = x_1 \log \frac{x_1}{0.5} + (1 - x_1) \log \frac{1 - x_1}{0.5}
\]

We evaluate this for \(x_1 = \{0.1, 0.2, 0.3, 0.4, 0.5\}\).

\hypertarget{results-table}{%
\section{Results Table}\label{results-table}}

The table below shows the calculations for \(D_{KL}\), \(P(x_1)\),
\(P(x_2)\), and their respective log terms.

\[
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{Value of } x_1 & D_{KL} & P(x_1) & \log \left(\frac{P(x_1)}{Q(x_1)}\right) & P(x_2) & \log \left(\frac{P(x_2)}{Q(x_2)}\right) \\
\hline
0.1 & 0.3681 & 0.1 & -1.609 & 0.9 & 0.5878 \\
0.2 & 0.1927 & 0.2 & -0.9163 & 0.8 & 0.4700 \\
0.3 & 0.08228 & 0.3 & -0.5108 & 0.7 & 0.3365 \\
0.4 & 0.02014 & 0.4 & -0.2231 & 0.6 & 0.1823 \\
0.5 & 0.0000 & 0.5 & 0.0000 & 0.5 & 0.0000 \\
\hline
\end{array}
\]

\hypertarget{observations-1}{%
\section{Observations}\label{observations-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(D_{KL}\) decreases as \(x_1\) approaches 0.5, where \(P = Q\).
\item
  The terms \(P(x_i) \log \frac{P(x_i)}{Q(x_i)}\) reflect the penalty
  for mismatches between \(P\) and \(Q\).
\end{enumerate}

\newpage

\hypertarget{log-loss}{%
\chapter{Log Loss}\label{log-loss}}

\[
-\frac{1}{N} \sum_{i=1}^N \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
\]

\textbf{Introduction}: Log Loss, or logistic loss, is a loss function
used for binary classification tasks.

\textbf{Description}: It measures the performance of a classification
model whose output is a probability value between 0 and 1.

\textbf{Importance in ML}: Log Loss is crucial in evaluating
classification models where the output is a probability. Lower log loss
indicates a more accurate model, especially for probabilistic
predictions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{support-vector-machine-svm-objective}{%
\chapter{Support Vector Machine (SVM)
Objective}\label{support-vector-machine-svm-objective}}

\[
\min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i (w \cdot x_i - b))
\]

\textbf{Introduction}: SVM is a supervised learning model used for
classification and regression tasks.

\textbf{Description}: It finds the hyperplane that best separates
different classes by maximizing the margin between them.

\textbf{Importance in ML}: SVMs are effective for high-dimensional
spaces and are used in classification problems where the decision
boundary is non-linear.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{singular-value-decomposition-svd}{%
\chapter{Singular Value Decomposition
(SVD)}\label{singular-value-decomposition-svd}}

\[
A = U \Sigma V^T
\]

\textbf{Introduction}: SVD is a matrix factorization technique used in
linear algebra.

\textbf{Description}: It decomposes a matrix into three other matrices,
revealing the intrinsic structure of the data.

\textbf{Importance in ML}: SVD is used in dimensionality reduction, data
compression, and noise reduction. It is also a core algorithm behind
recommendation systems.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\newpage

\hypertarget{lagrange-multiplier}{%
\chapter{Lagrange Multiplier}\label{lagrange-multiplier}}

\[
\max f(x) \;; \; g(x) = 0
\] \[
L(x, \lambda) = f(x) - \lambda \ast g(x)
\]

\textbf{Introduction}: The Lagrange multiplier is a method for finding
the local maxima and minima of a function subject to equality
constraints.

\textbf{Description}: It introduces a new variable, \(\lambda\), which
helps in optimizing a function while considering the constraint.

\textbf{Importance in ML}: Lagrange multipliers are used in optimization
problems with constraints, such as training machine learning models with
regularization terms.

\newpage

\hypertarget{the-human-equation}{%
\chapter{The Human Equation}\label{the-human-equation}}

\[
\textbf{HUMAN}
\]

\textbf{Introduction}:

\textbf{Description}:

\textbf{Importance in ML}:

\newpage

\backmatter

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{matrix-formulation-for-linear-separability}{%
\section*{Matrix Formulation for Linear
Separability}\label{matrix-formulation-for-linear-separability}}
\addcontentsline{toc}{section}{Matrix Formulation for Linear
Separability}

To check for linear separability, we can use a matrix formulation that
involves the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Matrix: Let \(X\) be the matrix of feature vectors, where each
  row corresponds to a data point. The size of \(X\) will be
  \(n \times d\), where \(n\) is the number of data points and \(d\) is
  the number of features.
\item
  Label Vector: Let \(y\) be the vector of labels, where each entry
  represents the class label of a data point (typically +1 or -1 for
  binary classification). The size of y will be \(n \times 1\).
\item
  Linearly Separable Condition: The data points are linearly separable
  if there exists a vector w and a bias term \(b\) such that:
\end{enumerate}

\[
\mathbf{y}_i\left(\mathbf{w}^T \mathbf{x}_{\mathrm{i}}+b\right)>0, \quad \forall i=1,2, \ldots, n
\]

This condition means that all data points from one class are on one side
of the hyperplane and all data points from the other class are on the
opposite side.

Matrix Representation of Linear Separability In matrix form, we can
express the linear separability condition as:

\[
\mathrm{y} \cdot(X \mathbf{w}+b \mathbf{1})>0
\]

Where: - \(X\) is the data matrix of size \(n \times d\), -
\(\quad \mathrm{y}\) is the label vector of size \(n \times 1\), -
\(\mathbf{w}\) is the weight vector of size \(d \times 1\), - \(b\) is
the bias term, and - 1 is a vector of ones of size \(n \times 1\).

SVM's Role in Linear Separability For SVM specifically, the goal is to
maximize the margin between the two classes while ensuring that the
classes remain linearly separable. The margin is the distance between
the closest points from each class (known as support vectors) to the
hyperplane. SVM tries to find the hyperplane that maximizes this margin,
ensuring that the data is not only separable but also well-separated by
the hyperplane.

Mathematically, this is represented as:

\[
\text { Maximize } \frac{2}{\|\mathbf{w}\|}
\]

Subject to the constraints:

\[
y_i\left(\mathbf{w}^T \mathbf{x}_i+b\right) \geq 1, \quad \forall i
\]

Where: - \(y_i\) is the class label for the \(i\)-th point (either +1 or
-1 ), - \(\mathbf{x}_i\) is the feature vector for the \(i\)-th data
point, - w is the weight vector, - \(b\) is the bias term.

This ensures that the points are classified correctly with the maximum
margin.

Conclusion: Matrix Formulation To summarize, linear separability in
machine learning is about finding a hyperplane that can separate the
classes of data in feature space. This is often formulated
mathematically as:

\[
\mathbf{y} \cdot(X \mathbf{w}+b 1)>0
\]

For a binary classification problem, a classifier like SVM finds the
optimal hyperplane that separates the classes with the largest margin,
ensuring that the data is linearly separable.

\newpage

\hypertarget{epilogue---guidance-and-test-materials}{%
\chapter{Epilogue - Guidance and Test
Materials}\label{epilogue---guidance-and-test-materials}}

\hypertarget{applying-musk-rules-to-writing-your-book-on-the-25-key-equations-in-machine-learning}{%
\section*{Applying Musk Rules to Writing Your Book on the ``25 Key
Equations in Machine
Learning''}\label{applying-musk-rules-to-writing-your-book-on-the-25-key-equations-in-machine-learning}}
\addcontentsline{toc}{section}{Applying Musk Rules to Writing Your Book
on the ``25 Key Equations in Machine Learning''}

\hypertarget{always-question-the-requirements}{%
\subsection*{\texorpdfstring{1. \textbf{Always Question the
Requirements}}{1. Always Question the Requirements}}\label{always-question-the-requirements}}
\addcontentsline{toc}{subsection}{1. \textbf{Always Question the
Requirements}}

\begin{itemize}
\tightlist
\item
  \textbf{Book Scope and Audience:} Reevaluate the core purpose and
  audience of the book. Who is your ideal reader? Do you really need to
  cover 25 equations, or would a different number be more impactful? Is
  there any unnecessary content that isn't truly essential to your goal
  of effective teaching and reference?
\item
  \textbf{Key Questions to Ask:} Do each of the sections contribute to
  the core learning objectives? Could some be combined, shortened, or
  presented differently to be clearer and more engaging?
\end{itemize}

\hypertarget{try-very-hard-to-delete-parts-or-processes}{%
\subsection*{\texorpdfstring{2. \textbf{Try Very Hard to Delete Parts or
Processes}}{2. Try Very Hard to Delete Parts or Processes}}\label{try-very-hard-to-delete-parts-or-processes}}
\addcontentsline{toc}{subsection}{2. \textbf{Try Very Hard to Delete
Parts or Processes}}

\begin{itemize}
\tightlist
\item
  \textbf{Delete Unnecessary Content:} Go through each chapter and
  decide if every subsection or detail is truly required. Are there
  parts of the explanations, historical contexts, or code examples that
  could be condensed or cut without losing value?
\item
  \textbf{Focus on Core Concepts:} Sometimes, less is more. Identify and
  focus on the key insights of each equation. For example, rather than
  going deep into every historical development of the normal
  distribution, focus on how the normal distribution is applied directly
  in machine learning.
\end{itemize}

\hypertarget{simplify-or-optimize-but-not-too-early}{%
\subsection*{\texorpdfstring{3. \textbf{Simplify or Optimize, But Not
Too
Early}}{3. Simplify or Optimize, But Not Too Early}}\label{simplify-or-optimize-but-not-too-early}}
\addcontentsline{toc}{subsection}{3. \textbf{Simplify or Optimize, But
Not Too Early}}

\begin{itemize}
\tightlist
\item
  \textbf{Simplification After Completion:} In the initial phase, write
  freely to capture all your thoughts and ideas. Don't worry too much
  about the length or complexity. Once you have a draft, then
  simplify---remove verbose explanations, simplify complex code
  snippets, or summarize complex derivations to keep it engaging.
\item
  \textbf{Iteration in Drafts:} Write each chapter in an exploratory way
  first, then optimize the language and reduce complexity in subsequent
  drafts.
\end{itemize}

\hypertarget{move-faster}{%
\subsection*{\texorpdfstring{4. \textbf{Move
Faster}}{4. Move Faster}}\label{move-faster}}
\addcontentsline{toc}{subsection}{4. \textbf{Move Faster}}

\begin{itemize}
\tightlist
\item
  \textbf{Accelerate Writing with LLMs and Tools:} Use tools like large
  language models (LLMs) to help generate content, summarize, and write
  initial drafts faster. You've already indicated using Mathpix, LaTeX,
  and R Markdown---lean into these tools to keep your process fast and
  flexible.
\item
  \textbf{Set Short-Term Goals:} Use milestones to maintain momentum.
  Instead of working on all 25 chapters at once, work on drafting 5
  chapters in a week. Use techniques like ``pomodoro'' to keep the pace
  fast without burnout.
\end{itemize}

\hypertarget{finally-automate}{%
\subsection*{\texorpdfstring{5. \textbf{Finally:
Automate}}{5. Finally: Automate}}\label{finally-automate}}
\addcontentsline{toc}{subsection}{5. \textbf{Finally: Automate}}

\begin{itemize}
\tightlist
\item
  \textbf{Automate Repetitive Tasks Last:} Once content is ready,
  automate formatting and typesetting. Given that you are using R
  Markdown and LaTeX, continue leveraging those tools for repetitive
  typesetting tasks, but do this after you've streamlined the core
  content.
\item
  \textbf{Automation for Consistency:} Automate consistency checks for
  formulas and code. This could be done with scripts that verify
  equation formatting, syntax checking for code snippets, or tools that
  flag unformatted variables.
\end{itemize}

\hypertarget{specific-strategies-to-speed-up-the-book-creation}{%
\subsection*{\texorpdfstring{\textbf{Specific Strategies to Speed Up the
Book
Creation:}}{Specific Strategies to Speed Up the Book Creation:}}\label{specific-strategies-to-speed-up-the-book-creation}}
\addcontentsline{toc}{subsection}{\textbf{Specific Strategies to Speed
Up the Book Creation:}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Outline Consolidation:}

  \begin{itemize}
  \tightlist
  \item
    Review your current outline and condense or merge sections that
    overlap. Consider if all 25 equations deserve equal weight. You
    could provide detailed explanations for 10-15 equations while giving
    more general summaries for the rest if they are less foundational.
  \end{itemize}
\item
  \textbf{Minimize and Prioritize:}

  \begin{itemize}
  \tightlist
  \item
    Focus on the most impactful content. Given the target audience, ask
    yourself: do they need a historical deep dive, or is the practical
    application what matters most? Emphasize applications, examples, and
    visuals over long theoretical discussions unless absolutely
    necessary.
  \end{itemize}
\item
  \textbf{Visuals and Examples:}

  \begin{itemize}
  \tightlist
  \item
    To make learning engaging, visuals and practical examples are key.
    Automate the creation of code snippets and figures that are
    repetitive. Tools like R and Python scripts can automatically
    generate the visuals needed for each equation, saving considerable
    time.
  \end{itemize}
\item
  \textbf{Leverage Existing Content:}

  \begin{itemize}
  \tightlist
  \item
    Since you are creating this as a reference book, consider using
    existing open-source implementations of the concepts as examples,
    rather than creating everything from scratch. This will save time
    while still providing value.
  \end{itemize}
\item
  \textbf{Peer Review and Feedback Early:}

  \begin{itemize}
  \tightlist
  \item
    Incorporate early feedback to streamline revisions. Share your draft
    with a small group of readers and ask for their feedback on clarity
    and structure. This will help you catch issues early, saving time
    during the editing phase.
  \end{itemize}
\item
  \textbf{Use a Versioned Approach:}

  \begin{itemize}
  \tightlist
  \item
    Set smaller, achievable versions (like v0.3, v0.4, etc.) where each
    iteration refines content. For example, v0.3 might be the initial
    full draft, v0.4 could focus on reducing complexity, and v0.5 could
    involve formatting and visual consistency. This will help you move
    in structured steps rather than getting bogged down in perfecting
    each detail upfront.
  \end{itemize}
\end{enumerate}

\hypertarget{moving-faster-in-practice}{%
\subsection*{Moving Faster, in
Practice:}\label{moving-faster-in-practice}}
\addcontentsline{toc}{subsection}{Moving Faster, in Practice:}

\begin{itemize}
\tightlist
\item
  \textbf{Sprint Write Sections:} Work on chapters in sprints. Set a
  specific time limit for each chapter, such as three days per equation.
  The goal is to create momentum and avoid stagnation.
\item
  \textbf{Lean on a Support System:} Delegate part of the process. If
  possible, outsource parts like copy-editing, formatting, or even
  generating diagrams. The faster you move past the basic draft stage,
  the better you can refine and make the book truly impactful.
\end{itemize}

\hypertarget{conclusion}{%
\subsection*{Conclusion:}\label{conclusion}}
\addcontentsline{toc}{subsection}{Conclusion:}

The Musk Rules encourage challenging the purpose of everything you're
including, focusing on essential content, and using tools and automation
wisely, but only after the core content is strong. By questioning,
deleting, simplifying, moving fast, and automating thoughtfully, you can
accelerate the process of creating your book without compromising
quality. Focus on the main principles that will make your book a
genuinely valuable reference---clear, effective, and engaging
explanations, visualizations, and examples---while avoiding unnecessary
details that slow you down.

How do these recommendations feel in the context of your goals for this
book? Would you like help refining a specific section or strategy for
moving faster?

\newpage

\hypertarget{temp-_-r-code---python-code---test-area}{%
\section{TEMP \_ R code - Python code - test
area}\label{temp-_-r-code---python-code---test-area}}

\hypertarget{testing-python-integration-temp-section---to-remove-later}{%
\section{testing Python Integration (temp section - to remove
later)}\label{testing-python-integration-temp-section---to-remove-later}}

Create a variable \texttt{x} in the Python session:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Access the Python variable \texttt{x} in an R code chunk:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{py}\SpecialCharTok{$}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

Create a new variable \texttt{y} in the Python session using R, and pass
a data frame to \texttt{y}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{py}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

Print the variable \texttt{y} in Python:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## {'speed': [4.0, 4.0, 7.0, 7.0, 8.0, 9.0], 'dist': [2.0, 10.0, 4.0, 22.0, 16.0, 10.0]}
\end{verbatim}

\newpage

\hypertarget{define-numbers-in-r}{%
\section{Define Numbers in R}\label{define-numbers-in-r}}

Let's define the numbers we will use in both R and Python:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a vector of numbers in R}
\NormalTok{define\_numbers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-code-block}{%
\section{R Code Block}\label{r-code-block}}

This is a simple R code block that calculates the sum of 5 numbers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# R code to sum 5 numbers}
\NormalTok{r\_sum }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(define\_numbers)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"The sum of the numbers in R is:"}\NormalTok{, r\_sum))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "The sum of the numbers in R is: 15"
\end{verbatim}

\hypertarget{python-code-to-pass}{%
\section{Python Code To Pass *}\label{python-code-to-pass}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{py}\SpecialCharTok{$}\NormalTok{n2 }\OtherTok{\textless{}{-}}\NormalTok{ define\_numbers}
\end{Highlighting}
\end{Shaded}

\hypertarget{python-code-block}{%
\section{Python Code Block}\label{python-code-block}}

This is a simple Python code block that calculates the sum of 5 numbers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Python code to calculate the sum of numbers defined in R}

\CommentTok{\# Define a list of numbers}
\NormalTok{numbers }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}

\CommentTok{\# Retrieve the numbers passed from R using reticulate\textquotesingle{}s \textquotesingle{}py\textquotesingle{} object}
\NormalTok{total\_sum }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(numbers)}
\NormalTok{total\_sum2 }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(n2)}

\CommentTok{\# Print the result}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The sum of the numbers \textquotesingle{}total sum\textquotesingle{} is: }\SpecialCharTok{\{}\NormalTok{total\_sum}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The sum of the numbers 'total sum' is: 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The sum of the numbers \textquotesingle{}n2\textquotesingle{} is: }\SpecialCharTok{\{}\NormalTok{total\_sum2}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The sum of the numbers 'n2' is: 15.0
\end{verbatim}

\newpage

\hypertarget{apendix-ii---test-area}{%
\chapter{Apendix II - test area}\label{apendix-ii---test-area}}

\renewcommand{\lstlistlistingname}{Code Block Listings}

\begin{lstlisting}[caption={Test Code Block Listing}, label={lst:Tst_Listing}]
\end{lstlisting}
\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Your R code here}
\FunctionTok{summary}\NormalTok{(cars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
\end{verbatim}

\normalsize

\listoffigures

\lstlistoflistings

\hypertarget{galley-sheet}{%
\chapter*{Galley Sheet}\label{galley-sheet}}
\addcontentsline{toc}{chapter}{Galley Sheet}

\hypertarget{galley-sheet-info}{%
\section{\texorpdfstring{\textbf{GALLEY Sheet
info}}{GALLEY Sheet info}}\label{galley-sheet-info}}

\layout

\hypertarget{galley-sheet-info---ends}{%
\section{\texorpdfstring{\textbf{GALLEY Sheet info -
ends}}{GALLEY Sheet info - ends}}\label{galley-sheet-info---ends}}

\newpage

\hypertarget{references}{%
\chapter{References}\label{references}}

\backmatter
\end{document}
